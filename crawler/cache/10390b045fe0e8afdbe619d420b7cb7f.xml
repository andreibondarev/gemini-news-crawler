<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 09 Apr 2024 12:11:22 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Fine Tuning Large Language Models: How Vertex AI Takes LLMs to the Next Level]]></title>
            <link>https://medium.com/google-cloud/fine-tuning-large-language-models-how-vertex-ai-takes-llms-to-the-next-level-3c113f4007da?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/3c113f4007da</guid>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[fine-tuning]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[supervised-learning]]></category>
            <dc:creator><![CDATA[Abirami Sukumaran]]></dc:creator>
            <pubDate>Mon, 08 Apr 2024 06:27:19 GMT</pubDate>
            <atom:updated>2024-04-08T06:27:19.482Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*F0bDfSj3tKoBaHPjgukktA.jpeg" /><figcaption>Image of stacked newspapers representing the use case we are addressing in this article</figcaption></figure><h3>Introduction</h3><p>Imagine a world where language models understand your business / industry’s specific needs, where they can generate text perfectly tailored to your unique domain or task. This isn’t science fiction; it’s the power of fine-tuning, and Google Cloud’s Vertex AI is making it accessible to everyone.</p><h3>Why Fine-Tuning Matters</h3><p>Tuning a foundation model can improve its performance. Foundation models are trained for general purposes and sometimes don’t perform tasks as well as you’d like them to. This might be because the tasks you want the model to perform are specialized tasks that are difficult to teach a model by using only prompt design. In these cases, you can use model tuning to improve the performance of a model for specific tasks. Model tuning can also help it adhere to specific output requirements when instructions aren’t sufficient. Think of large language models (LLMs) like incredibly talented students. They’ve learned a vast amount of information and can perform many tasks, but they excel when given specialized training. Fine-tuning is that training, allowing you to adapt a pre-trained LLM to your specific needs, whether it’s:</p><p><strong>Generating creative content:</strong> Imagine an LLM that writes marketing copy in your brand voice or composes poems in the style of your favorite poet.</p><p><strong>Summarizing complex information:</strong> Need to quickly grasp the key points of a lengthy research paper or news article in a particular domain or specialty like medicine or law? A fine-tuned LLM can do that for you.</p><p><strong>Translating languages with nuance:</strong> Go beyond literal translations and capture the cultural context and subtle meanings with a fine-tuned LLM.</p><blockquote>This article provides an overview of model tuning, describes the tuning options available on Vertex AI, and implements fine tuning using the supervised tuning approach for one of the models. More details about model customization are available <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models">here</a>.</blockquote><h3>Use case: From News Articles to Headlines</h3><p>Let’s see how this works in practice. Imagine you want to automatically generate headlines for news articles. Using Vertex AI, you can fine tune a Large Language Model that generates a suitable summarized title in a specific style and customization of titles that the news channel follows.</p><p>We will use BBC FULLTEXT DATA (made available by BigQuery Public Dataset <em>bigquery-public-data.bbc_news.fulltext</em>). We will fine tune an LLM (text-bison@002) to a new fine-tuned model called “bbc-news-summary-tuned” and compare the result to the response from the base model. The sample JSONL is made available for the implementation, feel free to upload it to your Cloud Storage Bucket to execute the fine tuning steps:</p><p><strong>Prepare your data:</strong> Start with a dataset of news articles and their corresponding headlines, like the BBC News dataset used in the example code.</p><p><strong>Fine-tune a pre-trained model:</strong> Choose a base model like “text-bison@002” and fine-tune it on your news data using Vertex AI’s Python sdk.</p><p><strong>Evaluate the results:</strong> Compare the performance of your fine-tuned model with the base model to see the improvement in headline generation quality.</p><p><strong>Deploy and use your model:</strong> Make your fine-tuned model available through an API endpoint and start generating headlines for new articles automatically.</p><p>For this we are going to use the <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/tuning/supervised-tuning">Supervised Tuning</a> approach. Supervised tuning improves the performance of a model by teaching it a new skill. Data that contains hundreds of labeled examples is used to teach the model to mimic a desired behavior or task. We are going to provide a labeled dataset for input text (prompt) and output text (response) to teach the model how to customize the responses for our specific use case.</p><p>Let’s dive in!</p><h3>Vertex AI: Your Fine-Tuning Partner</h3><p>Vertex AI provides a comprehensive suite of tools and services to guide you through the entire fine-tuning journey:</p><p><strong>Vertex AI Pipelines:</strong> Streamline your workflow by building and managing end-to-end machine learning pipelines, including data preparation, model training, evaluation, and deployment.</p><p><strong>Vertex AI Evaluation Services:</strong> Assess the performance of your fine-tuned model with metrics tailored to your specific task, ensuring it meets your quality standards.</p><p><strong>Vertex AI Model Registry:</strong> Keep track of all your models, including different versions and their performance metrics, in a centralized repository.</p><p><strong>Vertex AI Endpoints:</strong> Deploy your fine-tuned model as an API endpoint, making it easily accessible for integration into your applications.</p><h3>High Level Flow Diagram</h3><p>This diagram represents the flow of data and steps involved in the implementation. Please note that the owner for the respective step is mentioned in the text underneath.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/947/1*5233hdmreanFqQ91GB22iQ.png" /><figcaption>High Level Flow of Model Fine Tuning Steps</figcaption></figure><h3>Industry Use cases</h3><p>With Vertex AI, the possibilities are endless. You can fine-tune LLMs for sentiment analysis, chatbot development, code generation, and much more. This technology is democratizing access to powerful language models, allowing businesses and individuals to unlock new levels of creativity and efficiency.</p><h3>Hands-on Time</h3><p>This implementation is done with Vertex AI Python SDK for Generative AI models. You can also perform fine tuning in other ways — HTTP, CURL command, Java SDK, Console.</p><p>In 5 easy steps, you can fine-tune and evaluate your model for your customized responses!</p><ol><li><strong>Install and Import dependencies</strong></li></ol><pre>!pip install google-cloud-aiplatform<br>!pip install --user datasets<br>!pip install --user google-cloud-pipeline-components</pre><p>Follow the rest of the steps as shown in the .ipynb file in the repo. Make sure you replace the PROJECT_ID and BUCKET_NAME with your credentials.</p><pre>import os<br>os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;<br>import warnings<br>warnings.filterwarnings(&#39;ignore&#39;)<br>import vertexai<br>vertexai.init(project=PROJECT_ID, location=REGION)<br>import kfp<br>import sys<br>import uuid<br>import json<br>import pandas as pd<br>from google.auth import default<br>from datasets import load_dataset<br>from google.cloud import aiplatform<br>from vertexai.preview.language_models import TextGenerationModel, EvaluationTextSummarizationSpec</pre><p><strong>2. Prepare &amp; Load Training Data</strong></p><p>Replace YOUR_BUCKET with your bucket and upload the sample <a href="https://github.com/AbiramiSukumaran/LLMFineTuningSupervised/blob/main/TRAIN.jsonl">TRAIN.jsonl</a> training data file to it.</p><pre>json_url = &#39;https://storage.googleapis.com/YOUR_BUCKET/TRAIN.jsonl&#39;<br>df = pd.read_json(json_url, lines=True)<br>print (df)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/582/1*XRuJQcjxOyZ-09A5lVwc8Q.png" /><figcaption>Dataframe of the training dataset</figcaption></figure><p><strong>3. Fine Tune a Large Language Model</strong></p><pre>model_display_name = &#39;bbc-finetuned-model&#39; # @param {type:&quot;string&quot;}<br>tuned_model = TextGenerationModel.from_pretrained(&quot;text-bison@002&quot;)<br>tuned_model.tune_model(<br>training_data=df,<br>train_steps=100,<br>tuning_job_location=&quot;europe-west4&quot;,<br>tuned_model_location=&quot;europe-west4&quot;,<br>)</pre><p>The code above takes the pretrained model “text-bison@002” and tunes it with the data frame that has the training data we loaded in the previous step.</p><p>This step will take a few hours to complete. Remember you can always track the progress of the fine tuning pipeline in the pipeline job link it outputs in this step.</p><p><strong>4. Predict with the new Fine Tuned Model</strong></p><p>Once the fine tuning job is complete, you will be able to predict with your new model.</p><pre>response = tuned_model.predict(&quot;Summarize this text to generate a title: \n Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it it&#39;s putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn&#39;t stipulate a minimum amount of space for humans.&quot;)<br>print(response.text)</pre><p>Here is the output:</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/0*Y6cIFGtNVpsaxsb-" /><figcaption>Output of prediction using the new fine-tuned model</figcaption></figure><p><strong>Predict with Base Model (text-bison@002) for comparison</strong></p><pre>base_model = TextGenerationModel.from_pretrained(&quot;text-bison@002&quot;)<br>response = base_model.predict(&quot;Summarize this text to generate a title: \n Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it it&#39;s putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn&#39;t stipulate a minimum amount of space for humans.&quot;)<br>print(response.text)</pre><p>Here is the output:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/897/0*z-tfR03xCsgkqow3" /><figcaption>Output of prediction using the base model</figcaption></figure><p>Even though both titles generated look appropriate, the first one (generated with the fine-tuned model) is more in tune with the style of titles used in the dataset in question.</p><p><strong>Load the fine tuned model</strong></p><p>It might be easier to load a model that you just fine-tuned. But remember in step 3, it is invoked in the scope of the code itself so it still holds the tuned model in the variable tuned_model. But what if you want to invoke a model that was tuned in the past?</p><p>To do this, you can invoke the get_tuned_model() method on the LLM with the full ENDPOINT URL of the deployed fine tuned model from Vertex AI Model Registry.</p><pre>tuned_model_1 = TextGenerationModel.get_tuned_model(&quot;projects/273845608377/locations/europe-west4/models/4220809634753019904&quot;)<br>print(tuned_model_1.predict(&quot;YOUR_PROMPT&quot;))</pre><p><strong>5. Model Evaluation</strong></p><p>This is a big topic in itself. We will reserve that topic of detailed discussion to another day. For now, we will see how we can get some evaluation metrics on the fine tuned model and compare against the base model.</p><p>Load the <a href="https://github.com/AbiramiSukumaran/LLMEvaluationAuto/blob/main/EVALUATE.jsonl">EVALUATION dataset</a>:</p><pre>json_url = &#39;https://storage.googleapis.com/YOUR_BUCKET/EVALUATE.jsonl&#39;<br>df = pd.read_json(json_url, lines=True)<br>print (df)</pre><p>Evaluate:</p><pre> # Define the evaluation specification for a text summarization task on the fine tuned model<br>task_spec = EvaluationTextSummarizationSpec(<br>  task_name = &quot;summarization&quot;,<br>  ground_truth_data=df<br>)</pre><p>This step will take a few minutes to complete. You can track the progress using the pipeline job link in the step result. Once complete, you would be able to view the evaluation result:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*QoP9htlaWlXnszrw" /><figcaption>Evaluation Metric Output</figcaption></figure><p><strong>rougeLSum</strong>: This is the ROUGE-L score for the summary. ROUGE-L is a recall-based metric that measures the overlap between a summary and a reference summary. It is calculated by taking the longest common subsequence (LCS) between the two summaries and dividing it by the length of the reference summary.</p><p>The rougeLSum score in the given expression is 0.36600753600753694, which means that the summary has a 36.6% overlap with the reference summary.</p><p>If you run the evaluation step on the baseline model, you will observe that the summary score is RELATIVELY higher for the Fine Tuned Model.</p><p>You can find the evaluation results in the Cloud Storage output directory that you specified when creating the evaluation job. The file is named evaluation_metrics.json. For tuned models, you can also view evaluation results in the Google Cloud console on the Vertex AI Model Registry page.</p><blockquote>Important Considerations</blockquote><blockquote><strong>Model Support:</strong> Always check the model <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/tuning/supervised-tuning#models_that_support_supervised_tuning">documentation</a> for the latest compatibility.</blockquote><blockquote><strong>Rapid Development:</strong> The field of LLMs advances quickly. A newer, more powerful model could potentially outperform a fine-tuned model built on an older base. The good news is that you can apply these fine-tuning techniques to newer models when the capability becomes available.</blockquote><blockquote><strong>LoRA:</strong> LoRA is a technique for efficiently fine-tuning LLMs. It does this by introducing trainable, low-rank decomposition matrices into the existing pre-trained model’s layers. Read more about it <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/lora-qlora">here</a>. Instead of updating all the parameters of a massive LLM, LoRA learns smaller matrices that are added to or multiplied with the original model’s weight matrices. This significantly reduces the number of additional parameters introduced during fine-tuning.</blockquote><h3>Conclusion</h3><p>Fine-tuning is a powerful technique that allows you to customize LLMs to your domain and tasks. With Vertex AI, you have the tools and resources you need to fine-tune your models efficiently and effectively. Explore the GitHub repositories and experiment with the sample code to experience <a href="https://github.com/AbiramiSukumaran/LLMFineTuningSupervised">fine-tuning</a> and <a href="https://github.com/AbiramiSukumaran/LLMEvaluationAuto">evaluation</a> firsthand. Consider how fine-tuned LLMs can address your specific needs, from generating targeted marketing copy to summarizing complex documents or translating languages with cultural nuance. Utilize the comprehensive suite of tools and services offered by <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models">Vertex AI</a> to build, train, evaluate, and deploy your fine-tuned models with ease.</p><p>Register for the upcoming season of <a href="https://codevipassana.dev">Code Vipassana</a> (Season 6) where we will be building these out in instructor-led virtual hands-on sessions.</p><p><em>Also, if you are coming to Google Cloud NEXT on 9th, 10th and 11th of April 2024 at Vegas, check this in action or just come say hi at our Innovator’s Hive End to End AI demo station!</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3c113f4007da" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fine-tuning-large-language-models-how-vertex-ai-takes-llms-to-the-next-level-3c113f4007da">Fine Tuning Large Language Models: How Vertex AI Takes LLMs to the Next Level</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to Augment Text Data with Gemini through BigQuery DataFrames]]></title>
            <link>https://medium.com/google-cloud/how-to-augment-text-data-with-gemini-through-bigquery-dataframes-347bc6378413?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/347bc6378413</guid>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai]]></category>
            <dc:creator><![CDATA[Karl Weinmeister]]></dc:creator>
            <pubDate>Mon, 08 Apr 2024 06:26:44 GMT</pubDate>
            <atom:updated>2024-04-09T04:00:57.311Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JxjbysaQHi7-vJ5eQFbiJA.jpeg" /></figure><p>Data augmentation is a technique used in machine learning to increase the size of a dataset by creating new data out of existing data. This technique can help models <a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture">generalize</a> better, avoiding <a href="https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting">overfitting</a> on the data it was trained on.</p><p>You often think of doing this in visual data, by rotating data, flipping, cropping, and so forth. PyTorch has a very useful <a href="https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py">transforms</a> package that allows you to apply random transformations to your dataset with just a few lines of code. While this may reduce accuracy on the training set, it often results in <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417420305200">improved accuracy</a> on the test set of unseen data — which is what really matters!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*wWTbJJ62O5oO4lMZ" /><figcaption><strong>Example: Data Augmentations of Rock Images, </strong>Credit: TseKiChun, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons</figcaption></figure><p>We can apply this same technique to text data. It provides the same benefits of stretching your existing dataset, and making your model more robust to noise and outliers. There are proven <a href="https://link.springer.com/article/10.1186/s40537-021-00492-0/figures/1">benefits</a> to data augmentation of all datasets, which are particularly beneficial for small datasets.</p><p>Let’s explore a few examples using popular techniques:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/697/1*2sbkuAY-tFZgsIZ6H7SWDA.png" /></figure><p>There are a number of ways you can apply these techniques manually. Let’s say you want to apply the random deletion technique with <em>p</em>=0.1 of tokens deleted. You can tokenize the text and then add back tokens with (1-<em>p</em>) probability. Or, for back-translation, you can call the <a href="https://cloud.google.com/translate">Translation API</a> once for the target language, and then a second time to translate back to the original language. For synonyms, you could use a <a href="https://github.com/goodmami/wn">WordNet API</a> on random tokens.</p><p>With a powerful LLM like <a href="https://ai.google.dev/docs/migrate_to_cloud">Gemini</a>, you have a bag of tricks at your fingertips. You can easily make these modifications and much more in one toolset. No need to cobble together multiple tools any longer.</p><p>Let’s look at how to apply these techniques on a real world dataset of <a href="https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow">Stack Overflow questions and answers</a>. All of the details are provided in this <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/data-augmentation/data_augmentation_for_text.ipynb">notebook</a>, and I’ll point out the highlights here.</p><p>You can use <a href="https://cloud.google.com/python/docs/reference/bigframes/latest">BigQuery DataFrames</a> for all kinds of problems, but it will make text augmentation on our BigQuery dataset particularly straightforward. It provides a pandas-compatible DataFrame and scikit-learn-like ML API that enables us to query Gemini directly. It can handle batch jobs on massive datasets, as all DataFrame storage is in BigQuery.</p><p>So, let’s get started with one of these techniques, synonym replacement. First, we can query for accepted Stack Overflow Python answers since 2020, and put it into a BigQuery DataFrame:</p><pre>stack_overflow_df = bpd.read_gbq_query(<br>    &quot;&quot;&quot;SELECT<br>           CONCAT(q.title, q.body) AS input_text,<br>           a.body AS output_text<br>       FROM `bigquery-public-data.stackoverflow.posts_questions` q<br>       JOIN `bigquery-public-data.stackoverflow.posts_answers` a<br>         ON q.accepted_answer_id = a.id<br>       WHERE q.accepted_answer_id IS NOT NULL<br>         AND REGEXP_CONTAINS(q.tags, &quot;python&quot;)<br>         AND a.creation_date &gt;= &quot;2020-01-01&quot;<br>       LIMIT 550<br>    &quot;&quot;&quot;)</pre><p>Here’s a sneak peek of the Q&amp;A DataFrame:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/704/0*wxaiojCKMWB4qMCY" /></figure><p>Let’s now randomly sample a number of rows from the dataframe. Set <em>n_rows</em> to the number of new samples you’d like:</p><pre>df = stack_overflow_df.sample(n_rows)</pre><p>We can then define a Gemini text generator <a href="https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.llm.GeminiTextGenerator">model</a> like this:</p><pre>model = GeminiTextGenerator()</pre><p>Next, let’s create two columns: a <strong>prompt column</strong> with synonym replacement instructions concatenated with the input text, and a <strong>result column</strong> with the synonym replacement applied.</p><pre># Create a prompt with the synonym replacement instructions and the input text<br>df[&quot;synonym_prompt&quot;] = (<br>f&quot;Replace {n_replacement_words} words from the input text with synonyms, &quot;<br>+ &quot;keeping the overall meaning as close to the original text as possible.&quot;<br>+ &quot;Only provide the synonymized text, with no additional explanation.&quot;<br>+ &quot;Preserve the original formatting.\n\nInput text: &quot;<br>+ df[&quot;input_text&quot;])<br><br># Run batch job and assign to a new column<br>df[&quot;input_text_with_synonyms&quot;] = model.predict(<br>df[&quot;synonym_prompt&quot;]<br>).ml_generate_text_llm_result<br><br># Compare the original and new columns<br>df.peek()[[&quot;input_text&quot;, &quot;input_text_with_synonyms&quot;]]</pre><p>Here are the results! Notice the subtle changes in the text with synonym replacement.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/717/0*aifrCJkADKM8zuaO" /></figure><p>Using this framework, it is simple to apply all kinds of batch transformations to augment your data. In the <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/data-augmentation/data_augmentation_for_text.ipynb">notebook</a>, you’ll see more prompts you can use for back translation and noise injection. You’ve also seen how easy it is to enhance datasets with <a href="https://cloud.google.com/bigquery/docs/dataframes-quickstart">BigQuery DataFrames</a>. We hope this helps you in your data science journey using <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro">Gemini on Google Cloud</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=347bc6378413" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/how-to-augment-text-data-with-gemini-through-bigquery-dataframes-347bc6378413">How to Augment Text Data with Gemini through BigQuery DataFrames</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex Search and Conversation]]></title>
            <link>https://medium.com/google-cloud/vertex-search-and-conversation-364cdc591167?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/364cdc591167</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-chat-bots]]></category>
            <category><![CDATA[conversational-ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[vertex-search]]></category>
            <dc:creator><![CDATA[Anita Gutta]]></dc:creator>
            <pubDate>Mon, 08 Apr 2024 06:26:09 GMT</pubDate>
            <atom:updated>2024-04-08T06:26:09.282Z</atom:updated>
            <content:encoded><![CDATA[<p>Vertex AI Search is a fully-managed platform, powered by large language models, that lets you build AI-enabled search and recommendation experiences for your public or private websites or mobile applications. For a thorough understanding of the methodology and procedures associated with the creation, deployment, maintenance, and monitoring of a Vertex AI Search instance, please refer to the <a href="https://cloud.google.com/generative-ai-app-builder/docs/introduction">Vertex AI Search and Conversation documentation</a>.</p><p>There are many options to DIY a Search/Chat bot … <strong><em>BUT</em></strong> <strong>Demos are easy, Production is hard.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*UeqxMw2saNXhowf7" /></figure><p>The following are the steps involved in constructing a Retrieval-Augmented Generation (RAG) search application:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Y3LZ8-03KQ_pN7Qy" /></figure><p>The utilization of managed Vertex AI Search by Google Cloud considerably streamlines this process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7sUEga74WmFPm_pr" /></figure><p>Follow the simple steps to build your datastores and engines, then load and query the data in GCP Vertex AI Search and Conversation platform.</p><p><strong>What we are going to Build</strong></p><ol><li>Datastores and Engines from Terraform</li><li>Load data and query from python</li></ol><h3><strong>Step 1:</strong></h3><pre># Create Test DataStore<br>resource &quot;google_discovery_engine_data_store&quot; &quot;test-ds&quot; {<br>  location = &quot;global&quot;<br>  data_store_id = &quot;test-data-store-id&quot;<br>  display_name = &quot;test-unstructured-datastore&quot;<br>  industry_vertical = &quot;GENERIC&quot;<br>  content_config = &quot;CONTENT_REQUIRED&quot;<br>  solution_types = [&quot;SOLUTION_TYPE_SEARCH&quot;]<br>  create_advanced_site_search = false<br>  project = &lt;PROJECT ID&gt;<br>}<br><br># Create Test Serach Engine<br>resource &quot;google_discovery_engine_search_engine&quot; &quot;test-engine&quot; {<br>  engine_id = &quot;test_engine_id&quot;<br>  collection_id = &quot;default_collection&quot;<br>  location = google_discovery_engine_data_store.test-ds.location<br>  display_name = &quot;test-engine&quot;<br>  industry_vertical = &quot;GENERIC&quot;<br>  data_store_ids = [google_discovery_engine_data_store.test-ds.data_store_id]<br>  common_config {<br>  company_name = &quot;Test Company&quot;<br>  }<br>search_engine_config {<br>  search_add_ons = [&quot;SEARCH_ADD_ON_LLM&quot;]<br>  }<br>project = &lt;PROJECT ID&gt;<br>}<br><br># Output resource Id&#39;s<br>output &quot;test_data_store_id&quot; {<br>  value =resource.google_discovery_engine_data_store.test-ds.data_store_id<br>}<br>output &quot;test_engine_id&quot; {<br>  value = resource.google_discovery_engine_search_engine.test-engine.engine_id<br>}</pre><p>Run</p><pre>Plugin project ID in above terraform code and run below commands<br><br>terraform init<br>terraform plan<br>terraform apply</pre><p>If you running terraform with your own GCP identity you will run into an error like below.</p><pre>Error: Error creating DataStore: googleapi: Error 403: <br>Your application is authenticating by using local Application Default <br>Credentials. The discoveryengine.googleapis.com API requires a quota project, <br>which is not set by default. To learn how to set your quota project, <br>see https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds .</pre><p>Add provider block to specify billing project. billing project and project where datastores are created can be same.</p><pre>provider &quot;google&quot; {<br>  project         = &lt;PROJECT ID&gt;  // Your actual GCP project ID<br>  user_project_override = true <br>  billing_project = &lt;BILLING PROJECT ID&gt; // The project used for quota<br>}</pre><p>Once terraform is successful you can login to GCP Console -&gt; Vertex Search and Conversation</p><ol><li>Confirm an App named “test-engine” is created</li><li>Confirm an empty datastore named “test-unstructured-datastore” is created</li></ol><h3>Step 2:</h3><p>Create load_data.py with below contents. Plug in project Id and Datastore ID (output from terraform)</p><pre><br>from google.cloud import storage<br>from google.api_core.client_options import ClientOptions<br>from google.cloud import discoveryengine_v1alpha as discoveryengine<br><br><br>def import_documents(<br>    project_id: str,<br>    location: str,<br>    data_store_id: str,<br>    gcs_uri: str,<br>):<br>    # Create a client<br>    client_options = (<br>        ClientOptions(<br>            api_endpoint=f&quot;{location}-discoveryengine.googleapis.com&quot;)<br>        if location != &quot;global&quot;<br>        else None<br>    )<br>    client = discoveryengine.DocumentServiceClient(<br>        client_options=client_options)<br><br>    # The full resource name of the search engine branch.<br>    # e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}<br>    parent = client.branch_path(<br>        project=project_id,<br>        location=location,<br>        data_store=data_store_id,<br>        branch=&quot;default_branch&quot;,<br>    )<br><br>    source_documents = [f&quot;{gcs_uri}/*&quot;]<br><br>    request = discoveryengine.ImportDocumentsRequest(<br>        parent=parent,<br>        gcs_source=discoveryengine.GcsSource(<br>            input_uris=source_documents, data_schema=&quot;content&quot;<br>        ),<br>        # Options: `FULL`, `INCREMENTAL`<br>        reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,<br>    )<br><br>    # Make the request<br>    operation = client.import_documents(request=request)<br><br>    response = operation.result()<br><br>    # Once the operation is complete,<br>    # get information from operation metadata<br>    metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)<br><br>    # Handle the response<br>    return operation.operation.name<br><br><br>source_documents_gs_uri = (<br>    &quot;gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs&quot;<br>)<br><br>PROJECT_ID = &lt;PROJECT ID&gt;<br>DATASTORE_ID = &lt;DATASTORE ID&gt;<br>LOCATION = &quot;global&quot;<br>print(&quot; Starting loading data into datastore&quot;)<br>import_documents(PROJECT_ID, LOCATION, DATASTORE_ID, source_documents_gs_uri)<br>print(&quot; Completed loading data into datastore&quot;)</pre><p>Create search_data.py with below contents. Plug in project Id and Datastore ID (output from terraform)</p><pre>from google.cloud import storage<br>from google.api_core.client_options import ClientOptions<br>from google.cloud import discoveryengine_v1alpha as discoveryengine<br>from typing import List<br><br><br>def search_sample(<br>    project_id: str,<br>    location: str,<br>    data_store_id: str,<br>    search_query: str,<br>) -&gt; List[discoveryengine.SearchResponse]:<br>    #  For more information, refer to:<br>    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store<br>    client_options = (<br>        ClientOptions(<br>            api_endpoint=f&quot;{location}-discoveryengine.googleapis.com&quot;)<br>        if LOCATION != &quot;global&quot;<br>        else None<br>    )<br><br>    # Create a client<br>    client = discoveryengine.SearchServiceClient(client_options=client_options)<br><br>    # The full resource name of the search engine serving config<br>    # e.g. projects/{project_id}/locations/{location}/dataStores/{data_store_id}/servingConfigs/{serving_config_id}<br>    serving_config = client.serving_config_path(<br>        project=project_id,<br>        location=location,<br>        data_store=data_store_id,<br>        serving_config=&quot;default_config&quot;,<br>    )<br><br>    # Optional: Configuration options for search<br>    # Refer to the `ContentSearchSpec` reference for all supported fields:<br>    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest.ContentSearchSpec<br>    content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(<br>        # For information about snippets, refer to:<br>        # https://cloud.google.com/generative-ai-app-builder/docs/snippets<br>        snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(<br>            return_snippet=True<br>        ),<br>        # For information about search summaries, refer to:<br>        # https://cloud.google.com/generative-ai-app-builder/docs/get-search-summaries<br>        summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(<br>            summary_result_count=5,<br>            include_citations=True,<br>            ignore_adversarial_query=True,<br>            ignore_non_summary_seeking_query=True,<br>        ),<br>    )<br><br>    # Refer to the `SearchRequest` reference for all supported fields:<br>    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest<br>    request = discoveryengine.SearchRequest(<br>        serving_config=serving_config,<br>        query=search_query,<br>        page_size=10,<br>        content_search_spec=content_search_spec,<br>        query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(<br>            condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,<br>        ),<br>        spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(<br>            mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO<br>        ),<br>    )<br><br>    response = client.search(request)<br>    return response<br><br><br>QUERY = &quot;Who is the CEO of Google?&quot;<br><br><br>PROJECT_ID = &lt;PROJECT ID&gt;<br>DATASTORE_ID = &lt;DATASTORE ID&gt;<br>LOCATION = &quot;global&quot;<br><br>print(search_sample(PROJECT_ID, LOCATION, DATASTORE_ID, QUERY).summary.summary_text)</pre><p>Output of above python script should print the summary for the query.</p><p>Do create datastores and engines in python refer to this <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/create_datastore_and_search.ipynb">Github Resource</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=364cdc591167" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/vertex-search-and-conversation-364cdc591167">Vertex Search and Conversation</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Databases on the Google Cloud Next 24]]></title>
            <link>https://medium.com/google-cloud/databases-on-the-google-next-24-6e9941d6d361?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6e9941d6d361</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[alloydb-omni]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[alloydb]]></category>
            <dc:creator><![CDATA[Gleb Otochkin]]></dc:creator>
            <pubDate>Mon, 08 Apr 2024 06:25:14 GMT</pubDate>
            <atom:updated>2024-04-08T13:52:14.254Z</atom:updated>
            <content:encoded><![CDATA[<p>Heading to Google Next? Here is my list to attend or watch next week. I am a database person, so you can guess what my picks are. Here are the latest updates in database and AI across the entire line of database services.</p><p>Let’s start from keynotes — it is always fun to watch and it is some kind of essence of all news in a compressed format.</p><ul><li><a href="https://cloud.withgoogle.com/next/session-library?session=GENKEY&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Opening Keynote: The new way to cloud.</a> Tuesday 9am — A lot of announcements and news delivered by Thomas Kurian and the team including my colleague <a href="mailto:gweiss@google.com">Gabe Weiss</a> among presenters — don’t miss it out.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DEVKEY&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Fast. Simple. Cutting edge. Pick three.</a> Wednesday 10:30 AM Don’t miss! — Developers keynote done by Brad Calder, Richard Seroter, some of our customers and tons of my colleagues. It should be fun to watch. And the punch line in the title! I don’t know who came out with it but it is brilliant. My bet — it was Richard Seroter (disclaimer — I have no knowledge who has come with the title).</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=SPTL203&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">What’s next for Google Cloud databases</a> — SPTL203 Tuesday 2:15 PM delivered by Andi Gutmans with customers, partners and my dear colleagues Sandy Ghai, Anita Kibunguchy-Grant and <a href="mailto:gweiss@google.com">Gabe Weiss</a> — dedicated to the database news, a lot of good exciting news. I personally hope to watch really cool demos.</li></ul><p>Now let’s look in more details about databases. Of course one of the main themes is Gen AI in databases. Some sessions I think will be really interesting on how Google and customers can use the new generative AI.</p><ul><li><a href="https://cloud.withgoogle.com/next/session-library?session=IHWS118&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Intro AlloyDB AI</a> — IHWS118 — Tuesday 4:15 PM — Tech Training Zone 4 in Innovation Hive. If you have 30 min at the end of the first day come over and try AlloyDB AI integration with Vertex AI. And, of course, welcome to the Innovation Hive at any time to chat about databases and AI.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS219&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Under the hood: Redefining vector search for generative AI application development</a> — DBS219 — Tuesday 4:45 PM — Don’t miss this one if you are in AI and vector search. Andy Brook, Alan Li and Yannis Papakonstantinou will talk about the latest and greatest techniques in vector search. I am personally looking forward to that one.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS105&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Effortless database management with AI</a> — DBS105 — Wednesday 8 AM — My colleagues <a href="mailto:nimeshbhagat@google.com">Nimesh Bhagat</a>, Cat Colman and <a href="mailto:aossikine@google.com">Alexey Ossikine</a> along with Bogdan Capatina from Ford discuss how AI helps with database management and monitoring. If you have a production database to manage you know how important it is.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS103&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Next-generation AI-assisted migrations from Oracle to AlloyDB with Gemini</a> — DBS103 — Wednesday 9:15 AM — How AI helps with cross-engine migrations from Oracle to AlloyDB. I know 9:15 AM is hard and the coffee lines in Mandalay Bay are long. But it is worth it.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS200&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Accelerate your generative AI journey with Google Cloud databases</a> — DBS200 — Wednesday 12:30 PM — My friends <a href="mailto:yeilat@google.com">Yoav Eilat</a> and <a href="mailto:janavg@google.com">Jana van Greunen</a> will explain integration of our flagman databases services AlloyDB and Cloud SQL with AI and how it helps to bring all the benefits of AI to the real world. Should be really cool.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS216&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">A deep dive into AlloyDB for PostgreSQL</a> — DBS216 Wednesday 01:45 PM — Presented by <a href="mailto:ravimurthy@google.com">Ravi Murthy</a> and <a href="mailto:sbghai@google.com">Sandy Ghai</a> along with Aaron Joyce from Bayer. It will be a deep dive into AlloyDB and integration with AI.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS104&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">The future of databases and generative AI </a>— DBS104 — Wednesday 1:45 PM — Our VP of engineering Sailesh Krishnamurthy and Fiona Tan from Wayfair will discuss the vector search in Google databases and integration with AI. I am really interested to hear about real cases and how it is implemented.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS218&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Generative AI use-cases and design patterns with Databases</a> — DBS218 — Thursday 2:45 PM — Presented by <a href="mailto:pranavnambiar@google.com">Pranav Nambiar</a> and <a href="mailto:kvg@google.com">Kurtis Van Gent</a>. — Technical with deep dive to real use cases. Third day at Next is not a half day — it is still a real conference. Don’t miss it.</li></ul><p>The next topic is about my special point of interest — AlloyDB and its sibling AlloyDB Omni. I don’t know why but sometimes I feel like AlloyDB Omni is some kind of home slippers — you put it on easy and you really feel good wearing them.</p><ul><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS214&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Accelerate analytics and semantic search in real-time with AlloyDB for PostgreSQL</a> — DBS214 — Wednesday 3 PM — my colleagues Sam Idicula and Sridhar Ranganathan will tell about all AlloyDB AI and the AlloyDB unique features which can help with analytical workload.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS215&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Beyond PostgreSQL: Modernize your applications anywhere with AlloyDB Omni</a> — DBS215 Wednesday 4:15 PM — Bjoern Rost, <a href="mailto:kevinjernigan@google.com">Kevin Jernigan</a> ,Tabby Lewis along with old friend Nelson Calero will talk about our downloadable version of AlloyDB — AlloyDB Omni and how it can be deployed including Kubernetes with HA, DR and other great features.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS106&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Run your generative AI application anywhere with AlloyDB Omni — no strings attached</a> — DBS106 — Thursday 9:45 AM — GG Goindi, Tabby Lewis and Sharanya Desai will discuss about new possibilities when AlloyDB Omni is used with locally hosted models and sensitive data.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=ARC308&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">How to use managed database services in an air-gapped on-premises environment</a> — ARC308 — Thursday at 11 AM and delivered by my colleagues <a href="mailto:rost@google.com">Bjoern Rost</a>, Cat Colman and Jay Gindin. It talks about database management and new ways to integrate on-premises and cloud services.</li></ul><p>Some good sessions about migrations and database management.</p><ul><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS210&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Best practices to maximize the availability of your Cloud SQL databases</a> — DBS210 — on Tuesday at 12:15 PM — Gopal Ashok, <a href="mailto:rahulad@google.com">Rahul Deshmukh</a> and Patrick Kirby from Workday will discuss best practices for Cloud SQL.</li><li><a href="https://cloud.withgoogle.com/next/session-library?session=DBS213&amp;utm_source=copylink&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY24-Q2-global-ENDM33-physicalevent-er-next-2024-mc&amp;utm_content=next-homepage-social-share&amp;utm_term=-">Migrate enterprise-grade workloads to Cloud SQL for SQL Server</a> — DBS213 — Thursday 12:15 PM — about moving workloads to Cloud SQL for SQL Server delivered by Google folks Mohamed Kabiruddin and Erez Alsheich along with Nitin Shingate from GoodRx</li></ul><p>That’s only a small part of what is coming on the Next. It is going to be 3 full days of learning communications and fun. And don’t forget to visit Innovation Hive and Showcase areas with tons of great stuff.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6e9941d6d361" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/databases-on-the-google-next-24-6e9941d6d361">Databases on the Google Cloud Next 24</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Claude 3 Sonnet  Gemini 1.5 Pro]]></title>
            <link>https://medium.com/google-cloud/claude-vs-gemini-39559efcde8e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/39559efcde8e</guid>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[claude-3]]></category>
            <category><![CDATA[multimodal]]></category>
            <dc:creator><![CDATA[Vaibhav Malpani]]></dc:creator>
            <pubDate>Mon, 08 Apr 2024 06:24:26 GMT</pubDate>
            <atom:updated>2024-04-08T06:24:25.927Z</atom:updated>
            <content:encoded><![CDATA[<p>Recently Google launched Claude 3 Sonnet model on Google Cloud. To read about how to get started, follow the below blog.</p><p><a href="https://medium.com/google-cloud/claude-3-on-google-cloud-20c65b308f01">Getting Started with Claude 3 on Google Cloud</a></p><p>In this blog, we will focus on how <strong>“Claude 3 Sonnet”</strong> and <strong>“Gemini 1.5 Pro” </strong>perform in a head-on battle where I will provide both with exact same prompt and check which one performs better.</p><blockquote><strong>Disclaimer: While both Claude 3 and Gemini 1.5 Pro achieve similar overall performance, this comparison aims to highlight specific areas where one model might be preferable over the other.</strong></blockquote><blockquote>For easy of writing, I will call “<strong>Claude 3 Sonnet</strong>” as “<strong>Claude</strong>” and “<strong>Gemini 1.5 Pro</strong>” as “<strong>Gemini</strong>” for the context of this blog.</blockquote><h3>Text Prompts Example 1:</h3><p>The First thing I tried was giving both models a very simple prompt, and both were able to give answer quite nicely.</p><h4>Prompt: How to make a Banana Protein Shake in less than 100 words</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eS3Clr9clSwV8OXXCq-j1g.png" /><figcaption>Claude</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bqxrfaBdTZR59qxDtYGJFQ.png" /></figure><p>While the response time on Claude was <strong>~7 sec</strong>, the response from Gemini was <strong>~3 secs</strong>. Also, the <strong>presentaion of response</strong> from Gemini is far better than Claude. Giving a nice title to the response, giving the response in a listed format, adds a nice touch to the overall experience.</p><h3>Text Prompts Example 2:</h3><p>In this Scenario, I tried to give incorrect spelling to understand if the model is able to pick that up and correct the prompt to give me the desired output. Notice the intentional spelling mistake of “<strong>Moana</strong>” to “<strong>Maona</strong>”.</p><h3>Prompt: Tell me about the movie name Maona and who all starred in the movie</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*a5L4RBPmn-W1E5o4DxB3VQ.png" /><figcaption>Claude</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*l1ljo2mJmTLJEwbpz8xEIg.png" /><figcaption>Gemini</figcaption></figure><p>Despite <strong>very close response times</strong>, the outputs from the two models differed dramatically. While Gemini was able to correct the spelling mistake and get the desired output, Claude could not identify even a small spelling mistake and give response which is near to the query.</p><h3>Image Prompt Example 1:</h3><p>In this, I tried to give a single image and a prompt with the image to ask some questions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lnsWMpIz61oidGEiqgktZg.jpeg" /><figcaption>Image used along with prompt</figcaption></figure><h4>Prompt: From the image try to guess the city and weather</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PenvLPi96WsEZYTWnFAVtQ.png" /><figcaption>Claude</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/653/1*ZmLQXcB8QQB1ymMzc9IySQ.png" /><figcaption>Gemini</figcaption></figure><p>In this test, there was a huge difference in time, where Claude is taking <strong>~10 Sec</strong>, Gemini is able to get very precise response in just <strong>~2 Secs</strong>.</p><h3>Image Prompt Example 2:</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/0*B1CB2xnXCiE3zZ3p.jpg" /><figcaption>Image used from prompt</figcaption></figure><h4>Prompt: what is the name of character and give more details about him/her</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*B16o8NLA5-QZDgCIeUEaEA.png" /><figcaption>Claude</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OSy5gcao_Kh_u6Lxh1pnAw.png" /><figcaption>Gemini</figcaption></figure><p>In the above Scenario, Despite <strong>very close response times</strong>, the outputs from the two models differed dramatically. Claude 3 did not give any response, saying that it would be against the privacy, which i do not think is the right. The above shown character was part of a famous movie, giving details about the character is not at all a privacy infringement.</p><p>Also this is not specific to Movie characters, Claude is able to predict Famous cartoon characters like Tom and Jerry, Loony Toons, etc. on giving the same prompt as mentioned about, But when given a <a href="https://en.wikipedia.org/wiki/Anime">Anime</a> character, it says that “<em>It would go against respecting individual privacy</em>.”</p><h3>Image Prompt Example 3:</h3><p>Image used from <a href="https://www.youtube.com/watch?app=desktop&amp;v=QPE7zcqcJNU">YouTube</a>. Please refer to the video to understand the Integration steps.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ve4WjE9ZfT4a63Ia.jpg" /><figcaption>Image used for prompt</figcaption></figure><h4>Prompt: What is this equation for? Solve the problem.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*77Y3ITQM8CypBgXi7ly0XA.png" /><figcaption>Claude</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5E-_M2UcCUV--4yikq0Yuw.png" /><figcaption>Gemini</figcaption></figure><p>In the above Scenario, Gemini was faster compared to Claude and also the response given by Gemini was quite readable. But <strong>Both were not able to solve the problem correctly</strong>. Claude made mistake right from the first step, but Gemini was able to perform all the integration part and it failed on the very last step while doing <strong>(390625/24+15625/6)</strong></p><h3>Chat Prompts:</h3><p>I tried to ask one question and then asked question related to the response from first question. <br>While Gemini has Capability to maintain chat history and give response accordingly, But Claude does not have this capability.</p><p>The response time for each question is between 1 and 2 Secs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/886/1*V7hMQ3_WkrAaJpG4eHWqkw.png" /><figcaption>Gemini</figcaption></figure><h3>Conclusions:</h3><ol><li><strong>Response Time: </strong>Gemini is Faster as compared to Claude 3 in all cases.</li><li><strong>Accuracy: </strong>Gemini has overall higher accuracy while giving responses.</li><li><strong>Response Quality: </strong>Gemini is able to give Precise answers when asked for it. <em>(as shown in Image Prompt Example 1)</em></li><li><strong>Mathematical Capability: </strong>While both were not able to get to the final answer of the double integration problem, Gemini could reach till the last step, before it failed.</li><li><strong>Chat Capability: </strong>Capability to have a continues chat with Gemini is a big plus point over Claude.</li><li><strong>Spell Correction with Context: </strong>Gemini is able to correct the spelling based by understanding the context from prompt. <em>(as shown in Text Prompt Example 2)</em></li><li><strong>Markdown Answers: </strong>Gemini provides responses with markdown, which gives a good Presentation of the response <em>(as shown in Text Prompt Example 1)</em></li></ol><h3>If you enjoyed this post, give it a clap! 👏 👏</h3><h4>Interested in similar content? Follow me on <a href="https://medium.com/@IVaibhavMalpani">Medium</a>, <a href="https://twitter.com/IVaibhavMalpani">Twitter</a>, <a href="https://www.linkedin.com/in/ivaibhavmalpani/">LinkedIn</a> for more!</h4><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=39559efcde8e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/claude-vs-gemini-39559efcde8e">Claude 3 Sonnet 🆚 Gemini 1.5 Pro</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Recipe Mixer‍]]></title>
            <link>https://medium.com/google-cloud/recipe-mixer-88fe60836e43?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/88fe60836e43</guid>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[genai]]></category>
            <category><![CDATA[geminipro]]></category>
            <dc:creator><![CDATA[Gitesh Mahadik]]></dc:creator>
            <pubDate>Fri, 05 Apr 2024 04:51:23 GMT</pubDate>
            <atom:updated>2024-04-05T04:51:23.558Z</atom:updated>
            <content:encoded><![CDATA[<blockquote>Recipe Mixer is an AI-powered web application that encourages culinary exploration through recipe remixing. Users can input a recipe or list ingredients they have on hand, and the application utilizes the Gemini Pro model to suggest alternative ingredients based on flavor profiles and user preferences, including dietary restrictions.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WztpIzEwXLtIAPHgrdAntA.gif" /></figure><h4>Why use Recipe Mixer?</h4><p>Recipe Mixer is a user-friendly web application that helps you find delicious recipes based on the ingredients you have on hand. Whether you’re a busy parent, a cooking enthusiast, or someone with dietary restrictions, Recipe Mixer makes meal planning easy and enjoyable.</p><p>All you need to do is input the ingredients you have in your kitchen and specify any dietary preferences you may have, such as vegetarian or gluten-free. Recipe Mixer then suggests personalized recipe options that match your input. It even offers alternative ingredient suggestions and can adapt recipes to different cultural cuisines, so you can explore new flavors and cooking styles.</p><p>With Recipe Mixer, you no longer have to worry about what to cook for dinner or how to use up leftover ingredients. It’s like having a personal chef at your fingertips, ready to inspire you with creative and delicious meal ideas.</p><h4>Features</h4><ul><li><strong>Ingredient Matching:</strong> Users can input a list of ingredients they have on hand, and the application suggests recipes based on those ingredients.</li><li><strong>Dietary Preferences:</strong> Users can specify dietary preferences such as vegetarian, vegan, or gluten-free, and the suggested recipes take these preferences into account.</li><li><strong>Alternative Ingredient Suggestions:</strong> The application suggests alternative ingredients based on flavor profiles and dietary preferences, allowing users to experiment with different ingredients.</li><li><strong>Cultural Adaptation:</strong> The suggested recipes can be adapted to different cultural cuisines, promoting culinary exploration and diversity.</li></ul><h4>Dependencies</h4><ul><li><strong>Gemini Pro LLM:</strong> Natural language processing model for text classification.</li><li><strong>Streamlit:</strong> Web application framework for building interactive web applications.</li><li><strong>Google Generative AI:</strong> Integrates advanced AI capabilities into the application.</li><li><strong>python-dotenv:</strong> python-dotenv is a Python library that allows you to read environment variables from .env files.</li><li><strong>langchain.llms:</strong> langchain.llms is a library used to interact with language models, particularly for text generation tasks.</li></ul><h4>How it works?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BcrtnKnVez7YEpYpSjp84g.gif" /></figure><h3>Building Recipe Mixer🧑🏻‍🍳</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DCwRD0yFB7ylxDsl36BKUQ.gif" /></figure><h4>Requirements:</h4><ul><li>Python 3.10</li><li><a href="https://ai.google.dev/">Gemini Pro API key</a> (Note: Ensure you have the necessary credentials and permissions to access the Gemini Pro API)</li></ul><h4>Steps:</h4><ol><li>Clone the repository</li></ol><pre>git clone https://github.com/Gitesh08/recipe-mixer.git</pre><p>or create <strong>app.py</strong> file and paste below code.</p><pre>import streamlit as st<br>import google.generativeai as genai<br>from dotenv import load_dotenv<br>import os<br><br># Load environment variables from a .env file<br>load_dotenv()<br><br># Configure the generative AI model with the Google API key<br>genai.configure(api_key=os.getenv(&quot;GOOGLE_API_KEY&quot;))<br><br># Set up the model configuration for text generation<br>generation_config = {<br>    &quot;temperature&quot;: 0.4,<br>    &quot;top_p&quot;: 1,<br>    &quot;top_k&quot;: 32,<br>    &quot;max_output_tokens&quot;: 4096,<br>}<br><br>     # Create a GenerativeModel instance with &#39;gemini-pro&#39; as the model type<br>llm = genai.GenerativeModel(<br>    model_name=&quot;gemini-pro&quot;,<br>    generation_config=generation_config,<br>    )<br>    <br>def match_ingredients(user_ingredients, dietary_preferences=None):<br>    &quot;&quot;&quot;<br>    Matches ingredients with recipes using Gemini Pro (if available).<br><br>    Args:<br>        user_ingredients (list): List of user-provided ingredients (lowercase and stripped).<br>        dietary_preferences (str, optional): User&#39;s dietary preferences (e.g., vegetarian, vegan).<br><br>    Returns:<br>        tuple: A tuple containing the recipe name and instructions (if found), otherwise None.<br>    &quot;&quot;&quot;<br>    <br>    prompt_template = &quot;&quot;&quot;Find a delicious recipe using these ingredients: {ingredients}. <br>    {dietary_preferences_prompt}<br>    I want the response in a single structured format.&quot;&quot;&quot;<br><br>    dietary_preferences_prompt = f&quot;Considering dietary restrictions: {dietary_preferences}&quot; if dietary_preferences else &quot;&quot;<br><br>    prompt = prompt_template.format(ingredients=&quot;, &quot;.join(user_ingredients), dietary_preferences_prompt=dietary_preferences_prompt)<br><br>    response = llm.generate_content(prompt)<br><br>  # Parse the response from Gemini Pro to extract the matching recipe name and instructions (implementation depends on API response format)<br>    recipe = response.text<br>  # ... (code to parse response and extract recipe information)<br>    return recipe<br><br>st.set_page_config(page_title=&quot;Recipe Mixer&quot;)<br><br>st.title(&quot;Recipe Mixer&quot; + &quot;:sunglasses:&quot;)<br>st.markdown(&#39;&lt;style&gt;h1{color: orange; text-align: center; font-family:POPPINS}&lt;/style&gt;&#39;, unsafe_allow_html=True)<br><br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br><br>user_ingred = st.text_input(&quot;Enter your ingredients (comma-separated):&quot;)<br><br># Add dietary preference dropdown<br>dietary_options = st.selectbox(&quot;Dietary Preferences (Optional):&quot;, [None, &quot;Vegetarian&quot;, &quot;Vegan&quot;, &quot;Gluten-Free&quot;])<br><br>submit_button = st.button(&quot;Suggest me recipe&quot;)<br>user_ingredients = user_ingred.split(&quot;, &quot;)<br><br><br>if submit_button:<br>    if user_ingred is not None:<br>    # Preprocess user ingredients<br>        user_ingredients_str = [ingredient.strip().lower() for ingredient in user_ingred.split(&quot;, &quot;)]<br>        <br>        # Call match_ingredients once and assign results<br>        recipe =  match_ingredients(user_ingredients_str, dietary_preferences=dietary_options)<br><br>        if recipe:<br>            complete_recipe = f&quot;\n{recipe}\n&quot;<br>            st.write(complete_recipe.replace(&#39;\\n&#39;, &#39;\n&#39;))<br>        else:<br>            st.write(&quot;No Recipe&quot;)<br>    <br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br>st.text(&quot; \n&quot;)<br><br>footer=&quot;&quot;&quot;&lt;style&gt;<br>a:link , a:visited{<br>color: yellow;<br>background-color: transparent;<br>text-decoration: underline;<br>}<br><br>a:hover,  a:active {<br>color: red;<br>background-color: transparent;<br>text-decoration: underline;<br>}<br><br>.footer {<br>position: Bottom;<br>left: 0;<br>bottom: 0;<br>width: 100%;<br>background-color: transparent;<br>color: white;<br>text-align: center;<br>}<br>&lt;/style&gt;<br>&lt;div class=&quot;footer&quot;&gt;<br>&lt;p&gt;Developed with ❤ by&lt;a style=&#39;display: block; text-align: center;&#39; href=&quot;https://github.com/Gitesh08&quot; target=&quot;_blank&quot;&gt;Gitesh Mahadik&lt;/a&gt;&lt;/p&gt;<br>&lt;/div&gt;<br>&quot;&quot;&quot;<br>st.markdown(footer,unsafe_allow_html=True)</pre><p>Create file <strong>requirements.txt</strong> and paste below code.</p><pre>streamlit<br>google-generativeai<br>python-dotenv<br>langchain</pre><p>2. Create a Python virtual environment. Open a new terminal of your editor and paste below command.</p><pre>python -m virtualenv .</pre><p>3. Activate virtual environment. Paste below command in terminal.</p><pre>.\scripts\activate</pre><p>4. Install the required dependencies.</p><pre>pip install -r requirements.txt</pre><p>5. Generate Gemini Pro API Key:</p><p><a href="https://ai.google.dev/">Build with the Gemini API | Google AI for Developers</a></p><p>6. Create <strong>.env</strong> file and define your API Key.</p><pre>GOOGLE_API_KEY = &quot;Replace with your API Key&quot;</pre><p>7. Run the application.</p><pre>streamlit run app.py</pre><p>Access the application through your web browser using the provided local address.</p><p><strong>Great! You have successfully built Recipe Mixer using the Gemini Pro LLM model.</strong></p><p><strong>Please give a star to this repo!</strong></p><p><a href="https://github.com/Gitesh08/Recipe-Mixer">GitHub - Gitesh08/Recipe-Mixer</a></p><h4>Usage</h4><ul><li><strong>Input your ingredients:</strong> Enter a list of ingredients you have on hand, separated by commas.</li><li><strong>Specify dietary preferences (optional):</strong> Select your dietary preferences from the dropdown menu.</li><li>Click the <strong>“Suggest me recipe”</strong> button to receive recipe suggestions.</li><li>Explore alternative ingredient suggestions and recipe options.</li><li>Enjoy experimenting with different recipes and ingredients!</li></ul><p><strong>Contributions are welcome! If you have any suggestions, enhancements, or bug fixes, feel free to open an issue or submit a pull request.</strong></p><p>Thank you for reading! I hope you enjoyed this article. If you did, please consider subscribing to my Medium publication. You can also follow me on <a href="https://www.linkedin.com/in/gitesh-mahadik%E2%98%81%EF%B8%8F-7487961a0/">LinkedIn</a> for more updates.</p><p>If you have any questions or feedback, please feel free to leave a comment below. I would love to hear from you!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=88fe60836e43" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/recipe-mixer-88fe60836e43">Recipe Mixer🧑🏻‍🍳</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Calling Gemma with Ollama, TestContainers, and LangChain4j]]></title>
            <link>https://medium.com/google-cloud/calling-gemma-with-ollama-testcontainers-and-langchain4j-fbfe220ca715?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/fbfe220ca715</guid>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[ollama]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[testcontainer]]></category>
            <category><![CDATA[langchain4j]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 05 Apr 2024 04:50:25 GMT</pubDate>
            <atom:updated>2024-04-05T04:50:25.157Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MaT34TRoitiU1RMM.jpg" /></figure><p>Lately, for my Generative AI powered Java apps, I’ve used the <a href="https://deepmind.google/technologies/gemini/#introduction">Gemini</a> multimodal large language model from Google. But there’s also <a href="https://blog.google/technology/developers/gemma-open-models/">Gemma</a>, its little sister model.</p><p>Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Gemma is available in two sizes: 2B and 7B. Its weights are freely available, and its small size means you can run it on your own, even on your laptop. So I was curious to give it a run with <a href="https://docs.langchain4j.dev/">LangChain4j</a>.</p><h3>How to run Gemma</h3><p>There are many ways to run Gemma: in the cloud, via <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335">Vertex AI</a> with a click of a button, or <a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm">GKE</a> with some GPUs, but you can also run it locally with <a href="https://github.com/tjake/Jlama">Jlama</a> or <a href="https://github.com/google/gemma.cpp">Gemma.cpp</a>.</p><p>Another good option is to run Gemma with <a href="https://ollama.com/">Ollama</a>, a tool that you install on your machine, and which lets you run small models, like Llama 2, Mistral, and <a href="https://ollama.com/library">many others</a>. They quickly added support for <a href="https://ollama.com/library/gemma">Gemma</a> as well.</p><p>Once installed locally, you can run:</p><pre>ollama run gemma:2b<br>ollama run gemma:7b</pre><p>Cherry on the cake, the <a href="https://glaforge.dev/posts/2024/04/04/calling-gemma-with-ollama-and-testcontainers/">LangChain4j</a> library provides an <a href="https://docs.langchain4j.dev/integrations/language-models/ollama">Ollama module</a>, so you can plug Ollama supported models in your Java applications easily.</p><h3>Containerization</h3><p>After a great discussion with my colleague <a href="https://twitter.com/ddobrin">Dan Dobrin</a> who had worked with Ollama and TestContainers (<a href="https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp/blob/main/sessions/next24/books-genai-vertex-langchain4j/src/test/java/services/OllamaContainerTest.java">#1</a> and<a href="https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp/blob/main/sessions/next24/books-genai-vertex-langchain4j/src/test/java/services/OllamaChatModelTest.java#L37">#2</a>) in his <a href="https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp/tree/main">serverless production readiness workshop</a>, I decided to try the approach below.</p><p>Which brings us to the last piece of the puzzle: Instead of having to install and run Ollama on my computer, I decided to use Ollama within a container, handled by <a href="https://testcontainers.com/">TestContainers</a>.</p><p>TestContainers is not only useful for testing, but you can also use it for driving containers. There’s even a specific <a href="https://java.testcontainers.org/modules/ollama/">OllamaContainer</a> you can take advantage of!</p><p>So here’s the whole picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lHxJaKf0ALEEnoJS.png" /></figure><h3>Time to implement this approach!</h3><p>You’ll find the code in the Github <a href="https://github.com/glaforge/gemini-workshop-for-java-developers/blob/main/app/src/main/java/gemini/workshop/CallGemma.java">repository</a> accompanying my recent <a href="https://codelabs.developers.google.com/codelabs/gemini-java-developers">Gemini workshop</a></p><p>Let’s start with the easy part, interacting with an Ollama supported model with LangChain4j:</p><pre>OllamaContainer ollama = createGemmaOllamaContainer();<br>ollama.start();<br><br>ChatLanguageModel model = OllamaChatModel.builder()<br>    .baseUrl(String.format(&quot;http://%s:%d&quot;, ollama.getHost(), ollama.getFirstMappedPort()))<br>    .modelName(&quot;gemma:2b&quot;)<br>    .build();<br><br>String response = model.generate(&quot;Why is the sky blue?&quot;);<br><br>System.out.println(response);</pre><ul><li>You run an Ollama test container.</li><li>You create an Ollama chat model, by pointing at the address and port of the container.</li><li>You specify the model you want to use.</li><li>Then, you just need to call model.generate(yourPrompt) as usual.</li></ul><p>Easy? Now let’s have a look at the trickier part, my local method that creates the Ollama container:</p><pre>// check if the custom Gemma Ollama image exists already<br>List&lt;Image&gt; listImagesCmd = DockerClientFactory.lazyClient()<br>    .listImagesCmd()<br>    .withImageNameFilter(TC_OLLAMA_GEMMA_2_B)<br>    .exec();<br><br>if (listImagesCmd.isEmpty()) {<br>    System.out.println(&quot;Creating a new Ollama container with Gemma 2B image...&quot;);<br>    OllamaContainer ollama = new OllamaContainer(&quot;ollama/ollama:0.1.26&quot;);<br>    ollama.start();<br>    ollama.execInContainer(&quot;ollama&quot;, &quot;pull&quot;, &quot;gemma:2b&quot;);<br>    ollama.commitToImage(TC_OLLAMA_GEMMA_2_B);<br>    return ollama;<br>} else {<br>    System.out.println(&quot;Using existing Ollama container with Gemma 2B image...&quot;);<br>    // Substitute the default Ollama image with our Gemma variant<br>    return new OllamaContainer(<br>        DockerImageName.parse(TC_OLLAMA_GEMMA_2_B)<br>            .asCompatibleSubstituteFor(&quot;ollama/ollama&quot;));<br>}</pre><p>You need to create a derived Ollama container that pulls in the Gemma model. Either this image was already created beforehand, or if it doesn’t exist yet, you create it.</p><p>Use the Docker Java client to check if the custom Gemma image exists. If it doesn’t exist, notice how TestContainers let you create an image derived from the base Ollama image, pull the Gemma model, and then commit that image to your local Docker registry.</p><p>Otherwise, if the image already exists (ie. you created it in a previous run of the application), you’re just going to tell TestContainers that you want to substitute the default Ollama image with your Gemma-powered variant.</p><h3>And voila!</h3><p>You can <strong>call Gemma locally on your laptop, in your Java apps, using LangChain4j</strong>, without having to install and run Ollama locally (but of course, you need to have a Docker daemon running).</p><p>Big thanks to <a href="https://twitter.com/ddobrin">Dan Dobrin</a> for the approach, and to <a href="https://twitter.com/bsideup">Sergei</a>, <a href="https://twitter.com/EdduMelendez">Eddú</a> and <a href="https://twitter.com/shelajev">Oleg</a> from TestContainers for the help and useful pointers.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/04/04/calling-gemma-with-ollama-and-testcontainers/"><em>https://glaforge.dev</em></a><em> on April 3, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fbfe220ca715" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/calling-gemma-with-ollama-testcontainers-and-langchain4j-fbfe220ca715">Calling Gemma with Ollama, TestContainers, and LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Build Infrastructure on Google Cloud with Terraform — Google Challenge Lab Walkthrough]]></title>
            <link>https://medium.com/google-cloud/build-infrastructure-on-google-cloud-with-terraform-google-challenge-lab-walkthrough-30a592373d3e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/30a592373d3e</guid>
            <category><![CDATA[challenge-lab]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[terraform]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[terraform-import]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Fri, 05 Apr 2024 04:49:54 GMT</pubDate>
            <atom:updated>2024-04-05T04:49:54.155Z</atom:updated>
            <content:encoded><![CDATA[<h3>Build Infrastructure on Google Cloud with Terraform — Google Challenge Lab Walkthrough</h3><p>This is a walkthrough of the <a href="https://partner.cloudskillsboost.google/focuses/16515?parent=catalog">challenge lab</a> from the course <a href="https://www.cloudskillsboost.google/course_templates/636">Build Infrastructure with Terraform on Google Cloud</a>.</p><p>This lab tests your ability to:</p><ul><li>Import existing infrastructure into your Terraform configuration.</li><li>Build and reference your own Terraform modules.</li><li>Add a remote backend to your configuration.</li><li>Use and implement a module from the Terraform Registry.</li><li>Re-provision, destroy, and update infrastructure.</li><li>Test connectivity between the resources you’ve created.</li></ul><h3>Intro to Challenge Labs</h3><p>Google provides an online learning platform called Google <a href="https://www.cloudskillsboost.google/">Cloud Skills Boost</a>, formerly known as QwikLabs. On this platform, you can follow training courses aligned to learning paths, to particular products, or for particular solutions.</p><p>One type of learning experience on this platform is called a <strong>quest</strong>. This is where you complete a number of guided hands-on labs, and then finally complete a <strong>Challenge Lab</strong>. The <strong>challenge lab</strong> differs from the other labs in that goals are specified, but very little guidance on <em>how</em> to achieve the goals is given.</p><p>I occasionally create walkthroughs of these challenge labs. The goal is not to help you cheat your way through the challenge labs! But rather:</p><ul><li>To show you what I believe to be an ideal route through the lab.</li><li>To help you with particular gotchas or blockers that are preventing you from completing the lab on your own.</li></ul><p>If you’re looking for help with challenge lab, then you’ve come to the right place. But I strongly urge you to work your way through the quest first, and to try the lab on your own, before reading further!</p><p>With all these labs, there are always many ways to go about solving the problem. I generally like to solve them using the Cloud Shell, since I can then document a more repeatable and programmatic approach. But of course, you can use the Cloud Console too.</p><h3>Overview of this Lab</h3><p>In this lab we’re expected to use Terraform to create, deploy and manage infrastructure on Google Cloud. We also need to import some mismanaged instances into our configuration and fix them.</p><h3>My Solution</h3><p>Let’s start by defining some variables we can use throughout this challenge. The actual variables will be provided to you when you start the lab.</p><pre>gcloud auth list<br><br>region=&lt;ENTER REGION&gt;<br>zone=&lt;ENTER ZONE&gt;<br>prj=&lt;ENTER PRJ ID&gt;</pre><h4>Task 1 — Create the Configuration Files</h4><p>We’re told to create this folder structure:</p><pre>main.tf<br>variables.tf<br>modules/<br>└── instances<br>|   ├── instances.tf<br>|   ├── outputs.tf<br>|   └── variables.tf<br>└── storage<br>    ├── storage.tf<br>    ├── outputs.tf<br>    └── variables.tf</pre><p>We can do it like this:</p><pre># Create main.tf and variables.tf in the root directory<br>touch main.tf variables.tf<br><br># Create main directory and its files<br>mkdir -p modules/instances<br>mkdir modules/storage<br><br># Create the required files in the &#39;instances&#39; module directory<br>touch modules/instances/instances.tf<br>touch modules/instances/outputs.tf<br>touch modules/instances/variables.tf<br><br># Create the required files in the &#39;storage&#39; module directory<br>touch modules/storage/storage.tf<br>touch modules/storage/outputs.tf<br>touch modules/storage/variables.tf</pre><p>Now we update the variables.tf files to contain these variables:</p><pre>variable &quot;region&quot; {<br>  description = &quot;The Google Cloud region&quot;<br>  type        = string<br>  default     = &quot;Lab-supplied region&quot;<br>}<br><br>variable &quot;zone&quot; {<br>  description = &quot;The Google Cloud zone&quot;<br>  type        = string<br>  default     = &quot;Lab-supplied zone&quot;<br>}<br><br>variable &quot;project_id&quot; {<br>  description = &quot;The ID of the project in which to provision resources.&quot;<br>  type        = string<br>  default     = &quot;Your project ID&quot;<br>}</pre><p>Update the root module main.tf to include the Google Cloud Provider, which you can always look up in the <a href="https://registry.terraform.io/providers/hashicorp/google/latest/docs">Terraform Registry</a>. We’re asked to include all three of our variables in our provider block.</p><pre>terraform {<br>  required_providers {<br>    google = {<br>      source = &quot;hashicorp/google&quot;<br>    }<br>  }<br>}<br><br>provider &quot;google&quot; {<br>  project     = var.project_id<br>  region      = var.region<br>  zone        = var.zone<br>}</pre><p>Now we need to initialise Terraform. So run this command:</p><pre>terraform init</pre><h4>Task 2 — Import Infrastructure</h4><p>Here, the goal is to bring infrastructure under Terraform control, that has thus far been provisioned outside of Terraform.</p><p>We’re going to use the Terraform import workflow:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/737/0*cLTWjcD7L2rsBcRO.png" /><figcaption>Terraform import workflow</figcaption></figure><p>These are the import steps:</p><ol><li>Identify the existing infrastructure to be imported.</li><li>Import the infrastructure into your <strong>Terraform state</strong>.</li><li>Write a <strong>Terraform configuration</strong> that matches that infrastructure.</li><li><strong>Review the Terraform plan</strong> to ensure that the configuration matches the expected state and infrastructure.</li><li><strong>Apply </strong>the configuration to update your Terraform state.</li></ol><h4>Identify the existing infrastructure to be imported</h4><p>Two GCE instances have already been created. Examine one of the existing instances, tf-instance-1 in the Cloud Console. We want to retrieve:</p><ul><li>Network</li><li>Machine type</li><li>Disk</li></ul><p>Next we need to include two calls to our instances module in our main.tf. They will contain empty definitions, so that we can import.</p><pre>module &quot;tf_instance_1&quot; {<br>  source        = &quot;./modules/instances&quot;<br>  instance_name = &quot;tf-instance-1&quot;<br>  zone          = var.zone<br>  region        = var.region<br>}<br><br>module &quot;tf_instance_2&quot; {<br>  source        = &quot;./modules/instances&quot;<br>  instance_name = &quot;tf-instance-2&quot;<br>  zone          = var.zone<br>  region        = var.region<br>}</pre><p>Remember that each module definition must have a unique label.</p><p>Now initialise:</p><pre>terraform init</pre><p>Now we write the <a href="https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance">module configurations</a> in instances.tf. We’re told the arguments that need to be included in our minimal configuration:</p><pre>resource &quot;google_compute_instance&quot; &quot;instance&quot; {<br>  name         = var.instance_name<br>  machine_type = &quot;hard code from existing instance&quot;<br>  zone         = var.zone<br><br>  boot_disk {<br>    initialize_params {<br>      # image = &quot;debian-cloud/debian-11&quot;<br>      image = &quot;hard code from existing instance&quot;<br>    }<br>  }<br><br>  network_interface {<br>    # network = &quot;default&quot;<br>    network = &quot;hard code from existing instance&quot;<br>    access_config {<br>      // Ephemeral public IP<br>    }<br>  }<br><br>  metadata_startup_script = &lt;&lt;-EOT<br>          #!/bin/bash<br>      EOT<br>  allow_stopping_for_update = true<br>}</pre><p>Update variables.tf in the instance module, so we can pass in the instance_name:</p><pre>variable &quot;instance_name&quot; {<br>  description = &quot;The name of the instance.&quot;<br>  type        = string<br>}</pre><h4>Import the Existing Infrastructure into Terraform State</h4><pre>terraform import module.tf_instance_1.google_compute_instance.instance \<br>  projects/$prj/zones/$zone/instances/tf-instance-1<br><br>terraform import module.tf_instance_2.google_compute_instance.instance \<br>  projects/$prj/zones/$zone/instances/tf-instance-2<br><br># verify the import<br>terraform show</pre><p>The import should look like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1YafhrvtKcZG7MrQ.png" /><figcaption>terraform import</figcaption></figure><h4>Plan and Apply</h4><p>Now we update the instances in-place by running the apply:</p><pre>terraform plan<br>terraform apply</pre><h4>Task 3 — Configure a Remote Backend</h4><p>This is pretty easy. These are standard steps that you would run whenever we want to store Terraform state in a remote GCS backend:</p><ol><li>Provision a GCS bucket with Terraform.</li><li>Add a backend block that points to the new GCS bucket.</li><li>Reinitialise Terraform and migrate the state from the local state file to the remote backend.</li></ol><h4>Provision the GCS Bucket</h4><p>Add this resource definition to main.tf:</p><pre>resource &quot;google_storage_bucket&quot; &quot;test-bucket-for-state&quot; {<br>  name        = &quot;Bucket Name You Are Given&quot;<br>  location    = &quot;US&quot;<br>  uniform_bucket_level_access = true<br><br>  force_destroy = true<br>}</pre><p>And apply:</p><pre>terraform apply</pre><h4>Add the GCS Backend</h4><p>Modify main.tf and include the backend in the terraform block:</p><pre>terraform {<br>  backend &quot;gcs&quot; {<br>    bucket  = var.project_id<br>    prefix  = &quot;terraform/state&quot;<br>  }<br>}</pre><h4>Migrate the State</h4><p>This is where we migrate the Terraform state from the local state file into the GCS backend:</p><pre>terraform init -migrate-state</pre><p>It will ask you to confirm you want to migrate the state:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qJ3EoohrCM8M5h7V.png" /><figcaption>Migrating Terraform state</figcaption></figure><h4>Task 4 — Modify and Update the Infrastructure</h4><p>We need to update variables.tf to include a machine_type:</p><pre>variable &quot;machine_type&quot; {<br>  description = &quot;The machine type of an instance&quot;<br>  type        = string<br>  default     = &quot;e2-standard-2&quot;<br>}</pre><p>Then we need to modify instance.tf so that it can accept a machine_type parameter:</p><pre>resource &quot;google_compute_instance&quot; &quot;instance&quot; {<br>  name         = var.instance_name<br>  machine_type = var.machine_type<br>  zone         = var.zone<br><br>  ...</pre><p>Lastly, we need to modify main.tf such that we add the specified third instance to our main.tf, by calling the module for a third time. We don’t need to pass in the machine_type, as we’ve already set it to have a default.</p><p>Now initialise (because we’ve added another module instance) and apply.</p><pre>terraform init<br>terraform apply</pre><h4>Task 5 — Destroy Resources</h4><p>Now we remove the instance we previously added. Remove the call to this module from main.tf, then reapply:</p><pre>terraform init<br>terraform apply</pre><h4>Task 6 — Use a Module from the Registry</h4><p>We’re going to use the <a href="https://registry.terraform.io/modules/terraform-google-modules/network/google/6.0.0">Google Network Module</a>.</p><pre>module &quot;network&quot; {<br>  source  = &quot;terraform-google-modules/network/google&quot;<br>  version = &quot;6.0.0&quot;<br><br>  project_id   = var.project_id<br>  network_name = &quot;Use Supplied VPC Name&quot;<br>  routing_mode = &quot;GLOBAL&quot;<br><br>  subnets = [<br>    {<br>      subnet_name           = &quot;subnet-01&quot;<br>      subnet_ip             = &quot;10.10.10.0/24&quot;<br>      subnet_region         = var.region<br>    },<br>    {<br>      subnet_name           = &quot;subnet-02&quot;<br>      subnet_ip             = &quot;10.10.20.0/24&quot;<br>      subnet_region         = var.region<br>    }<br>  ]<br>}</pre><p>Initialise and apply:</p><pre>terraform init<br>terraform apply</pre><p>Update instances module to take a network parameter and a subnet parameter.</p><p>In variables.tf:</p><pre>variable &quot;network&quot; {<br>  description = &quot;The network&quot;<br>  type        = string<br>}<br><br>variable &quot;subnet&quot; {<br>  description = &quot;The subnet&quot;<br>  type        = string<br>}</pre><p>In instance.tf:</p><pre>network_interface {<br>  network = var.network<br>  subnetwork = var.subnet<br><br>  access_config {<br>    // Ephemeral public IP<br>  }<br>}</pre><p>Then update main.tf to create the instances like this:</p><pre>module &quot;tf_instance_1&quot; {<br>  source        = &quot;./modules/instances&quot;<br>  instance_name = &quot;tf-instance-1&quot;<br>  zone          = var.zone<br>  region        = var.region<br><br>  network       = module.network.network_name<br>  subnet        = &quot;subnet-01&quot;<br>}<br><br>module &quot;tf_instance_2&quot; {<br>  source        = &quot;./modules/instances&quot;<br>  instance_name = &quot;tf-instance-2&quot;<br>  zone          = var.zone<br>  region        = var.region<br>  network       = module.network.network_name<br>  subnet        = &quot;subnet-02&quot;<br>}</pre><pre>terraform init<br>terraform apply</pre><h4>Task 7 — Add a Firewall</h4><p>Update main.tf:</p><pre>resource &quot;google_compute_firewall&quot; &quot;default&quot; {<br>  name          = &quot;tf-firewall&quot;<br>  network       = module.network.network_name<br>  direction     = &quot;INGRESS&quot;<br>  source_ranges = [&quot;0.0.0.0/0&quot;]<br><br>  allow {<br>    protocol = &quot;tcp&quot;<br>    ports    = [&quot;80&quot;]<br>  }<br>}</pre><p>And one last apply…</p><pre>terraform apply</pre><p>And we’re done!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30a592373d3e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/build-infrastructure-on-google-cloud-with-terraform-google-challenge-lab-walkthrough-30a592373d3e">Build Infrastructure on Google Cloud with Terraform — Google Challenge Lab Walkthrough</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using AlloyDB Go connector for automatic IAM authentication (service account)]]></title>
            <link>https://medium.com/google-cloud/using-alloydb-connector-for-automatic-iam-authentication-service-account-ec29c4ee5d2b?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ec29c4ee5d2b</guid>
            <category><![CDATA[go]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[postgres]]></category>
            <category><![CDATA[alloydb]]></category>
            <dc:creator><![CDATA[Harinderjit Singh]]></dc:creator>
            <pubDate>Fri, 05 Apr 2024 04:49:36 GMT</pubDate>
            <atom:updated>2024-04-08T16:54:53.778Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>Introduction</strong></h3><p><a href="https://cloud.google.com/alloydb#documentation">AlloyDB</a> is a fully managed PostgreSQL compatible database service for your most demanding enterprise workloads. AlloyDB combines the best of Google with PostgreSQL, for superior performance, scale, and availability.</p><p>Since AlloyDB is PostgreSQL compatible, you can use <a href="https://github.com/jackc/pgx/">pgxpool</a> and <a href="https://github.com/brettwooldridge/HikariCP/blob/dev/README.md">HikariCP</a> for connection pooling in “Go” and “Java” applications respectively.</p><p>There are two prominent ways to <strong>connect securely to AlloyDB:</strong></p><ul><li>AlloyDB Auth Proxy</li><li>AlloyDB Language connectors</li></ul><h4>AlloyDB Auth Proxy</h4><ul><li><strong>IAM-based connection authorization (AuthZ):</strong> The Auth Proxy uses the credentials and permissions of an IAM principal to authorize connections to AlloyDB instances.</li><li><strong>Secure, encrypted communication:</strong> The Auth Proxy automatically creates, uses, and maintains a TLS 1.3 connection using a 256-bit AES cipher between your client and an AlloyDB instance.</li></ul><h4>AlloyDB Language connectors</h4><p>AlloyDB connectors are the language specific libraries for connecting securely to your AlloyDB instances. Using an AlloyDB connector provides the following additional benefits (besides the ones provided by AlloyDB Auth Proxy) :</p><ul><li><strong>Convenience</strong>: removes the requirement to use and distribute SSL certificates, as well as manage firewalls or source/destination IP addresses. <strong>You also don’t need to manage a separate Auth proxy container or process.</strong></li><li>(optionally) <strong>IAM DB Authentication</strong>: provides support for <a href="https://github.com/GoogleCloudPlatform/alloydb-go-connector?tab=readme-ov-file#automatic-iam-database-authentication">AlloyDB’s automatic IAM DB AuthN</a> feature. That means <strong>you can configure service accounts to connect to database</strong>. For applications deployed on GKE you can use workload identity to authenticate to the backend AlloyDB database.</li></ul><p>AlloyDB Language connectors are available for Go, Java and Python at this time.</p><h3>Purpose</h3><p>If you’re deploying a Go application on GCE/GKE and want to streamline secure connections to your AlloyDB database, the AlloyDB Go connector with IAM authentication is the way to go.</p><p>In this post I will walk you through the process of configuring your application and AlloyDB Instance for using AlloyDB Go connector such that your application can use a service account to connect to AlloyDB database. You don’t need to configure any vault to store the DB user password, no need to store any keyfile for the service account.</p><p>We are considering a hypothetical Go application (<a href="https://github.com/bijeshos/go-postgresql-pgx-example/blob/main/main.go">example</a>) which will be deployed on GCE for this article.</p><h3>Assumptions</h3><ol><li>GCP Project is already created</li><li>VPC network exists for this project and has a subnet defined for GCE/GKE</li><li>AlloyDB, Service Networking, Compute APIs are enabled</li></ol><h3>Steps</h3><h4>Create a service account</h4><p>This is the service account which will be configured as AlloyDB user and will be used by application to connect to database.</p><pre>read -p &quot;project_id: &quot; PROJECT_ID<br>read -p &quot;region: &quot; REGION<br>read -p &quot;serviceaccount: &quot; SERVICEACCOUNT<br>gcloud iam service-accounts create ${SERVICEACCOUNT} --display-name=&quot;alloydb service-account&quot; --project ${PROJECT_ID}</pre><h4>Add roles to the service account</h4><p>As per <a href="https://cloud.google.com/alloydb/docs/manage-iam-authn#role,">https://cloud.google.com/alloydb/docs/manage-iam-authn#role,</a> we need to assign roles alloydb.client, alloydb.databaseUser and serviceusage.serviceUsageConsumer to the service account.</p><pre>gcloud projects add-iam-policy-binding ${PROJECT_ID} --member=&#39;serviceAccount:${SERVICEACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com&#39; --role=&#39;roles/alloydb.client&#39;<br>gcloud projects add-iam-policy-binding ${PROJECT_ID} --member=&#39;serviceAccount:${SERVICEACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com&#39; --role=&#39;roles/alloydb.databaseUser&#39;<br>gcloud projects add-iam-policy-binding ${PROJECT_ID} --member=&#39;serviceAccount:${SERVICEACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com&#39; --role=&#39;roles/serviceusage.serviceUsageConsumer&#39;</pre><h4>Create private service access</h4><p>Use Private Services Access to connect to AlloyDB service.</p><p>Private services access requires you to first allocate an internal IPv4 address range and then create a private connection</p><pre>read -p &quot;region : &quot; REGION<br>read -p &quot;projectid : &quot; PROJECT_ID<br>read -p &quot;postgres_password: &quot; PASSWORD<br>read -p &quot;vpc network: &quot; VPC_NETWORK<br><br>gcloud compute addresses create alloydbpsa \<br>    --global \<br>    --purpose=VPC_PEERING \<br>    --prefix-length=16 \<br>    --description=&quot;Private service access&quot; \<br>    --network=$VPC_NETWORK \<br> --project ${PROJECT_ID}</pre><h4>Create VPC private connection to alloydb service</h4><p>Private connection enables private access for AlloyDB Instances</p><pre>gcloud services vpc-peerings connect \<br>     --service=servicenetworking.googleapis.com \<br>     --ranges=alloydbpsa \<br>     --network=$VPC_NETWORK  \<br>--project ${PROJECT_ID}</pre><h4>Create AlloyDB Cluster and Primary Instance</h4><p>Below commands will create an AlloyDB Cluster and Instance. Please update the cluster and Instance names as per requirements.</p><pre>read -p &quot;region : &quot; REGION<br>read -p &quot;projectid : &quot; PROJECT_ID<br>read -p &quot;postgres_password: &quot; PASSWORD<br>read -p &quot;vpc network: &quot; VPC_NETWORK<br>gcloud alloydb clusters create alloydb-cls-$(date +%d%m%Y) \<br>--region=${REGION} --password=$PASSWORD --network=${VPC_NETWORK} \<br>--project=${PROJECT_ID}<br><br>gcloud beta alloydb instances create alloydb-ins-primary-$(date +%d%m%Y) \<br>--cluster=alloydb-cls-$(date +%d%m%Y)  --region=${REGION} \<br>--instance-type=PRIMARY --cpu-count=2 \<br>--database-flags=alloydb.iam_authentication=on,alloydb.enable_auto_explain=on \<br>--availability-type=ZONAL --project=${PROJECT_ID}</pre><p>Notice that AlloyDB Instance has flag <strong>alloydb.iam_authentication</strong> set to on. This flag enables IAM authentication on an AlloyDB instance. If you already have an AlloyDB Instance, you can use below command to enable this flag.</p><pre>gcloud alloydb instances update $INSTANCE --cluster=$CLUSTER \<br> --region=$REGION --database-flags=alloydb.iam_authentication=on</pre><h4>Create a AlloyDB user with name same as the service account</h4><p>Username must be a same as the service account leaving the suffix “.gserviceaccount.com” and authentication type must be “IAM_BASED”.</p><pre>read -p &quot;serviceaccount: &quot; SERVICEACCOUNT<br>gcloud alloydb users create ${SERVICEACCOUNT}@${PROJECT_ID}.iam \<br>--cluster=alloydb-cls-$(date +%d%m%Y) --type=IAM_BASED \<br>--region=${REGION} --project=${PROJECT_ID}</pre><h4>Create Application schema and grant appropriate roles</h4><p>Connect to AlloyDB database postgres using a BUILTIN user (postgres) to create “application” database and grant appropriate roles to AlloyDB IAM user on “application” database.</p><p>To connect you may use AlloyDB SQL studio or psql.</p><pre>create database application;<br>---replace the ${SERVICEACCOUNT}@${PROJECT_ID}.iam with actual SA<br>grant all privileges on database application to &quot;${SERVICEACCOUNT}@${PROJECT_ID}.iam&quot;;</pre><h4>Create GCE Instance</h4><p>Create GCE spot Instance (experimentation purposes only) where our application will be deployed.</p><pre>read -p &quot;GCE subnet:&quot; GCE_SUBNET<br>gcloud compute instances create instance-$(date +%d%m%Y) \<br>--project=${PROJECT_ID} --zone=${REGION}-a --machine-type=e2-medium \<br>--network-interface=network-tier=PREMIUM,stack-type=IPV4_ONLY,subnet=$GCE_SUBNET \<br>--provisioning-model=SPOT --service-account=${SERVICEACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com \<br>--scopes=https://www.googleapis.com/auth/cloud-platform \<br>--create-disk=auto-delete=yes,boot=yes,device-name=instance-$(date +%d%m%Y)-230433,image=projects/debian-cloud/global/images/debian-12-bookworm-v20240213,mode=rw,size=10,type=projects/${PROJECT_ID}/zones/${REGION}-a/diskTypes/pd-balanced \<br>--no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring \<br>--labels=goog-ec-src=vm_add-gcloud --reservation-affinity=any \<br>--preemptible --metadata=startup-script=&#39;#! /bin/bash<br>  apt update -y<br>  apt -y install golang unzip git&#39;</pre><p>Notice that we bind the service account (DB User Service Account) that we created earlier to the GCE VM.</p><p>Now any process running on GCE VM can use that service account to authenticate to allowed APIs using assigned roles.</p><h4>Connecting to postgres using pgx pool (no connector)</h4><p>To make a connection to your database you would typically be using a function such as below. This uses <a href="https://pkg.go.dev/github.com/jackc/pgx/v5/pgxpool#pkg-overview">pgxpool package</a> to create a connection pool.</p><pre>func connectPostgres() (*pgxpool.Pool, error) {<br> ctx := context.Background()<br> var (<br>  dsn      string<br>  dbname   = os.Getenv(&quot;DBNAME&quot;)<br>  user     = os.Getenv(&quot;DBUSER&quot;)<br>  host     = os.Getenv(&quot;PGHOSTNAME&quot;)<br>  password = os.Getenv(&quot;PGPASSWORD&quot;)<br>  err      error<br> )<br><br> dsn = fmt.Sprintf(<br>  //user=jack password=secret host=pg.example.com port=5432 dbname=mydb sslmode=verify-ca pool_max_conns=10<br>  // connection instead.<br>  &quot;user=%s password=%s dbname=%s sslmode=disable host=%s&quot;,<br>  user, password, dbname, host,<br> )<br> config, err := pgxpool.ParseConfig(dsn)<br> if err != nil {<br>  return nil, fmt.Errorf(&quot;failed to parse pgx config: %v&quot;, err)<br> }<br> // Establish the connection.<br> pool, connErr := pgxpool.NewWithConfig(ctx, config)<br> if connErr != nil {<br>  return nil, fmt.Errorf(&quot;failed to connect: %s&quot;, connErr)<br> }<br> return pool, nil<br><br>}</pre><p>This requires you to define environment variables DBNAME, DBUSER, PGHOSTNAME and PGPASSWORD.</p><p>If you want to encrypt the data in transit, you have to configure sslmode and configure TLS certificates to encrypt the data in transit. You will need to manage the certificates on the application host.</p><h4>Connecting to postgres using pgx pool and Go Connector</h4><p>Update the definition of connectPostgres() in your application to the below. File containing the function definition must have “cloud.google.com/go/alloydbconn” in import section.</p><pre>func connectPostgres() (*pgxpool.Pool, error) {<br> ctx := context.Background()<br>// export DBNAME=application<br>// export DBUSER=${SERVICEACCOUNT}@${PROJECT_ID}.iam<br>// export INSTURI=projects/${PROJECT_ID}/locations/${REGION}/clusters/alloydb-cls--$(date +%d%m%Y)/instances/alloydb-ins-primary--$(date +%d%m%Y)<br> var (<br>  dsn     string<br>  dbname  = os.Getenv(&quot;DBNAME&quot;)<br>  user    = os.Getenv(&quot;DBUSER&quot;)<br>  instURI = os.Getenv(&quot;INSTURI&quot;)<br> )<br>// A Dialer can be configured to connect to an AlloyDB instance <br>// using automatic IAM database authentication with the WithIAMAuthN Option.<br> d, err := alloydbconn.NewDialer(ctx, alloydbconn.WithIAMAuthN())<br>if err != nil {<br>  return nil, fmt.Errorf(&quot;failed to init Dialer: %v&quot;, err)<br> }<br> dsn = fmt.Sprintf(<br>  // sslmode is disabled, because the Dialer will handle the SSL<br>  // connection instead.<br>  &quot;user=%s  dbname=%s sslmode=disable&quot;,<br>  user, dbname,<br> )<br> config, err := pgxpool.ParseConfig(dsn)<br> if err != nil {<br>  return nil, fmt.Errorf(&quot;failed to parse pgx config: %v&quot;, err)<br> }<br> // Tell pgx to use alloydbconn.Dialer to connect to the instance.<br> config.ConnConfig.DialFunc = func(ctx context.Context, _ string, _ string) (net.Conn, error) {<br>  return d.Dial(ctx, instURI)<br> }<br> // Establish the connection.<br> pool, connErr := pgxpool.NewWithConfig(ctx, config)<br> if connErr != nil {<br>  return nil, fmt.Errorf(&quot;failed to connect: %s&quot;, connErr)<br> }<br> return pool, nil<br>}</pre><p>This code excerpt is where the AlloyDB Go Connector is configured for pgxpool connection pool. <strong>alloydbconn.WithIAMAuthN()</strong> allows us to use IAM authentication when creating a connection.</p><p>We pass DBNAME, DBUSER and INSTURI as environment variables. “sslmode” is disabled, because the Dialer handles the SSL. You don’t need to manage any certificates and yet the data in transit is encrypted.</p><p>You would notice that we did not pass DB user password (PGPASSWORD) as parameter. Also we didn’t have to pass the IP for AlloyDB Instance as host, instead we used the Instance URI.</p><p>Below call to function connectPostgres() creates a connection pool.</p><pre> db, err := connectPostgres()<br> if err != nil {<br>  return err<br> }<br>// just an example<br> data, err = getEmployeesPG(db)<br> if err != nil {<br>  log.Printf(&quot;func getEmployeesPG: failed to get data: %v&quot;, err)<br>  return err<br> }</pre><p>Then we can use that connection to query the database in our application.</p><h4>Grant access to IAM user on application database</h4><p>Assuming there is a table called employees in this database “application”. We need to connect to the application databases as BUILTIN User and need to grant appropriate privileges to the IAM DB user.</p><pre><br>grant all privileges on table employees to &quot;${SERVICEACCOUNT}@${PROJECT_ID}.iam&quot;;</pre><p>You will need to manage the permissions on the relations as per your application requirements.</p><h4>Deploy and Execute the code on GCE VM</h4><p>This is just an example for demonstration.</p><pre>###Suppose you are in application directory<br>read -p &quot;region : &quot; REGION<br>read -p &quot;projectid : &quot; PROJECT_ID<br>read -p &quot;serviceaccount: &quot; SERVICEACCOUNT<br>export DBNAME=application<br>export DBUSER=${SERVICEACCOUNT}@${PROJECT_ID}.iam<br>export INSTURI=projects/${PROJECT_ID}/locations/${REGION}/clusters/alloydb-cls-$(date +%d%m%Y)/instances/alloydb-ins-primary-$(date +%d%m%Y)<br>go get cloud.google.com/go/alloydbconn<br>go get github.com/jackc/pgx/v5/pgxpool<br>go run ./main.go</pre><p>Application can successfully connect to database to read/write data.</p><h3>For applications deployed on GKE</h3><ul><li>You should have a valid docker container image of your application.</li><li>Workload identity must be enabled at the GKE cluster level.</li><li>Main difference is how the GKE uses Kubernetes service account to authenticate to AlloyDB Database on behalf of Google Service account which is also a Database user in this case. This is done using <a href="https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine#workload-identity">workload identity</a>.</li><li>We assign iam.workloadIdentityUser role to workload identity service account on your google service account.</li><li>This enables your Kubernetes service account (example alloydb-ksa) to retrieve the authentication token as your google service account (example alloydb-sa) which then can be used to authenticate to AlloyDB as IAM_BASED Database user.</li><li>This Kubernetes service account is then used by the application “deployment” kubernetes resource.</li><li>The environment variables such as DBNAME, DBUSER and INSTURI can be defined in a config map and then that config map can be used by the application “deployment” kubernetes resource.</li></ul><h3>Other Takeaways</h3><ul><li>If you application is written in Java, except the connection pool + Java connector code all steps are same. For connection pool using AlloyDB Java connector, you can use</li></ul><p><a href="https://github.com/GoogleCloudPlatform/alloydb-java-connector/blob/HEAD/alloydb-jdbc-connector/src/test/java/com/google/cloud/alloydb/AlloyDbJdbcConnectorDataSourceFactory.java">alloydb-java-connector/alloydb-jdbc-connector/src/test/java/com/google/cloud/alloydb/AlloyDbJdbcConnectorDataSourceFactory.java at 80864f2b1e3548f3dbfa87f87c95f42f79d363d5 · GoogleCloudPlatform/alloydb-java-connector</a></p><p>and set config.addDataSourceProperty(&quot;alloydbEnableIAMAuth&quot;, &quot;true&quot;);for Automatic IAM Authentication.</p><ul><li>If you desire to use Go connector to connect to your AlloyDB Instance but don’t want to use IAM Authentication, you can use <a href="https://github.com/GoogleCloudPlatform/alloydb-go-connector/blob/HEAD/pgxpool_test.go">https://github.com/GoogleCloudPlatform/alloydb-go-connector/blob/HEAD/pgxpool_test.go</a></li><li>You can configure AlloyDB to accept the connections only through connectors i.e <a href="https://cloud.google.com/alloydb/docs/enforce-connectors">connector enforcement</a>.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ec29c4ee5d2b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-alloydb-connector-for-automatic-iam-authentication-service-account-ec29c4ee5d2b">Using AlloyDB Go connector for automatic IAM authentication (service account)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Convert Soft Breaks to Hard Breaks on Google Documents using Google Apps Script]]></title>
            <link>https://medium.com/google-cloud/convert-soft-breaks-to-hard-breaks-on-google-documents-using-google-apps-script-4edfd7fef0c5?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/4edfd7fef0c5</guid>
            <category><![CDATA[google-document]]></category>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[google-docs]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:49:27 GMT</pubDate>
            <atom:updated>2024-04-03T04:49:27.459Z</atom:updated>
            <content:encoded><![CDATA[<h3>Description</h3><p>This script converts soft breaks to hard breaks in a Google Document using Google Apps Script.</p><h3>Usage</h3><p>Follow these steps:</p><h3>1. Create a New Google Document</h3><p>Create a new Google Document and open it. Go to “View” -&gt; “Show non-printing characters” in the top menu to see line breaks in the document body (as shown in the image below).</p><h3>2. Sample Script</h3><p>Copy and paste the following script into the script editor of your Google Document.</p><p>Important: Before using this script, enable the Google Docs API in Advanced Google services. <a href="https://developers.google.com/apps-script/guides/services/advanced#enable_advanced_services">Ref</a></p><pre>function myFunction() {<br>  const doc = DocumentApp.getActiveDocument();<br>  const requests = [<br>    { replaceAllText: { replaceText: &quot;\n&quot;, containsText: { text: &quot;\u000b&quot; } } },<br>  ];<br>  Docs.Documents.batchUpdate({ requests }, doc.getId());<br>}</pre><ul><li>The script searches for soft breaks using \u000b.</li><li>It replaces them with \n, which creates hard breaks.</li></ul><h3>Testing</h3><p>Running the script on a sample document with soft breaks will convert them to hard breaks as follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/0*e-8Yqw118TRcG9yo.png" /></figure><h3>Note</h3><ul><li>The soft breaks can be searched with findText(&quot;\\v&quot;). But, when replaceText(&quot;\\v&quot;, &#39;\n&#39;) is run, it seems that \n is used as the soft breaks. I’m not sure whether this is the current specification or a bug. From this situation, I thought that Google Docs API might be able to be used. But, it seems that Google Docs API cannot search the soft breaks with \v. So, I thought that \u000b might be able to be used.</li></ul><h3>References</h3><ul><li><a href="https://developers.google.com/docs/api/reference/rest/v1/documents/batchUpdate">Method: documents.batchUpdate</a></li><li><a href="https://developers.google.com/docs/api/reference/rest/v1/documents/request#replacealltextrequest">ReplaceAllTextRequest</a></li><li>Stack Overflow Thread: <a href="https://stackoverflow.com/q/78258654">https://stackoverflow.com/q/78258654</a> (original script post)</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4edfd7fef0c5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/convert-soft-breaks-to-hard-breaks-on-google-documents-using-google-apps-script-4edfd7fef0c5">Convert Soft Breaks to Hard Breaks on Google Documents using Google Apps Script</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>