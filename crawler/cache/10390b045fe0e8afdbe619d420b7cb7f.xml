<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 16 Apr 2024 19:00:06 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps…]]></title>
            <link>https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/c9ce870dcb99</guid>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[google-workspace]]></category>
            <category><![CDATA[google-spreadsheets]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-sheets]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 09:29:21 GMT</pubDate>
            <atom:updated>2024-04-16T09:29:21.906Z</atom:updated>
            <content:encoded><![CDATA[<h3>Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps Script</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*rqD6sT_95bXtlahR.jpg" /></figure><h3>Abstract</h3><p>Consolidate scattered cell references (A1Notation) in Google Sheets for efficiency. This script helps select cells by background color or update values/formats, overcoming limitations of large range lists.</p><h3>Introduction</h3><p>When working with Google Spreadsheets, there might be a scenario where you need to process scattered A1Notations (cell addresses in the format “A1”). This could involve selecting cells with specific background colors, updating cell values, or modifying cell formats.</p><p>One approach to handle scattered A1Notations is to create a range list containing the individual cell coordinates and activate it. However, this method becomes inefficient when dealing with a large number of cells due to the high processing cost associated with activating each cell individually.</p><p>To address this limitation, consolidating scattered A1Notations into continuous ranges offers a significant performance improvement. While a previous report discussed expanding consolidated A1Notations back into individual cells Ref: <a href="https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/">https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/</a>, consolidating them for processing efficiency had not been covered.</p><p>During the development of a script to achieve consolidation, it became apparent that existing solutions were not straightforward. To ensure clarity and facilitate debugging in the initial stages, the script was created by splitting each step into smaller, testable functions. While this approach might appear less elegant, it prioritizes understandability during the development process.</p><p>The provided script offers a solution for consolidating scattered A1Notations, as illustrated in the demonstration image. By consolidating the notations, the script can efficiently select cells with a specific background color, reducing the overall processing cost.</p><p>Furthermore, the script’s functionality can be extended to other use cases. For instance, it can be used to update the values or formats of scattered cells across the spreadsheet.</p><h3>Principle</h3><p>In this script, the process of consolidating A1Notations into rectangles is achieved by calculating the maximum rectangle size for all given A1Notations. This essentially combines scattered A1Notations into a single, most efficient rectangle.</p><p>The sample situation is as follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*53lMnCPL-j25cyO-.png" /></figure><p>The cells with a red background color are used in this example. When the A1Notations are retrieved from those cells, it is as follows.</p><pre>[<br>  &quot;C2&quot;,<br>  &quot;D2&quot;,<br>  &quot;E2&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;C3&quot;,<br>  &quot;D3&quot;,<br>  &quot;E3&quot;,<br>  &quot;C4&quot;,<br>  &quot;D4&quot;,<br>  &quot;C6&quot;,<br>  &quot;D6&quot;,<br>  &quot;C7&quot;,<br>  &quot;D7&quot;,<br>  &quot;B8&quot;,<br>  &quot;C8&quot;,<br>  &quot;D8&quot;,<br>  &quot;E8&quot;,<br>  &quot;F8&quot;<br>]</pre><p>When these A1Notations are consolidated, it becomes as follows.</p><pre>[<br>  &quot;C2:E3&quot;,<br>  &quot;C6:D8&quot;,<br>  &quot;C4:D4&quot;,<br>  &quot;E8:F8&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;B8&quot;<br>]</pre><p>Here, the maximum size of the rectangle is calculated starting from the top-left cell (C2 in this example). This approach determines the result values in the above output order.</p><p>It’s important to note that if the situation is changed, the maximum size of the rectangle might not always be obtainable using this method. While it’s possible to modify the starting cell for calculation to ensure the maximum rectangle size is always found, this can significantly increase the processing cost. Therefore, this script adopts the top-left cell as the starting point for calculation to strike a balance between efficiency and accuracy.</p><p>The image provides a visual representation of the consolidation process applied to a sample set of A1Notations.</p><p>The script can be seen at <a href="https://github.com/tanaikech/UtlApp/blob/master/forStringProcessing.js#L466">my repository</a>.</p><h3>Usage</h3><h3>1. Create a Google Spreadsheet</h3><p>Please create a new Google Spreadsheet. And, please set the background color as the above image. In this sample, the background colors are set in “B2:F8”.</p><p>And, please open the script editor of this Spreadsheet.</p><h3>2. Install library</h3><p>In this case, the script is a bit complicated. So, I added this script to my existing library <a href="https://github.com/tanaikech/UtlApp">UtlApp</a>. By this, this library can expand and consolidate the A1Notations.</p><p>You can see how to install this library at <a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#1-install-library">here</a>.</p><h3>3. Sample script 1</h3><p>In this sample, the situation of the above image is used. The script is as follows.</p><pre>function sampl1() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  Browser.msgBox(JSON.stringify(res));<br>}</pre><p>When this script is run, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/909/0*HGSpkIuucZ75_3Xh.gif" /></figure><h3>4. Sample script 2</h3><p>In this sample, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/0*xTvZlCIhIhj0z6mH.gif" /></figure><p>The cells of the red background color are selected by this script. And, the background color of the selected cells is manually changed.</p><p>The script is as follows.</p><pre>function sampl2() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  sheet.getRangeList(res).activate();<br>}</pre><h3>IMPORTANT</h3><ul><li>I’m worried that this method might not be able to be used on a Google Spreadsheet with a large size because of the process cost.</li></ul><h3>Note</h3><ul><li>The top abstract image was created by <a href="https://gemini.google.com/">Gemini</a> from the section of “Introduction”.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9ce870dcb99" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99">Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps…</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FHIR Whistle Data Mappings Validation]]></title>
            <link>https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/cd62c8613a92</guid>
            <category><![CDATA[healthcare-data-engine]]></category>
            <category><![CDATA[fhir-mapping]]></category>
            <category><![CDATA[whistle-data-mapping]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ashwinshetty]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:06:38 GMT</pubDate>
            <atom:updated>2024-04-16T06:26:24.849Z</atom:updated>
            <content:encoded><![CDATA[<h3>Business Scenario</h3><p>Healthcare Data Engine(HDE) is a popular GCP based solution to help Healthcare stakeholders transition to FHIR (Fast Healthcare Interoperability Resources). HDE provides pipelines which helps convert non FHIR data to FHIR and reconciles them to form a single Longitudinal Patient Record, which then makes deriving insights from patient data easy and quick.</p><p>One of the core components of HDE is the Data Mapping Language known as Whistle. This Open Source Data Mapping language is used for converting complex, nested data from one schema to another. For Example, from HL7 to FHIR.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/538/1*ODs0XPOT-aUALiRqchds9Q.png" /></figure><p>This article talks about how to use Whistle to write sample mappings for HL7 data. Run the mapping code locally and then test the resultant converted FHIR format data against a FHIR store.</p><h3><strong>What do we need</strong></h3><p>We will test the Whistle Mappings to convert sample HL7 data to FHIR. We will be leveraging APIs provided by GCP Healthcare API to ingest some sample HL7 messages to a HL7 store provided by GCP Healthcare API. We will use the schematized variant of this HL7 message from HL7 store and run Whistle mapping code to convert it to FHIR on our local machines. We will then test this converted FHIR data by ingesting it into GCP Healthcare API FHIR store</p><h3>Steps</h3><p><strong>Step 1</strong> — Enable GCP Cloud Healthcare API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/953/1*4QU0vG0lwQMn5RSS-yC2Fw.png" /></figure><p><strong>Step 2</strong> — Follow documentation in <a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">git repo — Healthcare Data Harmonization</a> to set up Whistle Engine on our local machines. This would need us to install below softwares on our local machines, as we will be using ‘<strong>gradle</strong>’ to run our Whistle engine application.</p><ul><li><a href="https://git-scm.com/">Git</a></li><li><a href="https://www.azul.com/downloads/?version=java-11-lts&amp;package=jdk#zulu">JDK 11.x</a></li><li><a href="https://gradle.org/next-steps/?version=7.6&amp;format=bin">Gradle 7.x</a></li></ul><p><strong>Step 3 </strong>— Create a <a href="https://cloud.google.com/healthcare-api/docs/datasets#create-dataset">Healthcare API Dataset</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2#creating_an_hl7v2_store">HL7 store</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/fhir#creating_a_fhir_store">Fhir store</a> inside that dataset. We will use these resources for our testing.</p><p>Once created we should see an output like below, where ‘<strong>datastore</strong>’ is our Healthcare API Dataset, ‘<strong>hl7v2store</strong>’ is the HL7 store and ‘<strong>fhirstore</strong>’ is the FHIR store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/793/1*FfLztPEw6stASDDU3c6C3w.png" /></figure><p><strong>Step 4</strong> — Let us <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2-messages#ingesting_hl7v2_messages">ingest a sample HL7 message</a> into our HL7 store. Save the below sample message in a file named ‘<strong>sample-hl7-msg.hl7</strong>’.</p><pre>MSH|^~\&amp;|FROM_APP|FROM_FACILITY|TO_APP|TO_FACILITY|20170703223000||ADT^A01|20170703223000|P|2.5|<br>EVN|A01|20210713083617|<br>PID|1||21004033^^^^MRN||SULIE^BRAN||19941208|M|||444 MAIN ST^^MOUNTAIN SPRINGS^CO^80444||1111111144|2222222244|<br>PV1||I|H44 RM4^1^^HIGHWAY 44 CLINIC||||5144^MARRIE QUINIE|||||||||Y||||||||||||||||||||||||||||20170703223000|</pre><p>The default segment separator in HL7v2 is a carriage return (\r). Most text editors use newline (\n) characters as segment separators. So we will use the below command to replace any \n with \r.</p><pre>sed -z &#39;s/\n/\r/g&#39; sample-hl7-msg.hl7 &gt; sample-hl7-msg-fixed.hl7</pre><p>HL7 store expects input messages to be in base64 encoded string format. So let us use the below command to encode the sample HL7 message.</p><pre>openssl base64 -A -in ./sample-hl7-msg-fixed.hl7 -out ./sample-hl7-msg-base64.txt</pre><p>Copy the encoded string from ‘<strong>sample-hl7-msg-base64.txt</strong>’ in the below format and save it in a file named ‘<strong>hl7v2-sample.json’</strong>.</p><pre>{<br>  &quot;message&quot;: {<br>    &quot;data&quot;: &quot;&lt;base64-encoded-string&gt;&quot;<br>  }<br>}</pre><p>We will run the below CURL command in a terminal to ingest this message to an HL7 store.</p><pre>curl -X POST      \<br>    -H &quot;Authorization: Bearer $(gcloud auth application-default print-access-token)&quot;      \<br>    -H &quot;Content-Type: application/json; charset=utf-8&quot;      \<br>    --data-binary @hl7v2-sample.json      \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages:ingest&quot;</pre><p>Once the command is successful, we will get a ‘<strong>message.name</strong>’ field in the response as shown below.</p><pre>{<br>  &quot;hl7Ack&quot;: &quot;&lt;base64-encoded-string&gt;&quot;,<br>  &quot;message&quot;: {<br>    &quot;name&quot;: &quot;&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;MESSAGE_ID&gt;&quot;,<br>    }<br>}</pre><p>Using the ‘<strong>message.name</strong>’ field we will next fetch the schematized message into an output json file. This file will act as an input for our whistle mappings.</p><pre>curl -X GET \<br>     -H &quot;Authorization: Bearer &quot;$(gcloud auth print-access-token) \<br>     -H &quot;Content-Type: application/json; charset=utf-8&quot; \<br>     &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;message-name&gt;&quot; \<br>     | jq &#39;.schematizedData.data | fromjson&#39; &gt; &lt;output-filename.json&gt;</pre><p><strong>Step 5</strong> — Let us open any IDE or terminal. We will run below gradle command to trigger mapping, in the directory where github repo was cloned.</p><pre>gradle :runtime:run -q --args=&quot;-m $HOME/wstl_codelab/codelab.wstl -i $HOME/wstl_cod<br>elab/&lt;output-filename.json&gt;&quot; &gt; converted-fhir.json</pre><blockquote>Explanation of the above <strong>gradle</strong> command:</blockquote><blockquote><strong>gradle</strong>: This invokes the Gradle build automation tool.<br><strong>:runtime:run</strong>: This tells Gradle to execute the run task to start the Whistle application.<br><strong>-q</strong>: This flag tells Gradle to run in “quiet” mode, suppressing most of the output except for errors.<br> <strong>— args</strong>: This introduces arguments that will be passed to the run task (and ultimately to the application it starts).<br><strong>-m $HOME/wstl_codelab/codelab.wstl</strong>: This argument specifies the path to a whistle file that the application will use for data mapping.<br><strong>-i $HOME/wstl_codelab/&lt;output-filename.json&gt;</strong>: This argument points to a JSON file containing input data for the Whistle mapping.</blockquote><p><strong>Sample Patient Whistle Mapping:</strong></p><blockquote>This code is just for demo purposes and does not represent the actual FHIR structure. It maps Patient fields like ‘<strong>identifier</strong>’, ‘<strong>name</strong>’ and ‘<strong>address</strong>’ from the PID segment in our input file. These mappings are structured into functions like ‘<strong>Build_Identifier</strong>’, ‘<strong>Build_Name</strong>’ and ‘<strong>Build_Address</strong>’ for better readability.</blockquote><pre>PID_Patient($root.ADT_A01.PID)<br><br>def PID_Patient(PID){<br>  identifier[]: Build_Identifier(PID.3[])<br>  name[]: Build_Name(PID.5[])<br>  address[]: Build_Address(PID.11[])<br>  active: true<br>  resourceType: &quot;Patient&quot;<br>}<br><br>def Build_Identifier(CX) {<br>  value: CX.1<br>}<br><br>def Build_Name(XPN) {<br>  family: XPN.1.1<br>  given[]: XPN.2<br>  given[]: XPN.3<br>}<br><br>def Build_Address(XAD) {<br>  line[]: XAD.2<br>  city: XAD.3<br>  state: XAD.4<br>  postalCode: XAD.5<br>}</pre><p><strong>Step 6</strong> — Once we have the mapped output, we can check if all the fields were converted as per our requirements. Once confirmed, we can try and load this to a FHIR store using the below command.</p><pre>curl -X POST \<br>    -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>    -H &quot;Content-Type: application/fhir+json&quot; \<br>    -d @converted-fhir.json \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/fhirStores/&lt;fhirstore-name&gt;/fhir/Patient&quot;</pre><p>Post successful completion of the above command, we should be able to see the record in our FHIR store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGeHlfu01LH9IC1oCEeN7Q.png" /></figure><h3>Conclusion</h3><p>By following the steps outlined above, we explored a method to validate the HL7 to FHIR conversion workflow utilizing the Open Source Whistle Data Mapping repository. This approach can be readily adapted to validate data conversion workflows involving any other data format to FHIR. This technique proves useful for conducting quick tests, proofs of concept (POCs), or pilot projects for healthcare data conversion to FHIR. Engaging with this process offers a deeper understanding of the capabilities of the powerful Whistle Data Mapping Language.</p><h3>Reference Links</h3><p><a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">Whistle github repo</a></p><p><a href="https://cloud.google.com/healthcare-api/docs">Cloud Healthcare API documentation</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd62c8613a92" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92">FHIR Whistle Data Mappings Validation</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dazbo’s Google Cloud Next ’24 Recap: Keynote]]></title>
            <link>https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f5518238c9d</guid>
            <category><![CDATA[ai-agent]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[wrap-up]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-next]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:05:10 GMT</pubDate>
            <atom:updated>2024-04-16T07:13:06.235Z</atom:updated>
            <content:encoded><![CDATA[<h3>Shall I? Shan’t I?</h3><p>It’s been a couple of days since Google Cloud Next ’24 wrapped up, and I’ve seen recaps appear on Medium already. So I ask myself: <em>“Should I bother this year?”</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*jvPtfzoKolvQBc2o.jpg" /><figcaption>To recap or not to recap…</figcaption></figure><p>I’ve decided “Yes” for two reasons…</p><ul><li>I’ve been doing recaps of these events for a few years, so I’d hate to break my streak! (Check out <a href="https://medium.com/google-cloud/google-next-2023-experience-and-favourite-sessions-fb00add5f59e">here</a>, and <a href="https://docs.google.com/presentation/d/1gfgijQjlQlvn6CEd29j5uVRXgynNFoBwMz2bUIXvydA/edit#slide=id.g27956d63397_0_230">here</a>.)</li><li>I find writing stuff down helps me learn and remember. So even if no one else finds this useful, I will!</li></ul><p>This year’s Google Cloud Next was in Las Vegas. Alas, this is the Next in the last few that I haven’t been able to attend in person. 😭 So, this wrap-up is based purely on watching the virtual content. And in case you weren’t aware, you can view all the recorded sessions at <a href="https://cloud.withgoogle.com/next">cloud.withgoogle.com/next</a>.</p><h3>Summary of Announcements</h3><p>I’ll update this list as a view more sessions.</p><ul><li>New investments in sub-sea cabling and data centres.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: A3 Mega VMs, powered by NVIDIA H100 Tensorcore GPUs. Twice as powerful has the previous iteration.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: GA of TPU v5p. Google’s most powerful TPU yet. These have 4x the compute capacity of the previous generation of TPUs.</li><li>Preview: Hyperdisk ML — next generation block storage optimised for AI workloads.</li><li>Vertex AI on <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>GKE Enterprise support for <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>AI Model support (including Gemma and Llama) on <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>Preview: <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805">Google Axion</a>. A custom ARM-based CPU. Claims 50% better performance and 60% more energy efficient than comparable current-gen x86 VMs! Google are migrating many services to Axiom.</li><li>Intel 5th Gen Xeon processors.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Gemini AI 1.5 Pro in Vertex AI</a>. Google’s multimodal foundational model. It can parse 1m tokens of information!</li><li>Gemini AI 1.5 Pro now integrated with Gemini Code Assist.</li><li>Supervised tuning for Gemini models.</li><li>Preview: Gemini Cloud Assist, which helps with the entire development lifecycle, including design and optimisation.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Grounding of Gemini models with Google Search</a>! This significantly reduces hallucination.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder">Vertex AI Agent Builder</a>: rapidly speed up the creation of multi-modal AI agents.</li><li><a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations">Google Vids</a> will be released to Workspace labs in June. This is an AI-powered collaborative video creation app, as part of Workspace.</li><li>Imagen 2.0 is now GA in Vertex AI. Google’s most advanced text-to-image model.</li><li>Public preview: Text-to-Live Image. This creates animated video-like images from a text prompt.</li><li>Public preview: Gemini in Looker.</li><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiant’s frontline threat intelligence using using natural language prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths and risks.</li></ul><h3>The Irony Isn’t Wasted On Me</h3><p>I’ve watched the keynote, and I’m summarising it here. Manually. Without AI.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/181/0*z3Pfn-gqrDOZ2lSc.gif" /></figure><h3>Opening Keynote: The New Way to Cloud</h3><p>You can see the full keynote <a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">here</a>.</p><h4>Keynote Quick Thoughts</h4><ul><li>It’s all about AI. Shocking.</li><li>The biggest announcements are around Gen AI capabilities.</li><li>I think the keynote mentioned AI agents 1,806,402 times. (Okay, I’m exaggerating slightly.)</li></ul><h4>Introduction</h4><blockquote>Google are at the forefront of the AI platform shift. More than 60% of funded Gen AI startups, and nearly 90% of Gen AI unicorns are Google Cloud customers.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5JXpAkiOWGrILB4z.jpg" /></figure><p>The keynote opens with an introductory video talking the power of AI today. (<em>“AI you say? I’m shocked. Shocked, I tell you!”</em>) The video talks about things we can do with AI now, like:</p><ul><li>Using satellites to reduce methane emissions.</li><li>Turning DNA into code to make… Crop-resistant corn!</li><li>Spoting and filling potholes.</li><li>Spoting diseases earlier.</li><li>Scanning 100K lines of code in 2 minutes, in order to spot and fix bugs.</li></ul><p>So this is <em>“The new way to Cloud.”</em></p><p>So far, so cool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1PHwomsMjmMnmhFTPJ62dw.png" /><figcaption>Google has announced partnerships with 100s of leading AI partners</figcaption></figure><p>The early keynote includes a brief introduction to some of the topics of this year’s Next:</p><ul><li>Over 300 customers and partners will be sharing their <strong>Gen AI success stories</strong> at this event.</li><li>Some discussion around the launch of <strong>Gemini </strong>and the advancements since its launch.</li><li><strong>Google Distributed Cloud and Edge</strong>, to support highly confidential and edge workloads.</li><li><strong>Cross-cloud networking</strong> now provides secure, low-latency connectivity of Google’s AI services to any application on any cloud.</li><li><strong>Chrome Enterprise Premium Browser</strong>.</li><li><strong>Multimodal Gen AI Agents</strong> will transform how we interact with the applications and the web. Agents are intelligent entities to do things like: customer agents, to help a shopper find the perfect dress; or helping an employee pick the right health benefits.</li></ul><h4>The AI Stack</h4><p>The keynote talks about <strong>Google’s AI stack</strong>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FxbiOgARL1a4-Q__qFFqfA.png" /><figcaption>Google’s AI stack</figcaption></figure><ul><li>Note the rebranding of Duet AI to “<a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-for-google-cloud-is-here"><strong>Gemini for Google Cloud</strong></a>”.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture"><strong>AI Hypercomputer</strong></a>: an integrated AI infrastructure platform for offering AI at scale. There are a number of announcements related to GPUs, TPUs, and AI-optimised storage.</li></ul><p>The keynote includes announcements around:</p><ul><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization/unlock-ai-anywhere-with-google-distributed-cloud?e=48754805"><strong>Google Distributed Cloud</strong></a>, which has a number of capability enhancements around GKE, Vertex AI, and AI model support. GDC now has both “secret” and “top secret” accreditations. Mobile operator “Orange” referenced as an organisation running across 26 countries and using GDC to keep data localised to each country.</li><li><a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805"><strong>Google Axion</strong></a>. A new custom ARM-based CPU that offers considerably higher performance and lowe energy consumption than caparable current gen x86.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates"><strong>Gemini 1.5 Pro in public preview</strong></a>. It has the world’s largest context window. In a single shot, it can process: 1M tokens, 1 hour of video, 11 hours of audio, and over 30K lines of code.</li><li><strong>Grounding of Gemini models with Google Search!</strong> This significantly reduces hallucination. Or you can ground with data from your own databases.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder"><strong>Vertex AI Agent Builder</strong></a> — to rapidly speed up creating AI Agents. Gemino Pro can create free-flowing conversations with text, voice, images and video as inputs. But also, it can even provide real time interactions in voice! Natural language can be used to train the AI agents, e.g. to describe topics that are verboten. You can configure transcription and summarisation. And response quality can be improved using vector search. Also, modular extensions can be integrated to complete standard customer workflows, e.g. booking a flight.</li></ul><h4>Shopping AI Agents</h4><p>The keynote then demonstrates a <strong>shopping AI Agent</strong>, and the ability to upload a video and ask it:</p><blockquote>Find me a checkered shirt like the keyboard player is wearing. I’d like to see prices, where to buy it, and how soon can I be wearing it?</blockquote><p>The response is near instantaneous on the website. And then we see a demo of interacting with a <em>voice </em>AI agent which continues the interaction and completes the transaction. That’s pretty cool!</p><h4>A Few Google Workspace Updates</h4><p>Then the keynote moves onto <strong>Gemini for Google Workspace</strong>. Use it to:</p><ul><li>Answer questions.</li><li>Create notes in meetings.</li><li>Extract insights from reports.</li><li>Create images to insert in presentations.</li><li>Real-time translation.</li></ul><p>Announcements related to <strong>Google Workspace</strong>:</p><ul><li>A recent benchmarking study shows <strong>Google Meet now outperforms Zoom and MS Teams</strong> for overall video performance.</li><li>Chat summarisation and real time translation now available for Google Meet.</li><li><strong>AI Security add-on</strong> can automatically classify and protect company data.</li><li><strong>Gemini in Google Chat</strong> can provide summaries of long conversations.</li></ul><p>We see a demo of reviewing proposals, comparing them, and asking questions, e.g.</p><blockquote>Does this offer comply with our compliance rule book?</blockquote><h4>Employee Agents</h4><p>Next, we talk about how to <strong>create a multi-modal AI employee agents using Vertex AI</strong>:</p><ul><li>Create a custom model with Vertex AI.</li><li>Connect the custom model to your company data and web data.</li><li>Ground in enterprise truth, e.g. with BigQuery and AlloyDB.</li></ul><p>Then we see a demo of how you can use a Vertex AI employee agent to summarise an employee benefits enrollment email, as well as a one hour benefits video. The agent is able to reason across text, video and the prompt, and provide a summary. Furthermore, the agent is able to compare the proposed plan to a previous plan, and make inferences.</p><h4>Creative AI Agents</h4><p>Now we move on to <strong>Creative AI Agents</strong>. Carrefore are using Creative AI Agents for marketing; they built a new marketing studio using Vertex AI, in just five weeks. Now they can build personalised campaigns in just a few clicks.</p><p>Creative agents uses Gemino Pro to look at existing material, documents and brand images, to infer a brand identity. We can generate multi-modal content; we can create live images, and even podcasts!</p><p>Then there was the announcement of <a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations"><strong>Google Vids</strong></a>, the AI-powered collaborative video creation app, as part of Google Workspace. Aparna then demos creating a recap video of the Next event, using Google Vids:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kZo1hMT6Ne-896WcxB3sGQ.png" /><figcaption>Creating a video recap in seconds, using Google Vids</figcaption></figure><p>Then we have announcements of <strong>Imagen 2.0 Text-to-Image</strong>, including new editing modes to edit a generated image. And there’s the new <strong>Text-to-Live Image</strong>, which is now in preview:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*K2OL48TfgPnzrPn6EXgqhw.gif" /><figcaption>Generating a live image from a prompt</figcaption></figure><h4>Data Agents</h4><p>So many agents!!</p><p>AI Data Agents us to ask natural language questions of our data. <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24"><strong>Gemini in BigQuery</strong></a> is now in Preview, and allows AI-powered data preparation, analysis and querying. BigQuery can be integrated directly with Vertex AI. So now we can perform multi-modal analysis across all of documents, images, videos, audio, and structured data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LlwikgKhm7Q-kjaSvSKqig.png" /><figcaption>Querying a data agent</figcaption></figure><p>One extremely cool thing about this demo was that the agent built a forecast dynamically, using BigQuery ML. And then uses vector embeddings to find products that look like a supplied image.</p><h4>Code Agents</h4><p>Surprise… More agents.</p><p>Google’s AI code assistant is now called <strong>Gemini Code Assist</strong>. (No more Duet AI.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/1*pizqI2uBk7FGn_0Y5TBAhg.png" /><figcaption>Benefits of using Code Assist</figcaption></figure><p>The keynote talks about how Gemini Code Assist can be used with a code base anywhere… On-prem, GitLab, GitHub, BitBucket, etc. Furthermore, Gemini Code Assist supports data residency requirements in multiple regions. It is now integrated with Gemini 1.5 Pro, and can leverage the new 1-million token context window.</p><p>The demo was cool… Show the visual mockup of a new UI to Gemini Code Assist, and it generates the code, leveraging our entire (huge) code base, and aligned to our code standards.</p><h4>Security Agents</h4><p>Please… No more agents!</p><p>These AI agents assist security operations teams, radically increasing the speed of security investigation and response.</p><p>There were a number of announcements relating to integration of Gemini into security products:</p><ul><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiant’s frontline threat intelligence using using natural language prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths and risks.</li></ul><h4>Wrap-Up</h4><p>Thomas Kurian wraps-up by saying:</p><blockquote>Our open platform offers choice at every layer.</blockquote><ul><li>Chips (CPUs, TPUs, GPUs) for training and serving.</li><li>Your choice of models.</li><li>Your choice of development environments.</li><li>Databases, including vector.</li><li>Your choice of business applications.</li></ul><blockquote>We’re creating a new era of generative AI agents, built on a new, truly open platform for AI. And we’re reinventing infrastructure to support it.</blockquote><h3>What’s Next?</h3><p>(See what I did there?)</p><p>I’ll watch a bunch of sessions I’m interested in, and provides some useful nuggets and summaries soon. I’ll put these in some separate articles, rather than just adding to this one.</p><h3>Links</h3><ul><li><a href="https://cloud.withgoogle.com/next">Google Cloud Next ‘24</a></li><li><a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">Keynote</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2024-wrap-up">All 218 things we announced at Google Cloud Next ‘24</a></li></ul><h3>Before You Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helps me!</li><li>Please give me claps! You know you clap more than once, right?</li><li>Feel free to <strong>leave a comment</strong> 💬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on these icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*73hF99AvDUGryMuV.png" /><figcaption>Follow and Subscribe</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f5518238c9d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d">Dazbo’s Google Cloud Next ’24 Recap: Keynote</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fine tuning Gemma with LoRA on GCP]]></title>
            <link>https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/5d25dbab9e0e</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[finetune-llm]]></category>
            <category><![CDATA[lora]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gemma]]></category>
            <dc:creator><![CDATA[pritam sahoo]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:52 GMT</pubDate>
            <atom:updated>2024-04-16T00:04:52.701Z</atom:updated>
            <content:encoded><![CDATA[<p>My obsession with Gemma continues. Folks new to the Gemma model can revisit my previous blog <a href="https://medium.com/google-cloud/gemma-open-models-from-google-0045263e53d2">link</a>.</p><p>In brief Gemma is the family of lightweight, state of the art (SOTA) open models powered by the same technology powering one of the most popular Google Cloud Gemini models.</p><p>In this blog we will get started with fine tuning with Gemma with LoRA.</p><p>Lets understand first a bit on fine tuning. One of the reasons finetuning is picking up is the reason Large language Models(LLMs) are not trained on specific tasks or domain related data. Primarily LLMs often called as foundational models are trained on internet scale massive corpus of data, texts etc. Doing a full training of pre-trained LLM models becomes technically challenging due to expensive computational resources as one of the major concerns.</p><p>Let’s understand the benefits of Fine tuning.</p><ol><li>Fine Tuning pre-trained model is much faster and cost effective leading to less computational resources required.</li><li>Better Performances for domain specific tasks especially on industry use cases related to Financial services, Insurance , Healthcare etc.</li><li>Lets not forget about democratization of GenAI models for individual users i.e. developers and others who have less computational power.</li></ol><p>Lets understand Parameter efficient fine tuning <a href="http://a.ka">a.k.a</a>. PEFT. It’s a subset of fine tuning which effectively utilizes parameters/weights with efficient output. Instead of altering all the parameters of the model PEFT selects a subset of them thereby reducing computational and memory requirements. PEFT plays a major role in the fine tuning process thereby improving the performance of base/foundational LLMs on specific tasks. This is super useful when training LLM models like Gemini and its different variants, PALM,even open source Gemma models etc from Google.</p><p>We will explore fine tuning Gemma Models with <strong>LoRA</strong>. <strong>LoRA</strong> stands for Low Rank Adaptation of Large Language Models. It’s a technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights/parameters of the base model and introducing a small number of new weights into the model.</p><p><strong>Crucial Point to consider</strong> In LoRA, the starting point hypothesis is super important . It assumes that the pre-trained model’s weights are already close to the optimal solution for the downstream tasks.</p><p>Advantages of using LoRA as fine tuning technique</p><ol><li>Reduces Parameter and memory footprint. LoRA significantly reduces the number of trainable parameters, making it much more memory-efficient and computationally cheaper.</li><li>Fine tuning and so does inference is faster ~ as it uses less parameters/weights.</li><li>Maintains performance: LoRA has been proved to maintain performance close to traditional fine-tuning methods in several tasks.</li></ol><p><strong>So let’s get started with Fine tuning with LoRA on the Gemma Model.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UkJdQkyIoYu3I-OH3bJYoA.jpeg" /></figure><p>For this demo I will be using Google Collab Notebook to get some horsepower with T4 GPUs.</p><p><strong>Step 1: Get access to Gemma</strong></p><p>To complete this collab, you will first need to complete the setup instructions at <a href="https://ai.google.dev/gemma/docs/setup">Gemma setup</a>. The Gemma setup instructions show you how to do the following:</p><ul><li>Get access to Gemma on <a href="https://kaggle.com/">kaggle.com</a>.</li><li>Select a Colab runtime with sufficient resources to run the Gemma 2B model.</li><li>Generate and configure a Kaggle username and API key.</li></ul><p>After you’ve completed the Gemma setup, move on to the next section, where you’ll set environment variables for your Colab environment.</p><p><strong>Step 2 : Select the Runtime</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/468/0*6QanOb6XI087IwsU" /></figure><h4><strong>Step 3 : Configure your secrets i.e. username and key in Account tab</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/834/0*m_Kg4FsICCsvEH73" /></figure><p><strong>Step 4 : Select the Data for fine tuning from hugging face. </strong><a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k"><strong>Databricks Dolly 15k dataset</strong></a><strong>. </strong>This dataset contains 15,000 high-quality human-generated prompt / response pairs specifically designed for fine-tuning LLMs. Brief screenshot of the datasets</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/997/0*wmgP3pZMciNG5Ix5" /></figure><p><strong>Step 5 : Set the environment variables and run the below commands in Collab</strong></p><p>import os</p><p>from google.colab import userdata</p><p>os.environ[“KAGGLE_USERNAME”] = userdata.get(‘username’)</p><p>os.environ[“KAGGLE_KEY”] = userdata.get(‘key’)</p><p><strong>Step 6 : Install the dependencies</strong></p><p>!pip install -q -U keras-nlp</p><p>!pip install -q -U keras&gt;=3</p><p><strong>Step 7 : Select the backend. You may choose from PyTorch or Tensorflow or Jax</strong></p><p>os.environ[“KERAS_BACKEND”] = “jax”.</p><p># Avoid memory fragmentation on JAX backend.</p><p>os.environ[“XLA_PYTHON_CLIENT_MEM_FRACTION”]=”1.00&quot;</p><p><strong>Step 8 : Import Packages i.e. Keras and KerasNLP.</strong></p><p>import keras</p><p>import keras_nlp</p><p><strong>Step 9 : Load the dataset from hugging face.</strong></p><p>!wget -O databricks-dolly-15k.jsonl <a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl">https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl</a></p><p><strong>Step 10 : For this demo purpose I will be using a subset of 1000 examples instead of 15K examples. For better fine tuning you may use more examples.</strong></p><p>import json</p><p>data = []</p><p>with open(“databricks-dolly-15k.jsonl”) as file:</p><p>for line in file:</p><p>features = json.loads(line)</p><p># Filter out examples with context, to keep it simple.</p><p>if features[“context”]:</p><p>continue</p><p># Format the entire example as a single string.</p><p>template = “Instruction:\n{instruction}\n\nResponse:\n{response}”</p><p>data.append(template.format(**features))</p><p># Only use 1000 training examples, to keep it fast.</p><p>data = data[:1000]</p><p><strong>Step 11 : Now its time to Load the Gemma 2B base Model. You may try using the Gemma 7B base model.</strong></p><p>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(“gemma_2b_en”)</p><p>gemma_lm.summary()</p><p>You will see below summary output if everything is working fine.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/0*df55cDERngDI3ig_" /></figure><p><strong>Step 11: Lets Inference the Model before fine tuning.</strong></p><p>Pass the below prompt i.e. “ What should I do on a trip to Europe?”</p><p>prompt = template.format(</p><p>instruction=”What should I do on a trip to Europe?”,</p><p>response=””,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>You will see very generic blant and not so great output from the base model as mentioned below</strong></p><p>— — — — — — — — — — — — — — — — — — — — —</p><p><strong>Instruction:</strong></p><p><strong>What should I do on a trip to Europe?</strong></p><p><strong>Response:</strong></p><p><strong>It’s easy, you just need to follow these steps:</strong></p><p><strong>First you must book your trip with a travel agency.</strong></p><p><strong>Then you must choose a country and a city.</strong></p><p><strong>Next you must choose your hotel, your flight, and your travel insurance</strong></p><p><strong>And last you must pack for your trip.</strong></p><p><strong>— — — — — — — — — — — — — — —</strong></p><p><strong>Step 12: Lets fine tuning using LoRA using Databricks Dolly 15K dataset.</strong></p><p>LoRA rank. It controls the expressiveness and precision of the fine-tuning adjustments.Lower rank means which requirement of computational power and also less precision adaptation. You may start with 4,8 etc for demo/experimentation purposes.</p><p>&gt;&gt; gemma_lm.backbone.enable_lora(rank=4)</p><p>&gt;&gt; gemma_lm.summary()</p><p><strong>Total params: 2,507,536,384 (9.34 GB)</strong></p><p><strong>Trainable params: 1,363,968 (5.20 MB)</strong></p><p><strong>Non-trainable params: 2,506,172,416 (9.34 GB)</strong></p><p><strong>While you run the below section in the collab notebook be patient as it will take some time and you will see reduction in losses.This step will reduce the number of trainable parameters significantly.Epoch = 1 means it will run for 1 time for 1000 datasets.</strong></p><p>gemma_lm.preprocessor.sequence_length = 512</p><p>optimizer = keras.optimizers.AdamW( // AdamW ~ optimizer for transformer models</p><p>learning_rate=5e-5,</p><p>weight_decay=0.01,</p><p>)</p><p>optimizer.exclude_from_weight_decay(var_names=[“bias”, “scale”])</p><p>gemma_lm.compile(</p><p>loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),</p><p>optimizer=optimizer,</p><p>weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],</p><p>)</p><p>gemma_lm.fit(data, epochs=1, batch_size=1)</p><p><strong>The output from the above step will show significant reduction in loss with just 1000 datasets.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qf-k1hlFro2K4DUh" /></figure><p><strong>Step 13: Let’s get started with Inferencing post fine tuning.</strong></p><p>Pass the below prompt again i.e. “ What should I do on a trip to Europe?”</p><p>prompt = template.format(</p><p>instruction=”What should I do on a trip to Europe?”,</p><p>response=””,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>**** Let me know the results. Must be better than before finetuning.</strong></p><p>Thats’ it folks on Gemma fine tuning with LoRA. Stay tuned for more updates coming your way on QLoRA……..</p><p><strong>Signing off…. Pritam</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d25dbab9e0e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e">Fine tuning Gemma with LoRA on GCP</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use Log Analytics for BigQuery Usage Analysis on Google Cloud]]></title>
            <link>https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f5454626c6c</guid>
            <category><![CDATA[log-analytics]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[observability]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[logs]]></category>
            <dc:creator><![CDATA[Xiang Shen]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:36 GMT</pubDate>
            <atom:updated>2024-04-16T12:18:26.756Z</atom:updated>
            <cc:license>https://creativecommons.org/publicdomain/mark/1.0/</cc:license>
            <content:encoded><![CDATA[<p>On Google Cloud you can use <a href="https://cloud.google.com/logging/docs/log-analytics#analytics">Log Analytics</a> to query and analyze your log data, and then you can view or <a href="https://cloud.google.com/logging/docs/analyze/charts">chart the query results</a>.</p><p><a href="https://cloud.google.com/bigquery">BigQuery</a> is Google Cloud’s fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence.</p><p>While BigQuery offers built-in observability capabilities like the <a href="https://cloud.google.com/bigquery/docs/information-schema-intro">INFORMATION_SCHEMA</a> views, detailed logging remains crucial for in-depth usage analysis, auditing, and troubleshooting potential issues.</p><p>This article will walk you through how to analyze BigQuery logs using log analytics.</p><h3>Upgrade Log bucket</h3><p>First, if you haven’t, you need to configure Cloud Logging to upgrade all the existing log buckets with Log Analytics enabled.</p><p>To upgrade an existing bucket to use Log Analytics, do the following:</p><ol><li>In the navigation panel of the Google Cloud console, select <strong>Logging</strong>, and then select <strong>Logs Storage.</strong></li><li>Locate the bucket that you want to upgrade.</li><li>When the <strong>Log Analytics available</strong> column displays <strong>Upgrade</strong>, you can upgrade the log bucket to use Log Analytics. Click <strong>Upgrade</strong>.<br>A dialog opens. Click <strong>Confirm</strong>.</li></ol><h3>Perform BigQuery Activities</h3><p>Complete the following tasks to generate some BigQuery logs. In the tasks, the BigQuery command line tool <a href="https://cloud.google.com/bigquery/docs/reference/bq-cli-reference">bq</a> is used.</p><p><strong>Task 1. Create datasets</strong></p><p>Use the <strong>bq mk</strong> command to create new datasets named <strong>bq_logs</strong> and <strong>bq_logs_test </strong>in your project:</p><pre>bq mk bq_logs<br>bq mk bq_logs_testbq mk bq_logs_test</pre><p><strong>Task 2. List the datasets</strong></p><p>Use the <strong>bq ls</strong> command to list the datasets:</p><pre>bq ls</pre><p><strong>Task 3. Delete a dataset</strong></p><p>Use the <strong>bq rm</strong> command to delete the a dataset (select <strong>y</strong> when prompted):</p><pre>bq rm bq_logs_test</pre><p><strong>Task 4. Create a new table</strong></p><pre>bq mk \<br> --table \<br> --expiration 3600 \<br> --description &quot;This is a test table&quot; \<br> bq_logs.test_table \<br> id:STRING,name:STRING,address:STRING</pre><p>You should have a new empty table named <strong>test_table</strong> that has been created for your dataset.</p><p><strong>Task 5. Run some example queries</strong></p><p>You can run a simple query like the following to generates a log entry. Copy and paste the following query into the BigQuery Query editor:</p><pre>bq query — use_legacy_sql=false ‘SELECT current_date’</pre><p>The following query will leverage weather data from the <a href="https://cloud.google.com/blog/products/data-analytics/noaa-datasets-on-google-cloud-for-environmental-exploration">National Oceanic and Atmospheric Administration (NOAA)</a>. Copy the query into the BigQuery editor and click <strong>RUN</strong>.</p><pre>bq query --use_legacy_sql=false \<br>&#39;SELECT<br> gsod2021.date,<br> stations.usaf,<br> stations.wban,<br> stations.name,<br> stations.country,<br> stations.state,<br> stations.lat,<br> stations.lon,<br> stations.elev,<br> gsod2021.temp,<br> gsod2021.max,<br> gsod2021.min,<br> gsod2021.mxpsd,<br> gsod2021.gust,<br> gsod2021.fog,<br> gsod2021.hail<br>FROM<br> `bigquery-public-data.noaa_gsod.gsod2021` gsod2021<br>INNER JOIN<br> `bigquery-public-data.noaa_gsod.stations` stations<br>ON<br> gsod2021.stn = stations.usaf<br> AND gsod2021.wban = stations.wban<br>WHERE<br> stations.country = &quot;US&quot;<br> AND gsod2021.date = &quot;2021-12-15&quot;<br> AND stations.state IS NOT NULL<br> AND gsod2021.max != 9999.9<br>ORDER BY<br> gsod2021.min;&#39;</pre><h3>Perform log analysis</h3><p>Now there are some log entries for BigQuery. You can run some queries using Log Analytics.</p><p><strong>Task 1. Open Log Analytics</strong></p><p>On the left side, under <strong>Logging</strong> click <strong>Log Analytics</strong> to access the feature. You should see something like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6PrYdj0eZePE3wF8" /></figure><p>If your query field is empty or you forget which table you want to use, you can click the <strong>Query</strong> button to get the sample query back.</p><p>Now you can run your own queries in the query field. Remember to replace <strong>[Your Project Id]</strong> with the project id you are using.</p><p><strong>Task 2. To find the activities for BigQuery datasets</strong></p><p>You can query the activities that a dataset is created or deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;datasetservice%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QgplyJgYAw8L93Rn" /></figure><p><strong>Task 3. To find the activities for BigQuery tables</strong></p><p>You can query the activities that a dataset is created or deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;%TableService%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*REyZPbwK41Vfotxk" /></figure><p><strong>Task 4. To view the queries completed in BigQuery</strong></p><p>Run the following query:</p><pre>SELECT<br> timestamp,<br> resource.labels.project_id,<br> proto_payload.audit_log.authentication_info.principal_email,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.query) AS query,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.statementType) AS statementType,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatus.error.message) AS message,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS startTime,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS endTime,<br> CAST(TIMESTAMP_DIFF( CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS TIMESTAMP), CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS TIMESTAMP), MILLISECOND)/1000 AS INT64) AS run_seconds,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalProcessedBytes) AS INT64) AS totalProcessedBytes,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalSlotMs) AS INT64) AS totalSlotMs,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.referencedTables) AS tables_ref,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalTablesProcessed) AS INT64) AS totalTablesProcessed,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.queryOutputRowCount) AS INT64) AS queryOutputRowCount,<br> severity<br>FROM<br> `[Your Project Id].global._Default._Default`<br>WHERE<br> log_id = &quot;cloudaudit.googleapis.com/data_access&quot;<br> AND proto_payload.audit_log.service_data.jobCompletedEvent IS NOT NULL<br>ORDER BY<br> startTime</pre><p>After the query completes, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yIhioPqh3mZwXguM" /></figure><p>Scroll through the results of the executed queries.</p><p><strong>Task 5. To chart the query result</strong></p><p>Instead of using a table to see the results, Log Analytics also supports creating charts for visualization. For example, to view a pie chart for the queries that have run, you can click the <strong>Chart</strong> button in the result view, select <strong>Pie chart</strong> as the chart type and <strong>query</strong> as the column. You should see a chart similar to the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HoiWqDgj6gqLAp58" /></figure><p>We’ve only scratched the surface of BigQuery log analysis; you can explore many other queries and charts to enhance your understanding of BigQuery. Feel free to contribute and create samples in <a href="https://github.com/GoogleCloudPlatform/observability-analytics-samples/tree/main/samples/logging">GCP’s sample GitHub repository</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f5454626c6c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c">Use Log Analytics for BigQuery Usage Analysis on Google Cloud</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Landing Zone Technical Onboarding— the “How-To” (Google Cloud Adoption Series)]]></title>
            <link>https://medium.com/google-cloud/landing-zone-technical-onboarding-the-how-to-google-cloud-adoption-series-9b7ba8710e83?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/9b7ba8710e83</guid>
            <category><![CDATA[technical-design]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-partner]]></category>
            <category><![CDATA[landingzone]]></category>
            <category><![CDATA[project-team]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Mon, 15 Apr 2024 08:06:46 GMT</pubDate>
            <atom:updated>2024-04-15T13:45:31.192Z</atom:updated>
            <content:encoded><![CDATA[<p>Welcome to the latest installment in the <a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Google Cloud Adoption and Migration: From Strategy to Operation</a> series.</p><p>Previously, we covered the various considerations that are part of LZ design. In this part we’ll cover:</p><ul><li>Establishing your <strong>LZ Core Project Team</strong>.</li><li>Establishing the <strong>support </strong>you need.</li><li><strong>Workshopping </strong>the design considerations.</li><li>Agreeing and documenting your <strong>LZ design decisions</strong> in the <strong>LZ Design Document</strong>.</li><li><strong>Deploy!</strong></li></ul><h3>Technical Onboarding?</h3><p>Google refers to the overall process of setting up a Cloud environment as <em>“Technical Onboarding”</em>. Historically it was referred to as <em>“Cloud Foundation”</em>. It includes:</p><ol><li>Kick-off, scoping and planning</li><li>Understanding organisational capability needs</li><li>Workshopping</li><li>Implementation (deployment)</li></ol><p>But before you can execute any steps, you’ll need a core project team.</p><h3>Establishing Your LZ Project Team</h3><p>If you’re working through the process of designing your LZ, then it’s probably fair to say that your maturity with Google Cloud is quite low. If you were mature in Google Cloud, you’d already have your well-designed LZ! So the first thing you’ll need to do is <strong>establish the core LZ project team</strong>, who will be responsible for:</p><ul><li>Working through the various LZ design considerations that I have outlined in the previous few parts.</li><li>Capturing your LZ design decisions.</li><li>Deploying your LZ.</li></ul><p><strong>What should this LZ core team look like?</strong> My recommendations are that you include the following:</p><ul><li>A <strong>lead cloud architect</strong>. This needs to be someone who fully understands cloud, and your organisation’s cloud strategy. Possibly, they were the enterprise cloud architect who created the organisation’s cloud strategy in the first place. This person will ultimately own the technical deliverables, and have the final say on the design decisions.</li><li><strong>A project manager.</strong> For obvious reasons!</li><li>A <strong>Platform / DevOps Lead</strong>. This is someone who has a strong understanding of Google Cloud, DevOps, and Terraform IaC. This person will ultimately be responsible for deployment of the LZ. They are likely also a key member of the <strong>Cloud Platform Team</strong>, which <em>may not yet formally exist at this point in the journey. </em>(And it is likely that this person will be a pivotal member of the new Cloud Platform Team.)</li><li>An <strong>SRE Lead</strong>. This person will be concerned with observability and embedding <a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE best practice</a>. They are likely a key member of the <strong>SRE Team</strong>, <em>which may not yet formally exist.</em></li><li>A <strong>network architect</strong>. Someone who understands your existing network topology, switching and routing, and your network strategy. This person will be well placed to describe the options and constraints, particularly when agreeing hybrid connectivity, DNS options, etc.</li><li>A <strong>security architect</strong>. Someone who understands the organisational security requirements, as well as existing capabilities and use cases for technologies like firewalls, IDS/IPS, WAF, and proxies. They will also be aware of the security strategy.</li><li><strong>Google Cloud SMEs</strong>. You need a some Google Cloud specialists who collectively have strong knowledge and experience of Google Cloud LZs, and of all the products and services that are fundamental the LZ, such as IAM, networking, GKE, monitoring and alerting, etc.</li></ul><p>Which brings us on to…</p><h3>Support</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iwfCzdjGNwy9VRke" /></figure><p>Again: if you’re in the process of designing and building your LZ, it’s quite likely that your organisation doesn’t yet have the necessary expertise and experience to design and deploy the LZ. You’re going to need some help.</p><p>For this, I would recommend engaging with Google, or with a <a href="https://cloud.google.com/find-a-partner/">Google Cloud Partner</a> that offers a <strong>Landing Zone design and build service</strong>. During the LZ design and build phase, you can supplement your internal team with all the expertise (and experience) that you need. For example, the Google Cloud partner can supply you with:</p><ul><li>A<strong> Google Cloud Architect</strong> who is an expert in Google Cloud and LZ design.</li><li><strong>Additional experts</strong> who can handle any specific queries that the architect doesn’t have the answers to. These additional resources could be deployed into your core team, or simply be available as “background resources” that your partner can tap into.</li><li><strong>DevOps engineering capability</strong>, to either build and deploy your IaC, or to support and guide your own Platform Team.</li></ul><p>Additionally, such a partner can assist you to:</p><ul><li>Build your <a href="https://medium.com/@derailed.dash/google-cloud-adoption-organisational-change-capabilities-upskilling-and-cloud-centre-of-15bc49ae7ae6">CCoE capability</a>.</li><li>Build your Platform Team.</li><li>Build your <a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE capability</a>.</li><li>Execute an initial migration PoC.</li><li>Help you build an application migration factory team.</li></ul><p>The partner can advise on organisational structure, provide resource augmentation until your organisation is self sufficient, and help upskill your internal staff.</p><p>It would be remiss of me not to mention <a href="https://www.epam.com/?gad_source=1&amp;gclid=Cj0KCQjwlN6wBhCcARIsAKZvD5h7jf_UxQE04IrMPl5G9rxYkX2YhMjUgUVU_jPCnO9AWjz9fnPgXLkaAiX0EALw_wcB"><strong>EPAM</strong></a><strong> </strong>at this stage, because a) they are a <strong>Google Cloud Premier Partner</strong> that offers such a service; and b) I happen to work for them!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*az-PumqTxzgAcYpE.png" /><figcaption>EPAM — A Google premier partner</figcaption></figure><p>They are multinational, with around 50000 engineers and consultants. They have over 1200 Google Cloud certified experts, and they have won <strong>Google Cloud Partner of the Year</strong> in 2018, 2023 and 2024!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/868/1*gv58BYiOq362nTAF4flxgQ.png" /><figcaption>EPAM’s credentials</figcaption></figure><p>I’ve worked with a lot of consultancies over my career (though typically, as the client), and I can honestly say: <strong>EPAM are the gold standard</strong>.</p><p>If you do want to engage with EPAM, you can reach out through the link above, or you can <a href="https://github.com/derailed-dash">connect with me directly</a>.</p><h3>Workshops</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RejtZ3_J4Zzn-rtG.jpg" /><figcaption>Tried-and-Tested</figcaption></figure><p>Now you’ve got your project team and engaged with the support you need, it’s time to crack on with some workshops!</p><p><strong>Google’s tried-and-tested approach is to hold a series of workshops</strong>, aligned to the various LZ design considerations that I’ve previously covered. (Google Cloud partners will typically use a similar approach.) I would recommend a 2 hour workshop for each of the following topics:</p><ul><li><strong>Initiation:</strong> LZ goals; current state; overall project scope; confirming roles and responsibilities; ways-of-working.</li><li><strong>Identity and Access Management:</strong> IAM; roles; master IdP decisions; Google Workspace superadmins; Google Cloud org admins; other groups; IdP integration; SSO and MFA.</li><li><strong>Resource Hierarchy and Management:</strong> org and folder structure; org policies; tenants and project factory principles; environments and sandboxes.</li><li><strong>Network and Security: </strong>hybrid connectivity; shared VPC topology; VPC-SC; firewall; ingress and egress (including Internet connectivity) patterns; current network/appliance considerations; DDoS and WAF (e.g. with Cloud Armor); DNS; DR and region considerations; org policies revisit.</li><li><strong>Compute and GKE:</strong> GKE strategy; multitenant cluster design; fleet design; GKE address ranges; release channels; Workload Identity Federation; Anthos service mesh; IaaS OS management and upgrade/patching strategy.</li><li><strong>Operations and Visibility: </strong>monitoring, logging and alerting; predefined and custom dashboards; metrics, including GKE and Istio metrics, and Ops Agent; metrics scope (aggregation and isolation) design; SIEM considerations; audit logging; network service logging; logging aggegration and organisational log sinks; log archiving and exports; integration with other operations software (e.g. on-prem); SRE considerations.</li><li><strong>Billing and Cost Optimisation: </strong>billing roles; tenant/project cost visibility; billing exports; project budgets; sandbox budgets; budget alerts; labelling strategy and standards.</li><li><strong>Automation, GitOps and Foundation Enablement:</strong> IaC, GitOps and CI/CD; LZ IaC frameworks and accelerators (e.g. Google Fabric FAST and Google CFT); IaC policy enforcement; project factory; tenant onboarding processes and support. (I’m going to cover tenant enablement later in the series.)</li></ul><p>So, that’s eight workshops with the core LZ team, and you can bring in specialists as required.</p><h3>Documenting Your LZ Design</h3><p>As you progress through the workshops, document the design decisions as you go. Capture your overall design and decisions in an artefact that Google calls the <strong>LZ Technical Design Document (TDD)</strong>.</p><p>This doc should include:</p><ul><li>Introduction and scope of the solution.</li><li>A summary of all design decisions. These should point to the relevant sections in the document.</li><li>Sections corresponding to each of the workshops.</li></ul><h3>Deploy!</h3><p>I’m going to cover this in the next installment!</p><h3>Before You Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helps me!</li><li>Please <strong>give me claps</strong>! You know you clap more than once, right?</li><li>Feel free to <strong>leave a comment</strong> 💬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on these icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*fF62z2-FT03ui0O5.png" /><figcaption>Follow and Subscribe</figcaption></figure><h3>Links</h3><ul><li><a href="https://medium.com/google-cloud/landing-zones-on-google-cloud-b42b08e1abaa">Landing Zones on Google Cloud: What It Is, Why You Need One, and How to Create One</a></li><li><a href="https://cloud.google.com/find-a-partner/">Google Cloud Partners</a></li><li><a href="https://www.epam.com/?gad_source=1&amp;gclid=Cj0KCQjwlN6wBhCcARIsAKZvD5h7jf_UxQE04IrMPl5G9rxYkX2YhMjUgUVU_jPCnO9AWjz9fnPgXLkaAiX0EALw_wcB">EPAM</a></li><li><a href="https://cloud.google.com/architecture/framework">Google Cloud Architecture Framework</a></li><li><a href="https://cloud.google.com/architecture/security-foundations">Enterprise Foundations Blueprint</a></li></ul><h3>Series Navigation</h3><ul><li><a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Series overview and structure</a></li><li><a href="https://medium.com/@derailed.dash/google-cloud-adoption-organisational-change-capabilities-upskilling-and-cloud-centre-of-15bc49ae7ae6">Org change, upskilling and CCoE Establishment</a></li><li><a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE best practice</a></li><li>Previous: <a href="https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-4-iac-gitops-and-ci-cd-google-cloud-ae3f533c6dbd">Design your Landing Zone — Design Considerations Part 4: IaC, GitOps and CI/CD</a></li><li>Next: Technical Onboarding and Landing Zone Deployment</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9b7ba8710e83" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/landing-zone-technical-onboarding-the-how-to-google-cloud-adoption-series-9b7ba8710e83">Landing Zone Technical Onboarding— the “How-To” (Google Cloud Adoption Series)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Potentials Network Security Issues In Google Cloud Platform You Need To Know]]></title>
            <link>https://medium.com/google-cloud/potentials-network-security-issues-in-google-cloud-platform-you-need-to-know-ef1fa64e01dd?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ef1fa64e01dd</guid>
            <category><![CDATA[google-cloud-networking]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-security-operations]]></category>
            <category><![CDATA[google-cloud-security]]></category>
            <category><![CDATA[network-security]]></category>
            <dc:creator><![CDATA[Dolly Aswin]]></dc:creator>
            <pubDate>Mon, 15 Apr 2024 08:06:31 GMT</pubDate>
            <atom:updated>2024-04-15T13:55:43.507Z</atom:updated>
            <content:encoded><![CDATA[<h3>The Potentials Network Security Issues In Google Cloud Platform You Need To Know</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Cv7Zt-zcepDs_3bOb1fytg.png" /></figure><p><a href="https://cloud.google.com/">Google Cloud Platform</a> (GCP) offers several features that can make network management streamlined and efficient compared to traditional on-premise deployments. There are some benefits of using network management in GCP:</p><ul><li><strong>Simplified Setup and Management<br></strong>Automation and pre-configured settings make network configuration faster and less error-prone.</li><li><strong>Scalability on Demand<br></strong>The platform automatically scales network resources to meet your application’s fluctuating traffic demands.</li><li><strong>Flexible Customization<br></strong>GCP offers a wide range of networking options and services to tailor your network environment to your specific needs.</li><li><strong>Enhanced Security<br></strong>Built-in security features like firewalls and access controls provide a strong foundation for protecting your network.</li><li><strong>Reduced Operational Overhead<br></strong>Managed services handle complex network tasks like load balancing and route optimization, freeing up your team’s time.</li></ul><p>Google Cloud Platform (GCP) strives to make network deployment and management as frictionless and efficient as possible, it<strong> </strong>empowers you to focus on your application development while ensuring a secure and efficient network environment.</p><h3>Potential Network Security Issues in GCP</h3><p>There is no such thing as a perfect network security settings, not even on Google Cloud Platform (GCP). Security is an ongoing process that requires constant vigilance and adaptation. These caused of these following things:</p><ul><li><strong>The Evolving Threat Landscape<br></strong>Hackers are constantly developing new methods to exploit vulnerabilities. Security settings that might be sufficient today could become obsolete tomorrow as new threats emerge.</li><li><strong>Human Error<br></strong>Accidental mistakes during configuration or management can introduce vulnerabilities. Even with GCP’s automation features, human oversight still plays a role.</li><li><strong>Shared Responsibility Model<br></strong>In cloud environments like GCP, security is a shared responsibility. While GCP provides a secure platform, you are ultimately responsible for configuring and managing your resources securely.</li></ul><p>Here are some potentials network security issues you face when deploying an application to Google Cloud Platform (GCP) can be broadly categorized into four main areas:</p><h4><strong>1. Misconfiguration</strong></h4><ul><li><strong>Overly Permissive Access Control<br></strong>Granting excessive permissions through firewall rules or <a href="https://cloud.google.com/security/products/iam">Identity and Access Management (IAM)</a> can create vulnerabilities. Accidentally allowing access to more resources than necessary increases the attack surface.</li><li><strong>Insecure Service Defaults<br></strong>Using default configurations for GCP services might expose vulnerabilities. Not customizing security settings for services can leave them susceptible to attacks.</li><li><strong>Public IP Addresses<br></strong>Exposing your application directly to the public internet without proper access restrictions makes it more vulnerable to unauthorized access attempts.</li></ul><h4><strong>2. Unsecured Communication</strong></h4><ul><li><strong>Unencrypted Data Transfer<br></strong>Sensitive information transmitted over the network without encryption (<a href="https://en.wikipedia.org/wiki/HTTPS">HTTPS</a>) is vulnerable to interception by attackers. This includes communication between your application and users, as well as internal communication within your GCP environment.</li><li><strong>Lack of Internal Encryption<br></strong>If communication between different components of your application within GCP isn’t encrypted, data might be exposed even within the platform.</li></ul><h4><strong>3. Outdated Security Practices</strong></h4><ul><li><strong>Unpatched Systems<br></strong>Failing to update software and operating systems with security patches leaves them susceptible to known exploits. Hackers can easily exploit these vulnerabilities to gain unauthorized access.</li><li><strong>Weak Password Management<br></strong>Reusing passwords or using weak passwords for user accounts, databases, or services significantly increases the risk of unauthorized access.</li><li><strong>Lack of Security Monitoring<br></strong>Not having proper tools in place to monitor network activity, user access logs, and system logs makes it difficult to detect suspicious behavior and potential attacks.</li></ul><h4><strong>4. Service Misconfigurations</strong></h4><ul><li><strong>Unintended Resource Sharing<br></strong>Accidentally sharing resources with other projects or users within GCP can lead to unauthorized access to your application data or resources.</li><li><strong>Misconfigured Security Groups<br></strong>Security groups act as firewalls within GCP. Incorrect configuration can leave resources exposed or restrict legitimate access.</li></ul><h3>Mitigating The Problem</h3><p>Mitigating the potentials network security issues in Google Cloud Platform (GCP) is possible. But there’s no one-size-fits-all recipe for mitigating network security issues in GCP because security is an ongoing process.</p><p>However, there are best practices and strategies you can implement to significantly reduce risks and create a strong security posture in GCP:</p><h4><strong>1. The Principle of Least Privilege</strong></h4><p>Grant users and services only the minimum permissions required to perform their tasks. This minimizes the potential damage if a security breach occurs.</p><h4><strong>2. Access Control Enforcement</strong></h4><p>Utilize Identity and Access Management (IAM) policies and firewalls to restrict access to your resources. Define granular access controls to limit who can access what, when, and from where.</p><h4><strong>3. HTTPS Everywhere</strong></h4><p>Enforce HTTPS encryption for all communication within your application and between your application and users. This ensures data confidentiality by scrambling it during transmission.</p><h4>4. <strong>Patching Regularly</strong></h4><p>Maintain a regular patching schedule to keep your software and operating systems updated with the latest security fixes. These patches address known vulnerabilities that attackers might exploit.</p><h4><strong>5. Strong Password Management</strong></h4><p>Enforce strong password policies that require complex passwords and consider implementing <a href="https://en.wikipedia.org/wiki/Multi-factor_authentication">Multi Factor Authentication (MFA)</a> for added security.</p><h4><strong>6. Network Activity Monitoring</strong></h4><p>Utilize Cloud Monitoring and other security tools to monitor network activity, user access logs, and system logs for suspicious behavior. This helps you detect potential threats early on.</p><h4><strong>7. Security Audits</strong></h4><p>Conduct regular security audits to assess your overall security posture and identify any vulnerabilities that might exist. <a href="https://en.wikipedia.org/wiki/Penetration_test">Penetration testing</a>, where ethical hackers attempt to exploit vulnerabilities, can be particularly valuable.</p><h3><strong>GCP Features for Enhanced Security</strong></h3><p>Beyond those best practices and strategies above, GCP also offers various built-in features that contribute to a secure network environment:</p><ul><li><strong>Automated Security Features<br></strong><a href="https://cloud.google.com/firewall/docs/firewalls">Firewalls</a> and access controls provide a strong foundation for network security.</li><li><strong>Security Command Center<br></strong><a href="http://loud.google.com/security/products/security-command-center">This central hub</a> provides visibility into security threats and helps you manage security posture across your GCP resources.</li><li><strong>Managed Services<br></strong>GCP offers managed services like <a href="https://cloud.google.com/security/products/security-key-management">Cloud Key Management Service (KMS) </a>for secure key storage and <a href="https://cloud.google.com/identity">Cloud Identity</a> for centralized identity management, reducing your administrative burden.</li></ul><p>Security is a shared responsibility in GCP. While Google provides a secure platform, you are ultimately responsible for configuring and managing your resources securely. By implementing these strategies, leveraging GCP’s security features, and staying informed about evolving threats, you can significantly reduce network security risks in your cloud environment.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ef1fa64e01dd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/potentials-network-security-issues-in-google-cloud-platform-you-need-to-know-ef1fa64e01dd">Potentials Network Security Issues In Google Cloud Platform You Need To Know</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kubeflow Summit Europe 2024 ✨]]></title>
            <link>https://medium.com/google-cloud/kubeflow-summit-europe-2024-ed60cada2f03?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ed60cada2f03</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[kubeflow]]></category>
            <category><![CDATA[cloud-native]]></category>
            <dc:creator><![CDATA[Malek ZAAG]]></dc:creator>
            <pubDate>Fri, 12 Apr 2024 12:26:03 GMT</pubDate>
            <atom:updated>2024-04-12T12:26:02.994Z</atom:updated>
            <content:encoded><![CDATA[<p>In this blog, we are going to highlight some keynotes of the Kubeflow Summit Europe 2024 which was held this year at Paris. Unfortunately, i couldn’t assist physically but i watched lately the cncf playlist on youtube and tried to do a small wrap up.</p><h3>💻What is Kubeflow ?</h3><p>Kubeflow is a Kubernetes-native, open-source framework for developing, managing, and running machine learning (ML) workloads. Kubeflow is an AI/ML platform that brings together several tools covering the main AI/ML use cases: data exploration, data pipelines, model training, and model serving.</p><h3>What is Kubeflow used for?</h3><p>Kubeflow solves many of the challenges involved in orchestrating machine learning pipelines by providing a set of tools and <a href="https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces">APIs</a> that simplify the process of training and deploying ML models at scale.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/271/1*rYv0ZLAHY6tYZydMto3eBw.png" /><figcaption>Kubeflow pipeline</figcaption></figure><p>Now we defined what is Kubeflow, let’s start talking about the keynotes and what they brang to us this year :</p><h3>🤖Scalable Platform for Training and Inference Using Kubeflow at CERN</h3><p>The <strong>European Organization for Nuclear Research</strong>, known as <strong>CERN</strong> is an intergovernmental organization that operates the largest particle physics laboratory in the world.</p><p>This talk will go into the details of how a kubeflow based machine learning platform handles all the steps from data preparation, interactive analysis, distributed training and inference.</p><p>The requirements at CERN :</p><ul><li>The platform should manage the full machine learning lifecycle Using multiple services can be confusing and hard to integrate.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/703/1*G5s1ZtE7MPx3xDGqwQcwdw.png" /><figcaption>MLOps lifecycle</figcaption></figure><ul><li>The platform needs to be integrated with CERN systems Auth, storage systems, etc…</li><li>The platform should be centralized to ensure easy and efficient access to GPUs and other accelerators.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/697/1*YtYQ7G-HtoFD9KrjjFlbgQ.png" /><figcaption>Reasons for centralizing resources</figcaption></figure><ul><li>The platform should be easy to use many scientists are not infrastructure experts.</li></ul><h4>⚛How MLOPS and Kubeflow are used at CERN ?</h4><p>ATLAS is one of two general-purpose detectors at the Large Hadron Collider (LHC). It investigates a wide range of physics, from the Higgs boson to extra dimensions and particles that could make up dark matter.</p><blockquote>I want to find Higgs bosons in the recorded collisions to study them.</blockquote><p>And this was their pipeline workflow to study the Higgs bosons particles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uyu37xBGm6rGxCFU11_IYg.png" /><figcaption>CERN Atlas pipeline</figcaption></figure><p><strong>Salt: </strong>General-purpose software to train multi-modal, multi-task transformer models.</p><p><strong>Katib:</strong> Used within Kubeflow to tune model Hyperparmeters.</p><p><strong>Kubeflow Notebooks:</strong> Store notebooks to be run in containers.</p><p><strong>Ceph: </strong>an open-source, distributed storage system.</p><h3>Transforming Data Science at PepsiCo: The Kubeflow Revolution</h3><p>Kubeflow is also used at Pepsi and this is for many reasons :</p><ul><li>We already have K8S clusters and infrastructure team to<br>maintain it</li><li>Lots of data deserves lots of models</li><li>Hyperparameters tuning -&gt; Katib</li><li>Serve models -&gt; KServe</li><li>Model training -&gt; training operators</li></ul><h4>The need for Kubeflow ?</h4><p>There was several reasons for using kubeflow at PepsiCo :</p><ul><li>Production is PAINFUL</li><li>With all the gaps Data Science was left to fend for themselves.</li><li>A lot of non-efficient work, going to production (or even staging) is a slog.</li></ul><p>This led to creating multiple solutions that works with kubeflow to bring the best to the AI/ML ecosystem like the Monorepo for all of Data Science/AI project :</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/541/1*qHcDzBczY0PuyoPinSss-w.png" /><figcaption>Monorepo benefits</figcaption></figure><p>Or even the Prometheus CLI that is built on top of the <strong>kfp</strong> SDK:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/444/1*p8yRdZztyx5OlA-p5UASkA.png" /><figcaption>Prometheus cli features</figcaption></figure><h4>🔄Culture Shift at PepsiCo</h4><ul><li>None of the code we built matters without rethinking our relationship to Kubeflow.</li><li>If all we built was better tooling for a broken workflow, there would be no fundamental change.</li></ul><h3>The Good, the Bad, and the Missing Parts of Kubeflow</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/326/1*cP606VqXOXJkKPWdIZBjxg.png" /><figcaption>Kubeflow</figcaption></figure><h4>😄The good parts:</h4><ul><li>pipelines</li><li>notebooks, katib, kserve</li></ul><h4>😞The bad parts:</h4><ul><li>documentation, tutorials, installation</li></ul><h4>🤔The missing parts:</h4><ul><li>Monitoring models</li><li>Model registry</li><li>Initial setup</li></ul><h4>What’s coming for kubeflow ?</h4><ul><li>finish cncf graduation</li><li>establish a TOC (technical oversigh tcommitte)</li><li>arm64 support</li><li>conformance testing</li></ul><h3>🧠AutoML &amp; Training Working Group Updates</h3><p>AutoML working group (WG) is responsible for all aspects of AutoML features on Kubeflow with Katib as the sub-project. Katib is a Kubernetes-native project with rich support for HyperParameter tuning, Neural Architecture Search, and Early Stopping algorithms.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/769/1*NJyayYjUBsFkBpzEGVvvmQ.png" /><figcaption>Katib features</figcaption></figure><h4>Katib architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DFrIJ9Std7EY_gkp7RePHA.png" /><figcaption>Katib architecture</figcaption></figure><h4>Katib future ?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/635/1*nDdJCyqqquAxV_FviVpf0g.png" /></figure><h4>Training operator overview</h4><p>Kubeflow Training Operator is a Kubernetes-native project for fine-tuning and scalable distributed training of machine learning (ML) models created with various ML frameworks such as PyTorch, TensorFlow, XGBoost, and others.</p><p>User can integrate other ML libraries such as <a href="https://huggingface.co/">HuggingFace</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, or <a href="https://github.com/NVIDIA/Megatron-LM">Megatron</a> with Training Operator to orchestrate their ML training on Kubernetes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/790/1*Cv8dwiSuJZaPcVLHF7j80Q.png" /><figcaption>Training Operator features</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/719/1*gkEN7mITUsh_BdYoTdTghg.png" /><figcaption>Example of distributed training for PyTorch</figcaption></figure><h4>Training Operator Roadmap :</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/756/1*Yz5Mr_HJfvACp_jTazPUBQ.png" /></figure><h4>Conclusion :</h4><p>With this AI trend and need for performant and cost effective deployment strategies for ML models, kubeflow can be an interesting option for companies that haven’t already migrated to cloud-native environments.</p><p>— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p>Was this helpful? Confusing? If you have any questions, feel free to contact me!</p><p>Before you leave:</p><p>👏 Clap for the story</p><p>📰 Subscribe for more posts like this @malek.zaag ⚡️</p><p>👉👈 Please follow me: <a href="https://github.com/Malek-Zaag">GitHub </a>| <a href="https://www.linkedin.com/in/malekzaag/">LinkedIn</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed60cada2f03" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/kubeflow-summit-europe-2024-ed60cada2f03">Kubeflow Summit Europe 2024 ✨</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building an intelligent alerting with Gemini & Function Calling]]></title>
            <link>https://medium.com/google-cloud/building-an-intelligent-alerting-with-gemini-function-calling-c1981d38fa94?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/c1981d38fa94</guid>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gemini]]></category>
            <dc:creator><![CDATA[CC]]></dc:creator>
            <pubDate>Fri, 12 Apr 2024 01:29:03 GMT</pubDate>
            <atom:updated>2024-04-12T01:29:03.730Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>We use alerting to send an alarm to the operation team to solve or mitigate problems as soon as possible. It’s normally going to take a relatively long time to investigate issues and then come up with clear solutions to combat the issues. I’ve always imagined it would be nicer to have a altering along side a solution, in turn helping the engineer quickly work on problem solving rather than tedious process. The Large Language Models (LLMs) provide us with this capability to move towards this direction. This article is going to explore how we can leverage Gemini and native features to address these issues.</p><p>Let’s assume our real world scenarios are the following:</p><ul><li>GKE as production environment</li><li>Microservices are running on top of GKE</li><li>Using Google Operation Suite as a centralized monitoring platform</li><li>We’re getting an alerting message triggered by breaching the metric - <strong>kubernetes.io/container/restart_count</strong> from a GKE cluster.</li></ul><p>In traditional practice when an operational engineer was paged by an alert, what probably would have happened:</p><ol><li>Interpreted the message.</li><li>Configured the credential to the target GKE cluster in order to collect information.</li><li>Ran proper commands or tools in order to collect whatever information is necessary.</li><li>Analyzed variety of collected information</li><li>Composed an incident report with a potential solution</li><li>Sent it to a related person in Chat to fix or further investigate.</li></ol><p>The LLM can help with analyzing and reasoning the alert, and work as a centralized control plane to handle various issues. However as a LLM, which doesn’t have access to real time data or any external APIs and services, they are constrained to the information and knowledge that they were trained on. This can lead to frustration for end users who are trying to use the LLM to work with the most up-to-date-information from external systems. <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling">Function calling</a> is a feature of Gemini 1.0 Pro models, which allows developers to do exactly this and helps you connect generative models to real-world data via API calls.</p><h3>Function call: The bridge to external world</h3><p>Function calling allows developers to define custom functions that can output structured data from generative models and invoke external APIs. This enables LLMs to access real-time information and interact with various services, such as SQL databases, customer relationship management systems, document repositories, and anything else with an API.</p><p>The following diagram illustrates a sequence of interactions between the user, the application, the model, and the function API. It represents a complete text modality set of interactions or a single conversation turn of a chat modality.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/516/0*_yK5piU17sWjI_9V" /></figure><p>If you want to know more about how function calling works, here’s a <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#how-works">detailed link</a>.</p><p>Let’s dive into the detail how we are going to implement this idea to leverage the power of LLMs.</p><h3>Architecture overview</h3><p>This is a high level diagram of how the workflow looks like regarding to our previous vision.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aJZ7D70lyjp8-R6vJa669g.png" /></figure><p>The major sequence is composed of the following steps:</p><ol><li>Altering is triggered by the breach threshold of the metric which could be native metrics or customized one.</li><li>The Cloud Run service will call the model to determine what the proper solution would be as well as mapped functions are going to be invoked.</li><li>Execute each step as per proposed solution.</li><li>The model will summarize the result of executing the solution and aggregated a report.</li><li>The Cloud Run service will push the report to Chat.</li></ol><h3>Build an intelligent alerting handler</h3><p>At the heart of intelligent alerting is the handler interpreting the alerting message and taking necessary actions, such as collecting logs, events, etc. Then put things together for further solution.</p><p>Firstly, we define each individual tool as a Function Declaration in order to help us configure credentials to the GKE cluster and collect information about the pod. Here are the declarations:</p><pre># Function to get GKE credential <br>get_credentail_func = FunctionDeclaration(<br>    name=&quot;get_credential&quot;,<br>    description=&quot;Configure the credential and connect to the GKE cluster.&quot;,<br>    parameters={<br>        &quot;type&quot;: &quot;object&quot;,<br>        &quot;properties&quot;: {<br>            &quot;cluster_name&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The name of the Kubernetes cluster&quot;<br>            },<br>            &quot;region&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The region of the Kubernetes cluster&quot;<br>            },<br>            &quot;project_id&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The project ID of the Kubernetes cluster&quot;<br>            },<br>            &quot;isZonal&quot;: {<br>                &quot;type&quot;: &quot;boolean&quot;,<br>                &quot;description&quot;: &quot;If the cluster is zonal, set this to True, otherwise set this to False&quot;<br>            }<br>        },<br>        &quot;required&quot;: [<br>            &quot;cluster_name&quot;,<br>            &quot;region&quot;,<br>            &quot;project_id&quot;,<br>            &quot;isZonal&quot;<br>        ],<br>        &quot;required&quot;: [<br>            &quot;cluster_name&quot;,<br>            &quot;region&quot;,<br>            &quot;project_id&quot;<br>        ]<br>    }<br>)<br><br># Function to collect information of issued pod and analyse<br>collect_pod_information_fun = FunctionDeclaration(<br>    name=&quot;collect_pod_information&quot;,<br>    description=&quot;&quot;&quot;<br>        Collect pod information from the GKE cluster.<br>    &quot;&quot;&quot;,<br>    parameters={<br>        &quot;type&quot;: &quot;object&quot;,<br>        &quot;properties&quot;: {<br>            &quot;namespace_name&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The namespace where the pod is located, which is a lowercase RFC 1123 label must consist of lower case alphanumeric characters or &#39;-&#39;, and must start and end with an alphanumeric character (e.g. &#39;my-name&#39;,  or &#39;123-abc&#39;, regex used for validation is &#39;[a-z0-9]([-a-z0-9]*[a-z0-9])?&#39;)&quot;<br>            },<br>            &quot;pod_name&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The name of the pod to get by&quot;<br>            },<br>            &quot;kubernetes_context&quot;: {<br>                &quot;type&quot;: &quot;string&quot;,<br>                &quot;description&quot;: &quot;The kubernetes context to use, defaults to the current context&quot;<br>            }<br>        },<br>        &quot;required&quot;: [<br>            &quot;kubernetes_context&quot;<br>        ],         <br>    },<br>)</pre><p>Secondly, we’re going to put tools together as a toolset and leverage Function Calling of Gemini to determine which tool should be used in order to achieve the goal.</p><pre># Function calling<br>gke_cluster_tool = Tool (<br> function_declarations= [<br> get_credentail_func, <br> collect_pod_information_fun<br> ]<br>)<br><br>...<br><br># Setup model and temperature<br>model = GenerativeModel(&quot;gemini-1.0-pro-002&quot;,<br>generation_config={&quot;temperature&quot;: 0.5}, tools=[gke_cluster_tool])<br>chat = model.start_chat(response_validation=False)</pre><p>Lastly we prepare a prompt to instruct the model on what to do when an alerting message is hitting.</p><pre>@app.post(&quot;/alerting&quot;)<br>def analyse_alerting(message: Union[str, dict]) -&gt; dict:<br> <br> prompt = &quot;&quot;&quot;<br> You are a Kubernetes expert and highly skilled in all Google Cloud services, Linux, and shell scripts. <br> Your task is to troubleshoot the problematic pod as per CONTEXT with the following steps: <br> <br> 1. Configure the credential and connect to the GKE cluster.<br> 2. Collect the pod information from the GKE cluster.<br> 3. Provide a summary of pod, a concise explanation of the issue and follow by step by step solutions to address the issues. <br> <br> Only use information that provided, do not make up information.<br> CONTEXT:<br> {}<br> &quot;&quot;&quot;.format(message[&quot;incident&quot;][&quot;resource&quot;][&quot;labels&quot;])<br> <br> response = chat.send_message(prompt)<br> ...</pre><p>Finally we see how the magic happens in your Chat when the alerting is triggered. Here is what I had:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9v8MRt5P8zIcd3BH" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GV6rVG6QPAiU1gQ8" /></figure><h3>Summary</h3><p>To deploy the reference architecture that this document describes, see the <a href="https://github.com/cc4i/llm-alerting">Building an intelligent alerting with Gemini &amp; Function calling</a> on Google Cloud GitHub repository.</p><p>The foundational model can help you with reasoning as well as aggregate a proper solution from provided functions. Gemini is a very powerful foundational model you can harness to build more complex functions than what was built in this demo. I believe an end to end automated process can make the operational engineers’ lives so much easier.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c1981d38fa94" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/building-an-intelligent-alerting-with-gemini-function-calling-c1981d38fa94">Building an intelligent alerting with Gemini &amp; Function Calling</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud SQL for PostgreSQL Optimization during Migration using Database Migration Service.]]></title>
            <link>https://medium.com/google-cloud/cloudsql-for-postgresql-optimization-during-migration-using-database-migration-service-68ba35ec3040?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/68ba35ec3040</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[cloud-sql]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[google-cloud-partner]]></category>
            <category><![CDATA[database-migration]]></category>
            <dc:creator><![CDATA[Somdyuti]]></dc:creator>
            <pubDate>Fri, 12 Apr 2024 01:28:01 GMT</pubDate>
            <atom:updated>2024-04-12T19:07:08.270Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/430/1*wTbr8sMHT1BlgQDIINWymQ.jpeg" /><figcaption>DMS now performs Parallel Full Load and Parallel CDC from PostgreSQL</figcaption></figure><p><a href="https://cloud.google.com/database-migration">Database Migration Service</a>(DMS) makes it easier for you to migrate your data to Google Cloud. This service helps you lift and shift your MySQL and PostgreSQL workloads into Cloud SQL and AlloyDB for PostgreSQL. In addition, you can lift and modernize your Oracle workloads into Cloud SQL for PostgreSQL or AlloyDB for PostgreSQL.</p><p>In this document we will discuss how you can optimize the DMS Initial Load and CDC when migrating to Cloud SQL for PostgreSQL Instance. The source can be either Oracle or PostgreSQL.</p><h4>The Cloud SQL for PostgreSQL Parameters</h4><p><strong><em>The suggested parameters need to be properly tested and verified against the chosen target Cloud SQL Instance type. They should not be set in your actual production workload.</em></strong></p><p>1. <strong>max_wal_size</strong>= 20GB.</p><p>This will make sure that Database checkpoints happen when 20GB worth of WAL data is generated. If 20 GB worth of WAL data is generated &gt; 5 minutes then checkpoints will happen every 5 minutes as per checkpoint_timeout setting.</p><blockquote>Given that, during DMS Load with the default max_wal_size which is 1.5GB for Cloud SQL PostgreSQL Enterprise and 5GB for Enterprise Plus editions, checkpoints are happening every few seconds which increases the I/O and CPU. Higher value will increase the checkpoint frequency which will reduce the I/O footprint.</blockquote><p>Also monitor the following wait events from Cloud SQL Console <strong>System Insights</strong> <strong>“WALWrite”</strong>- and the <strong>event_type</strong> will be <strong>“LWlock”</strong>. Be aware that <strong><em>WALWrite can be both a LWLock and an IO event_type.</em></strong> For frequent checkpoints it will manifest as LWLock event type as the CKPT process will wait for a latch on the WAL Segments to write the Checkpoint_change# (aka Oracle’s redo latches). High commits will manifest WALWrite as an IO wait event_type where the WAL Writer will be busy writing changes from WAL buffers to WAL Files.</p><p>There can be waits on <strong>“DataFileWrite”</strong> and <strong>“DataFileFlush”</strong> events also during very frequent and aggressive checkpoints.</p><p>2. <strong>commit_delay</strong> = 1000 (start with this and go upto 50000)</p><p>commit_delay sets the <strong>delay in microseconds between transaction commit and flushing WALs to disk</strong>. Basically it will help improve transaction throughput by performing batch commits during Bulk Inserts as it delays the WAL flush (which by default happens in every transaction commit) by 1000 microseconds after transaction commits , provided load is high enough to accumulate more transactions in that delay (which will be the case during DMS initial load)</p><blockquote>Monitor the following wait events in <strong>System Insights</strong> <strong>“WALSync” , “WALWrite” </strong>which are IO wait event_types for waits related to high commits and also the <strong>‘Transaction count’</strong> metric in System Insights.</blockquote><p>3. <strong>wal_buffers</strong> = 32–64 MB in 4 vCPU machines and 64–128 MB in 8–16 vCPU machines. It can be set to even 256MB for higher vCPU targets.</p><p>Smaller wal_buffers increase commit frequency, so increasing the value will help in initial load.</p><p>Again monitor the wait events as mentioned in (2) above.</p><p>4. <strong>Parallelism</strong>- As Postgres <strong>does not support parallel DMLs</strong> Bulk Inserts will not benefit.</p><p>5. <strong>autovacuum</strong>- Turn it to off</p><p>Note after the Initial Load is complete, make sure the autovacuum is turned On after running manual vacuum.</p><p>But run a <strong><em>manual vacuum first before releasing the database for actual production usage</em></strong> and set the following to make manual vacuum fast as it will have a lot of work to do first time.</p><p><strong>max_parallel_maintenance_workers=4</strong> (set it to number of vCPUs of the Cloud SQL Instance)</p><p><strong>maintenance_work_mem=10GB</strong></p><p>Note that manual vacuum will take memory from maintenance_work_mem.</p><p><strong>Subsequently to make autovacuum faster,</strong> set</p><p><strong><em>autovacuum_work_mem to 1GB, otherwise autovacuum workers will consume memory from maintenance_work_mem</em></strong></p><p>From Cloud SQL PostgreSQL database parameter perspective we need to <strong>tune Checkpoints and Commits during DMS Initial Load </strong>(in general for any Bulk Load operations) as they significantly affect IOs and also to an extent CPU.</p><blockquote>6. The below recommendation is very specific when the source is PostgreSQL as DMS now supports Parallel Full Load and Parallel CDC when migrating from PostgreSQL to Cloud SQL PostgreSQL or AlloyDB- <a href="https://cloud.google.com/database-migration/docs/postgres/create-migration-job#specify-source-connection-profile-info">faster PostgreSQL migrations</a></blockquote><p>The following parameter settings will help in more optimized Initial Data Copy and CDC when using PGMS (PostgreSQL Multiple Subscriptions)</p><p>In the source PostgreSQL database</p><p><strong>max_replication_slots</strong>- Set it to at least 20. It must be set to at least the number of subscriptions expected to connect which is max 10 subscriptions when DMS Parallelism is configured to Maximum (4 subscriptions per database), plus some reserve for table synchronization.</p><p><strong>max_wal_senders</strong>- Set it to higher value, preferably 20 and at least same as max_replication_slots.This controls the maximum number of concurrent connections from the target Cloud SQL PostgreSQL. With DMS Parallelism configured to Maximum there can be 4 subscriptions created per database with a max of 10 subscriptions for the PostgreSQL Cluster.</p><p>Assuming the target Instance has enough vCPU and memory available</p><p><strong>max_worker_processes</strong>- Set it to number of vCPUs in the target.</p><p>max_replication_slots- Set it to 20. It must be set to at least the number of subscriptions that will be added to the subscriber which can be upto 10, plus some reserve for table synchronization.</p><blockquote>Even with PGMS(PostgreSQL Multiple Subscriptions), when the subscription is initialized , there can be only one synchronization worker per table.(which means a table cannot be copied in parallel). Tables are copied in parallel across the subscriptions/replication sets.</blockquote><blockquote>max_logical_replication_workers and max_sync_workers_per_subscription will not affect the DMS Parallelism as these parameters influence Native Logical Replication and DMS uses pglogical.</blockquote><blockquote>7. This is very specific when you are migrating from Oracle that has many and large LOB segments. If your target Cloud SQL PostgreSQL or AlloyDB is in Version 14 and above. To make the initial load faster by 3x times, <br>change the default_toast_compression in target Cloud SQL PostgreSQL or AlloyDB to LZ4.</blockquote><p>The CLOBs and BLOBs in Oracle are converted to TEXT and BYTEA respectively in PostgreSQL. If the LOBs are large then it is extremely likely that the size of tuple/row &gt; 2KB and they will be spilled to TOAST segments (store out-of-line) in PostgreSQL (they will be stored in pg_toast schema as pg_toast_&lt;OIDoftable&gt;). TOAST data is compressed/decompressed while being inserted/queried. The default compression technique that PostgreSQL uses is PGLZ which is CPU Intensive and not as performant as LZ4 which is available from PostgreSQL 14 onwards. Using LZ4 the SELECTs speed is close to that of uncompressed data, and the speed of data insertion is upto 80% faster compared to PGLZ. Additionally you will get faster performance during SELECTs.</p><h4>Target Cloud SQL PostgreSQL Instance Sizing and Storage</h4><p>More resources you give to the target Cloud SQL Instance, the better the performance of DMS will be.</p><p><strong>Network Throughput, Disk Throughput and Disk IOPS</strong>. Network Throughput is limited by the number of vCPUs- we get <strong>250MBps Network throughput per vCPU </strong>and the <strong>Disk Throughput (0.48MBps per GB</strong>) is limited by Network Throughput. For <strong>Disk IOPS</strong> we get <strong>30 IOPS/GB</strong>.</p><p>So, the <strong><em>correct Instance size, along with the storage size will help you improve the DMS Initial Load performance</em></strong>. In general DMS will need more IOPS and decent Disk throughput and you can configure your disk size in such a way that you utilize as much as Network throughput Bandwidth for Disk Throughput (as most of the network bandwidth consumed will be from Database VM to underlying storage).</p><p>Take for example, for a 4 vCPU Cloud SQL Enterprise Instance you will get 1000 MB/s as Network throughput. So if you allocate a 600GB Disk you will get Disk Throughput close to 300 MB/s and 18000 IOPS. (I am not taking into account your Database size, of course you need to allocate more storage than your database size)</p><blockquote>So do not size the initial storage based on the source database size only, take into account the Throughput and IOPS requirement of the workload.</blockquote><p><strong><em>You can always later reduce storage using either a request with Google Support team or via Self Service Storage Shrink which is in Preview mode now. Target Cloud SQL Instance can be downscaled before the application cut-over.</em></strong></p><h4>Few More Tips</h4><p>Do not create a Regional Cloud SQL Instance during the Migration time. Enable High Availability, if you need so, after the migration is done and before application cut-over.</p><p>Do not enable Automated Backups during the time of migration.</p><p>DMS does not create Secondary Indexes and Constraints during Initial Load; it creates after the initial load completes and before CDC.</p><p>Install<strong> pg_wait_sampling extension</strong> which will be helpful in diagnosing wait events related to PostgreSQL slow performance during Migration and even after production cut-over. Query <strong>pg_stat_bgwriter</strong>, <strong>pg_stat_wal</strong> for information on Checkpoints and Commits which can be used to diagnose further. Enable the log based alerts and log based Metrics related to Frequent checkpoints.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=68ba35ec3040" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/cloudsql-for-postgresql-optimization-during-migration-using-database-migration-service-68ba35ec3040">Cloud SQL for PostgreSQL Optimization during Migration using Database Migration Service.</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>