<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 03 Apr 2024 10:48:01 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Convert Soft Breaks to Hard Breaks on Google Documents using Google Apps Script]]></title>
            <link>https://medium.com/google-cloud/convert-soft-breaks-to-hard-breaks-on-google-documents-using-google-apps-script-4edfd7fef0c5?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/4edfd7fef0c5</guid>
            <category><![CDATA[google-document]]></category>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[google-docs]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:49:27 GMT</pubDate>
            <atom:updated>2024-04-03T04:49:27.459Z</atom:updated>
            <content:encoded><![CDATA[<h3>Description</h3><p>This script converts soft breaks to hard breaks in a Google Document using Google Apps Script.</p><h3>Usage</h3><p>Follow these steps:</p><h3>1. Create a New Google Document</h3><p>Create a new Google Document and open it. Go to “View” -&gt; “Show non-printing characters” in the top menu to see line breaks in the document body (as shown in the image below).</p><h3>2. Sample Script</h3><p>Copy and paste the following script into the script editor of your Google Document.</p><p>Important: Before using this script, enable the Google Docs API in Advanced Google services. <a href="https://developers.google.com/apps-script/guides/services/advanced#enable_advanced_services">Ref</a></p><pre>function myFunction() {<br>  const doc = DocumentApp.getActiveDocument();<br>  const requests = [<br>    { replaceAllText: { replaceText: &quot;\n&quot;, containsText: { text: &quot;\u000b&quot; } } },<br>  ];<br>  Docs.Documents.batchUpdate({ requests }, doc.getId());<br>}</pre><ul><li>The script searches for soft breaks using \u000b.</li><li>It replaces them with \n, which creates hard breaks.</li></ul><h3>Testing</h3><p>Running the script on a sample document with soft breaks will convert them to hard breaks as follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/0*e-8Yqw118TRcG9yo.png" /></figure><h3>Note</h3><ul><li>The soft breaks can be searched with findText(&quot;\\v&quot;). But, when replaceText(&quot;\\v&quot;, &#39;\n&#39;) is run, it seems that \n is used as the soft breaks. I’m not sure whether this is the current specification or a bug. From this situation, I thought that Google Docs API might be able to be used. But, it seems that Google Docs API cannot search the soft breaks with \v. So, I thought that \u000b might be able to be used.</li></ul><h3>References</h3><ul><li><a href="https://developers.google.com/docs/api/reference/rest/v1/documents/batchUpdate">Method: documents.batchUpdate</a></li><li><a href="https://developers.google.com/docs/api/reference/rest/v1/documents/request#replacealltextrequest">ReplaceAllTextRequest</a></li><li>Stack Overflow Thread: <a href="https://stackoverflow.com/q/78258654">https://stackoverflow.com/q/78258654</a> (original script post)</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4edfd7fef0c5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/convert-soft-breaks-to-hard-breaks-on-google-documents-using-google-apps-script-4edfd7fef0c5">Convert Soft Breaks to Hard Breaks on Google Documents using Google Apps Script</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making sense of Vector Search and Embeddings across GCP products]]></title>
            <link>https://medium.com/google-cloud/making-sense-of-vector-search-and-embeddings-across-gcp-products-46cedad68934?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/46cedad68934</guid>
            <category><![CDATA[vector-search]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[embedding]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Steve Loh]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:49:10 GMT</pubDate>
            <atom:updated>2024-04-03T09:32:37.490Z</atom:updated>
            <content:encoded><![CDATA[<h3>Intro</h3><p>Many of you have already used the <strong>Large Language Model (LLM)</strong> from Generative AI. These models are great in performing certain creative tasks like content generation, text summarization, entity extraction and etc, but that’s not sufficient for enterprises that need to:</p><ul><li>provide accurate and up-to-date information (reducing hallucination)</li><li>offer contextual user experiences</li><li>offer secure and governed access to the data</li></ul><p>Hence comes the <strong>Retrieval-Augmented Generation technique (RAG)</strong> to fulfill those requirements. It combines the power of LLMs with the ability to reference external knowledge sources, by incorporating the following 2 systems:</p><ul><li><strong>Retrieval</strong>: When a user asks a question, RAG first searches through a database of documents or text to find relevant passages.</li><li><strong>Generation</strong>: the user then sends the retrieved information along as the context in the LLM prompt, effectively grounding LLM’s language understanding with specific knowledge in order to generate a more informed and accurate answer.</li></ul><p>So how does the RAG retrieval system find the relevant knowledge? Welcome to the world of embeddings and vector search.</p><ul><li><strong>Vector embeddings</strong> are numerical representations of text that capture the semantic meaning and relationships between words and concepts. You would use a pre-trained model to help generate embeddings. For example the Google Vertex <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings">textembedding-gecko model</a> generates a 768-dimensional embedding, while a <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings">multimodal embedding model</a> would generate a 128, 256, 512, or 1408 dimensional embedding.</li><li><strong>Vector search</strong> comes into play by comparing the user’s query embedding to the vectors representing documents or passages in the knowledge base. This comparison uses similarity metrics to find the most relevant pieces of information based on their semantic closeness to the query.</li></ul><p>Now with these concepts explained, you can implement RAG with the following steps:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/794/1*F0Yz_slYGzqsOaqf6ipHTQ.png" /></figure><ul><li>Break down large documents or text corpus using a suitable <a href="https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d">chunking strategy</a></li><li>Generate embeddings for each chunk using a selected embedding model</li><li>Store the chunked data and vector embeddings together in a vector database</li><li>User posts a prompt query</li><li>Use the same pre-trained embedding model to generate a vector embedding for the user query</li><li>Use the query embedding to search for most similar embeddings in vector database, then retrieve the corresponding data chunk</li><li>Create a new prompt for the LLM by incorporating the retrieved chunked text alongside the original user query</li></ul><p>Vector embeddings need to be stored in a vector database before you can search for embeddings. But adding a vector database to your software stack increases complexity, cost and learning curve. The great news is that most of the GCP data products already support vector out of the box, which means users will no longer need to choose between vector query and other critical database functionality. For example, all GCP transactional databases aims to fully support Vector features in near future:</p><ul><li>AlloyDB (GA)</li><li>Cloud SQL for PostgreSQL (GA)</li><li>Cloud SQL for MySQL (Preview)</li><li>Spanner (Preview)</li><li>Memorystore for Redis (Preview)</li><li>Firestore (Preview)</li><li>Bigtable (Preview)</li></ul><p>Here I will showcase vector implementation across 3 main data product families on GCP:</p><ul><li><strong>AlloyDB</strong> — Transactional database</li><li><strong>BigQuery</strong> — Enterprise data warehouse</li><li><strong>Vertex AI Vector Search</strong> — Machine learning platform</li></ul><blockquote>Disclaimer</blockquote><blockquote>I work as a Data Analytics practice lead in Google Cloud, This article is my own opinion and does not reflect the views of my employer.</blockquote><blockquote>Please take note that by the time you read this article, the information may already be obsolete as GenAI is a fast developing domain and Google Cloud is actively releasing new product features in this space.</blockquote><h3>AlloyDB</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HAe09_TeYj5Dhbg6EWs7Bg.png" /></figure><p>AlloyDB is a fully Managed, PostgreSQL-compatible cloud native database service built to deliver superior performance, scalability, and high availability for most demanding enterprise workloads. It now comes with AlloyDB AI feature suite that provides the semantic and predictive power of ML models for your data out-of-the-box.</p><h4>Setup</h4><ul><li>Make sure you already have an <a href="https://cloud.google.com/alloydb/docs/cluster-create">AlloyDB cluster and instance setup</a>.</li><li>Enable <strong>Vertex AI integration</strong> and the <strong>pgvector</strong> extensions in your AlloyDB instance:</li></ul><pre>psql &quot;host=$INSTANCE_IP user=alloydb_user dbname=vector_db&quot; -c &quot;CREATE EXTENSION IF NOT EXISTS google_ml_integration CASCADE&quot;<br><br>psql &quot;host=$INSTANCE_IP user=alloydb_user dbname=vector_db&quot; -c &quot;CREATE EXTENSION IF NOT EXISTS vector&quot;</pre><h4>Embeddings generation</h4><ul><li>Create a new column with the type <strong>vector</strong> to store the embeddings:<br>- The vector dimension should match the model that you use. For example textembedding-gecko model has 768 dimensions.<br>- AlloyDB implements embeddings as arrays of real values, but it can automatically cast from real array to a vector value.</li></ul><pre>ALTER TABLE my_products ADD COLUMN embedding_column VECTOR(768);</pre><ul><li>To generate embedding, use the <strong>embedding() </strong>function:<br>- To use <strong>textembedding-gecko model</strong>, the AlloyDB cluster must reside in <strong>region us-central1</strong> to match the region of the model.<br>- You can <a href="https://cloud.google.com/alloydb/docs/ai/invoke-predictions">invoke predictions</a> to get around the region restriction.<br>- 003 is the latest version of textembedding-gecko model. Note that it’s always advisable to specify the version tag to avoid mistakes, as a new published model may return different embeddings.</li></ul><pre>SELECT embedding(&#39;textembedding-gecko@003&#39;, &#39;Google Pixel 8 Pro redefines smartphone photography with its advanced AI-powered camera system&#39;);</pre><ul><li>To generate embedding value based on another column:</li></ul><pre>UPDATE my_products SET embedding_column = embedding(( &#39;textembedding-gecko@003&#39;, product_description);</pre><ul><li>Alternatively, you can also create an embedding column with default value generated from another column:</li></ul><pre>ALTER TABLE my_products ADD COLUMN embedding_column vector GENERATED ALWAYS AS (embedding(&#39;textembedding-gecko@003&#39;, product_description)) STORED;</pre><h4>Vector index</h4><ul><li>By default pgvector performs exact nearest neighbor search which provides perfect recall. It can support approximate nearest-neighbor searching through indexing of HNSW or IVFFlat. AlloyDB provides built-in optimizations for pgvector by adding a scalar quantization feature (SQ8) to IVF index creation that can significantly speed up queries.<br>- SQ8 supports vectors with up to 8000 dimensions. <br>- You can choose among 3 distance functions: vector_l2_ops (L2 distance), vector_ip_ops (Inner product) or vector_cosine_ops (Cosine distance).</li></ul><pre>CREATE INDEX embedding_column_idx ON my_products<br>  USING ivf (embedding_column vector_l2_ops)<br>  WITH (lists = 20, quantizer = &#39;SQ8&#39;);</pre><h4>Vector search</h4><ul><li>Perform vector search using the pgvector nearest-neighbor operator &lt;-&gt; in order to find the database rows with the most semantically similar embeddings:</li></ul><pre>SELECT product_name FROM my_products<br>    ORDER BY embedding_column<br>    &lt;-&gt; embedding((&#39;textembedding-gecko@003&#39;, &#39;I need a phone that provides the best photography quality&#39;)::vector<br>    LIMIT 10;</pre><p>Check the following links for more information:</p><ul><li><a href="https://cloud.google.com/alloydb/docs/ai/work-with-embeddings">Work with vector embeddings on AlloyDB</a></li><li><a href="https://github.com/pgvector/pgvector#indexing">pgvector indexing</a></li></ul><h3>BigQuery</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2XV9VBLwcU7bdryyp-U5DQ.png" /></figure><h4>Setup</h4><ul><li>BigQuery is a serverless service and no resource setup is needed for it.</li><li>Create a remote connection to Vertex AI remote models:</li></ul><pre>bq mk --connection --location=US --project_id={PROJECT_ID}  --connection_type=CLOUD_RESOURCE vertex_embeddings</pre><ul><li>Grant ‘Vertex AI User’ role to the service account of the created connection:</li></ul><pre>gcloud projects add-iam-policy-binding {PROJECT_ID} \<br>  --member=&#39;serviceAccount:{CONNECTION_SERVICE_ACCOUNT}&#39; \<br>  --role=&#39;roles/aiplatform.user&#39;</pre><h4>Embeddings generation</h4><ul><li>Create a remote embedding model to represented the hosted textembedding-gecko model:</li></ul><pre>CREATE OR REPLACE MODEL test_embeddings.llm_embedding_model<br>  REMOTE WITH CONNECTION `us.vertex_embeddings`<br>  OPTIONS(ENDPOINT=&#39;textembedding-gecko@003&#39;);</pre><ul><li>You can now generate text embeddings using the <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-embedding">ML.GENERATE_EMBEDDING</a> function:<br>- We use data from a public dataset table called imdb.reviews in this example.<br>- The text_embedding column is of type ARRAY&lt;FLOAT&gt; with 768-dimensions.</li></ul><pre>CREATE OR REPLACE TABLE test_embeddings.embedded_reviews<br>AS SELECT content as review, text_embedding<br>FROM<br>  ML.GENERATE_TEXT_EMBEDDING(<br>    MODEL `test_embeddings.llm_embedding_model`,<br>    (SELECT review as content<br>      FROM bigquery-public-data.imdb.reviews limit 8000<br>    ),<br>    STRUCT(TRUE AS flatten_json_output)<br>  );</pre><h4>Vector index</h4><ul><li>Create vector index on the embeddings column. Vector index enables Approximate Nearest Neighbor search to help improve vector search performance.<br>- Currently supported distance types are EUCLIDEAN (L2) and COSINE.<br>- Currently only IVF is supported for index type.<br>- The created index is fully managed by BigQuery, the refresh happens automatically as data changes.<br>- The metadata information of the vector index is available via <a href="https://cloud.google.com/bigquery/docs/information-schema-vector-indexes">INFORMATION_SCHEMA.VECTOR_INDEXES</a> view.</li></ul><pre>CREATE VECTOR INDEX embedded_reviews_idx ON test_embeddings.embedded_reviews(text_embedding) OPTIONS(distance_type = &#39;EUCLIDEAN&#39;, index_type=&#39;IVF&#39;);</pre><h4>Vector search</h4><ul><li>Use VECTOR_SEARCH function to perform text similarity search:<br>- It first generates embeddings from the text query, then compares them to the column `embeddings.embedded_reviews.text_embedding`.</li></ul><pre>SELECT<br>  *<br>FROM<br>  VECTOR_SEARCH( TABLE `embeddings.embedded_reviews`, &#39;text_embedding&#39;, (<br>    SELECT<br>      ml_generate_embedding_result,<br>      content AS query<br>    FROM<br>      ML.GENERATE_EMBEDDING( MODEL `embeddings.llm_embedding_model`,<br>        (<br>        SELECT &#39;Our family enjoyed this movie, especially the kids were so fascinated by the magical world&#39; AS content<br>        )) <br>    ),<br>    top_k =&gt; 5);</pre><p>Check the following links for more information:</p><ul><li><a href="https://cloud.google.com/bigquery/docs/vector-search">Search embeddings with vector search</a></li><li>A <a href="https://github.com/steveloh/demo/blob/main/bigquery/notebook/BigQuery%20Embedding%20and%20Vector%20Search.ipynb">notebook</a> that I created to showcase embedding and vector search in BigQuery.</li></ul><h3>Vertex AI Vector Search</h3><p>Vertex AI is a unified machine learning platform that simplifies and accelerates the end-to-end process of building, deploying, and managing ML models at scale. Vector Search (previously known as Matching Engine) provides highly scalable and performant vector similarity search.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6Qe6HxbxDsnrxlpgz5BGIA.png" /></figure><p>Following code snippets are based on Python.</p><h4>Setup</h4><ul><li>Import aiplatform package:</li></ul><pre>from google.cloud import aiplatform<br>aiplatform.init(project=PROJECT_ID, location=LOCATION)</pre><ul><li>Vector Search does not provide services to generate embeddings. You can for example generate embeddings via BigQuery, then export the embeddings to a file in a storage bucket, before importing them into Vector Search.</li></ul><h4>Vector index</h4><ul><li>Create a vector index endpoint, which is a server instance that accepts query requests for your index.</li></ul><pre>my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(<br>    display_name = f&quot;index-endpoint-{PROJECT_ID}&quot;,<br>    public_endpoint_enabled = True<br>)</pre><ul><li>Create a vector search index:<br>- EMBEDDING_BUCKET_URI is where you store the files with embeddings, read here about the required <a href="https://cloud.google.com/vertex-ai/docs/vector-search/setup/format-structure">input data format and structure</a><br>- Approximate_neighbors_count specifies the number of neighbors to find through approximate search before exact reordering is performed.<br>- See here for available <a href="https://cloud.google.com/vertex-ai/docs/vector-search/configuring-indexes#distance-measure-type">distance measure type</a>.</li></ul><pre><br>my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(<br>    display_name={DISPLAY_NAME},<br>    contents_delta_uri={EMBEDDING_BUCKET_URI},<br>    dimensions=768,<br>    approximate_neighbors_count=10,<br>    distance_measure_type=&quot;DOT_PRODUCT_DISTANCE&quot;,<br>)</pre><ul><li>Deploy the index to the index endpoint:</li></ul><pre>my_index_endpoint = my_index_endpoint.deploy_index(<br>    index=my_index, deployed_index_id = DEPLOYED_INDEX_ID<br>)</pre><h4>Vector search</h4><ul><li>Now you can search the vector index using a query embedding:</li></ul><pre># get the query embedding<br>model = TextEmbeddingModel.from_pretrained(&quot;textembedding-gecko@003&quot;)  <br>query = &quot;Our family enjoyed this movie, especially the kids were so fascinated by the magical world&quot;<br>query_embeddings = model.get_embeddings([query])[0]<br><br># query the index endpoint to find 3 nearest neighbors.<br>response = my_index_endpoint.find_neighbors(<br>    deployed_index_id=my_index_endpoint.deployed_indexes[0].id,<br>    queries=[query_embeddings.values],<br>    num_neighbors=3,<br>)</pre><p>I have created a <a href="https://github.com/steveloh/demo/blob/main/vertex/vector-search/vertex-vector-search-python-call.ipynb">notebook</a> to demonstrate how to do vector search in Vertex AI.</p><h3>Summary</h3><p>2023 was the booming year of GenAI foundation models, while this year organizations will focus on building applications harnessing values from these models. This may include accelerating organization’s access to insights, improving productivity, streamlining operations and business processes and building innovative product services. Vector storage and vector search are the backbone for storing and organizing the rich semantic information to ground generative AI models. <strong>Their ability to handle various structures of data, power meaningful search, scale efficiently, and support rapid development makes them the ideal engine for the next generation of AI innovation.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=46cedad68934" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/making-sense-of-vector-search-and-embeddings-across-gcp-products-46cedad68934">Making sense of Vector Search and Embeddings across GCP products</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deterministic Generative AI with Gemini Function Calling in Java]]></title>
            <link>https://medium.com/google-cloud/using-gemini-function-calling-in-java-for-deterministic-generative-ai-responses-4c86a5ab80a9?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/4c86a5ab80a9</guid>
            <category><![CDATA[calling-function]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[generative-ai]]></category>
            <dc:creator><![CDATA[Abirami Sukumaran]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:48:45 GMT</pubDate>
            <atom:updated>2024-04-03T04:48:45.378Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/738/1*cwsJqTsvzudgNKgCZqg8ow.png" /><figcaption>Navigate better with deterministic Generative AI &amp; Reverse Geocoding API</figcaption></figure><p>Generative AI models are remarkable at understanding and responding to natural language. But what if you need precise, predictable outputs for critical tasks like address standardization? Traditional generative models can sometimes provide different responses at different times for the same prompts, potentially leading to inconsistencies. That’s where Gemini’s Function Calling capability shines, allowing you to deterministically control elements of the AI’s response.</p><p>In this blog, we’ll illustrate this concept with the address completion and standardization use case. For this we will be building a Java Cloud Function that:</p><ol><li><strong>Takes latitude and longitude coordinates</strong></li><li><strong>Calls the Google Maps Geocoding API to get corresponding addresses</strong></li><li><strong>Uses Gemini 1.o Pro Function Calling feature to deterministically standardize and summarize those addresses in a specific format that we need</strong></li></ol><p>Let’s dive in!</p><h3>Gemini Function Calling</h3><p>Gemini Function Calling stands out in the Generative AI era because it lets you blend the flexibility of generative language models with the precision of traditional programming. Here’s how it works:</p><p><strong>Defining Functions:</strong> You describe functions as if you were explaining them to a coworker. These descriptions include:</p><ol><li>The function’s name (e.g., “getAddress”)</li><li>The parameters it expects (e.g., “latlng” as a string)</li><li>The type of data it returns (e.g., a list of address strings)</li></ol><p><strong>“Tools” for Gemini:</strong> You package function descriptions in the form of API specification into “Tools”. Think of a tool as a specialized toolbox Gemini can use to understand the functionality of the API.</p><p><strong>Gemini as API Orchestrator:</strong> When you send a prompt to Gemini, it can analyze your request and recognize where it can use the tools you’ve provided. Gemini then acts as a smart orchestrator:</p><ol><li>Generates API Parameters: It produces the necessary parameters to call your defined functions.</li><li>Calls External APIs: Gemini doesn’t call the API on your behalf. You call the API based on the parameters and signature that Gemini function calling has generated for you.</li><li>Processes Results: Gemini feeds the results from your API calls back into its generation, letting it incorporate structured information into its final response which you can process in any way you desire for your application.</li></ol><h3>High Level Flow Diagram</h3><p>This diagram represents the flow of data and steps involved in the implementation. Please note that the owner for the respective step is mentioned in the text underneath.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2-HYu2sGPsNwe46UdJuVdA.png" /><figcaption>High level flow diagram of the implementation</figcaption></figure><h3>Industry Use cases and why it matters</h3><p>Below are some of the examples and industry specific use cases for function calling with Gemini.</p><ol><li><strong>Geocoding (Our Use Case):</strong> You’ve seen how to define a “getAddress” function and use it within the context of address standardization.</li><li><strong>Data Validation:</strong> Imagine a function called “validateEmail” that takes a string and checks it against an email validation service. Gemini can help you formulate the parameters string so you can call the email validation API to ensure the quality of generated responses. Remember, Gemini does not make the API call for you.</li><li><strong>Fact-Checking:</strong> Define a “lookupFact” function. Gemini could use this to consult a trusted knowledge base, making its responses more reliable within specific domains.</li></ol><p><strong>Why Function Calling Matters</strong></p><p><strong>Bridging Two Worlds:</strong> LLMs can’t know everything (especially private information, customer details, news that are more recent than its knowledge cut-off date), and by integrating function calling, it’s able to extend its knowledge and capabilities to bridge the gap between open-ended generative AI and the deterministic execution of traditional code.</p><p><strong>Controlled Creativity:</strong> Function Calling injects structured, predictable elements into Gemini’s creative process, ideal for critical use cases or maintaining consistency.</p><p><strong>Building Agent:</strong> Chain together multiple function calls and Gemini processing steps, enabling multi-stage generative AI workflows.</p><h3>Create the Java Cloud Function</h3><p>This is the Gen 2 Cloud Function implementation where we will invoke the Gemini model to orchestrate the input for function Calling, invoke the API and then process the response in another Gemini call and deploy it to a REST endpoint.</p><h3>Setup</h3><ol><li>In the <a href="https://console.cloud.google.com/">Google Cloud Console</a>, on the project selector page, select or create a Google Cloud <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects">project</a>.</li><li>Make sure that billing is enabled for your Cloud project. Learn how to <a href="https://cloud.google.com/billing/docs/how-to/verify-billing-enabled">check if billing is enabled on a project</a>.</li><li>You will use Cloud Shell, a command-line environment running in Google Cloud that comes preloaded with bq. From the Cloud Console, click Activate Cloud Shell on the top right corner.</li><li>Just for support with building and delivering the app, let’s enable Duet AI. Navigate to <a href="https://console.cloud.google.com/marketplace/product/google/cloudaicompanion.googleapis.com">Duet AI Marketplace</a> to enable the API. You can also run the following command in the Cloud Shell terminal:</li></ol><pre>gcloud services enable cloudaicompanion.googleapis.com –project PROJECT_ID</pre><p>5. Enable necessary APIs for this implementation if you haven’t already.</p><p>Alternative to the gcloud command is through the console using this <a href="https://console.cloud.google.com/apis/enableflow?apiid=bigquery.googleapis.com,bigqueryconnection.googleapis.com,aiplatform.googleapis.com">link</a>.</p><h3>Java Cloud Function</h3><ol><li>Open Cloud Shell Terminal and navigate to the root directory or your default workspace path.</li><li>Click the Cloud Code Sign In icon from the bottom left corner of the status bar and select the active Google Cloud Project that you want to create the Cloud Functions in.</li><li>Click the same icon again and this time select the option to create a new application.</li><li>In the Create New Application pop-up, select Cloud Functions application:</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*n53iiiF7ZuyG9J4U" /><figcaption>Create New Application page 1</figcaption></figure><p>5. Select Java: Hello World option from the next pop-up:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jRH5u3oXSLqx-a_k" /><figcaption>Create New Application page 2</figcaption></figure><p>6. Provide a name for the project in the project path. In this case, “GeminiFunctionCalling”.</p><p>7. You should see the project structure opened up in a new Cloud Shell Editor view:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*z7PMp8qsGOUik2j1" /><figcaption>Cloud Shell Editor showing the project structure with pom.xml open</figcaption></figure><p>8. Now go ahead and add the necessary dependencies within the &lt;dependencies&gt;… &lt;/dependencies&gt; tag in the pom.xml file. You can access the entire <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling/blob/main/pom.xml">pom.xml</a> from this project’s github <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling">repository</a>.</p><pre>&lt;dependency&gt;<br>      &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;<br>      &lt;artifactId&gt;google-cloud-vertexai&lt;/artifactId&gt;<br>&lt;/dependency&gt;<br><br>&lt;dependency&gt;<br>  &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;<br>  &lt;artifactId&gt;gson&lt;/artifactId&gt;<br>  &lt;version&gt;2.10&lt;/version&gt;<br>&lt;/dependency&gt;</pre><p>9. You can access the entire HelloWorld.java (or whatever you changed it to) class from the github <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling/blob/main/src/main/java/cloudcode/helloworld/HelloWorld.java">link</a>. Let’s understand Function Calling by breaking down this class:</p><p><strong>Prompt Input:<br></strong>In this example, this is what the input prompt looks like:</p><pre>“What&#39;s the address for the latlong value 40.714224,-73.961452”</pre><p>You can find the below code snippet relevant to the input prompt in the file:</p><pre>String promptText = &quot;What&#39;s the address for the latlong value &#39;&quot; + latlngString + &quot;&#39;?&quot;; //40.714224,-73.961452</pre><p><strong>API Specification / Signature Definition:<br></strong>We decided to use the <a href="https://developers.google.com/maps/documentation/geocoding/requests-reverse-geocoding">Reverse Geocoding API</a>. In this example, this is what the API spec looks like:</p><pre>/* Declare the function for the API that we want to invoke (Geo coding API) */<br>      FunctionDeclaration functionDeclaration = FunctionDeclaration.newBuilder()<br>          .setName(&quot;getAddress&quot;)<br>          .setDescription(&quot;Get the address for the given latitude and longitude value.&quot;)<br>          .setParameters(<br>              Schema.newBuilder()<br>                  .setType(Type.OBJECT)<br>                  .putProperties(&quot;latlng&quot;, Schema.newBuilder()<br>                      .setType(Type.STRING)<br>                      .setDescription(&quot;This must be a string of latitude and longitude coordinates separated by comma&quot;)<br>                      .build())<br>                  .addRequired(&quot;latlng&quot;)<br>                  .build())<br>          .build();</pre><p><strong>Gemini to orchestrate the prompt with the API specification:<br></strong>This is the part where we send the prompt input and the API spec to Gemini:</p><pre>// Add the function to a &quot;tool&quot;<br> Tool tool = Tool.newBuilder()<br> .addFunctionDeclarations(functionDeclaration)<br> .build();<br><br><br>// Invoke the Gemini model with the use of the  tool to generate the API parameters from the prompt input.<br>GenerativeModel model = GenerativeModel.newBuilder()<br> .setModelName(modelName)<br> .setVertexAi(vertexAI)<br> .setTools(Arrays.asList(tool))<br> .build();<br>GenerateContentResponse response = model.generateContent(promptText);<br>Content responseJSONCnt = response.getCandidates(0).getContent();</pre><p>The response from this is the orchestrated parameters JSON to the API. Output from this step would look like below:</p><pre>role: &quot;model&quot;<br>parts {<br>  function_call {<br>    name: &quot;getAddress&quot;<br>    args {<br>      fields {<br>        key: &quot;latlng&quot;<br>        value {<br>          string_value: &quot;40.714224,-73.961452&quot;<br>        }<br>      }<br>    }<br>  }<br>}</pre><p>The parameter that needs to be passed to the Reverse Geocoding API is this:</p><p>“latlng=40.714224,-73.961452”</p><p>From the Content object, you can get the Part, call the hasFunctionCall() method to know if it has a function call request that’s returned by the LLM. Call getFunctionCall() to get a FunctionCall object. Use the hasArgs() method to check if there are arguments, and then a getArgs() method to get the actual arguments. It’s a protobuf Struct object. Match the orchestrated result to the format “latlng=VALUE”. Refer to the full code <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling/blob/main/src/main/java/cloudcode/helloworld/HelloWorld.java">here</a>.</p><p><strong>Invoke the API:<br></strong>At this point you have everything you need to invoke the API. The part of the code that does it is below:</p><pre>// Create a request<br>      String url = API_STRING + &quot;?key=&quot; + API_KEY + params;<br>      java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()<br>          .uri(URI.create(url))<br>          .GET()<br>          .build();<br>      // Send the request and get the response<br>      java.net.http.HttpResponse&lt;String&gt; httpresponse = client.send(request, java.net.http.HttpResponse.BodyHandlers.ofString());<br>      // Save the response<br>      String jsonResult =  httpresponse.body().toString();</pre><p>The string jsonResult holds the stringified response from the reverse Geocoding API. It looks something like this: (This is a formatted version of the output. Please note the result is truncated as well).</p><pre>“...277 Bedford Ave, Brooklyn, NY 11211, USA; 279 Bedford Ave, Brooklyn, NY 11211, USA; 277 Bedford Ave, Brooklyn, NY 11211, USA;...”</pre><p><strong>Process the API response and prepare the prompt:<br></strong>The below code processes the response from the API and prepares the prompt with instructions on how to process the response:</p><pre>// Provide an answer to the model so that it knows what the result<br>      // of a &quot;function call&quot; is.<br>      String promptString =<br>      &quot;You are an AI address standardizer for assisting with standardizing addresses accurately. Your job is to give the accurate address in the standard format as a JSON object containing the fields DOOR_NUMBER, STREET_ADDRESS, AREA, CITY, TOWN, COUNTY, STATE, COUNTRY, ZIPCODE, LANDMARK by leveraging the address string that follows in the end. Remember the response cannot be empty or null. &quot;;<br><br><br>Content content =<br>          ContentMaker.fromMultiModalData(<br>              PartMaker.fromFunctionResponse(<br>                  &quot;getAddress&quot;,<br>                  Collections.singletonMap(&quot;address&quot;, formattedAddress)));<br>      String contentString = content.toString();<br>      String address = contentString.substring(contentString.indexOf(&quot;string_value: \&quot;&quot;) + &quot;string_value: \&quot;&quot;.length(), contentString.indexOf(&#39;&quot;&#39;, contentString.indexOf(&quot;string_value: \&quot;&quot;) + &quot;string_value: \&quot;&quot;.length()));<br><br><br>      List&lt;SafetySetting&gt; safetySettings = Arrays.asList(<br>        SafetySetting.newBuilder()<br>            .setCategory(HarmCategory.HARM_CATEGORY_HATE_SPEECH)<br>            .setThreshold(SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH)<br>            .build(),<br>        SafetySetting.newBuilder()<br>            .setCategory(HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT)<br>            .setThreshold(SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH)<br>            .build()<br>    );</pre><p><strong>Invoke Gemini and return the standardized address :<br></strong>The below code passes the processed output from the above step as prompt to Gemini:</p><pre>GenerativeModel modelForFinalResponse = GenerativeModel.newBuilder()<br>      .setModelName(modelName)<br>      .setVertexAi(vertexAI)<br>      .build();<br>      GenerateContentResponse finalResponse = modelForFinalResponse.generateContent(promptString + &quot;: &quot; + address, safetySettings);<br>       System.out.println(&quot;promptString + content: &quot; + promptString + &quot;: &quot; + address);<br>        // See what the model replies now<br>        System.out.println(&quot;Print response: &quot;);<br>        System.out.println(finalResponse.toString());<br>        String finalAnswer = ResponseHandler.getText(finalResponse);<br>        System.out.println(finalAnswer);</pre><p>The finalAnswer variable has the standardized address in JSON format. Sample output below:</p><pre>{&quot;replies&quot;:[&quot;{ \&quot;DOOR_NUMBER\&quot;: null, \&quot;STREET_ADDRESS\&quot;: \&quot;277 Bedford Ave\&quot;, \&quot;AREA\&quot;: \&quot;Brooklyn\&quot;, \&quot;CITY\&quot;: \&quot;New York\&quot;, \&quot;TOWN\&quot;: null, \&quot;COUNTY\&quot;: null, \&quot;STATE\&quot;: \&quot;NY\&quot;, \&quot;COUNTRY\&quot;: \&quot;USA\&quot;, \&quot;ZIPCODE\&quot;: \&quot;11211\&quot;, \&quot;LANDMARK\&quot;: null} null}&quot;]}</pre><p>Now that you have understood how Gemini Function Calling works with the address standardization use case, go ahead and deploy the Cloud Function.</p><p>10. Now that you have understood how Gemini Function Calling works with the address standardization use case, go ahead and deploy the Cloud Function.</p><pre>gcloud functions deploy gemini-fn-calling --gen2 --region=us-central1 --runtime=java11 --source=. --entry-point=cloudcode.helloworld.HelloWorld --trigger-http</pre><p>The result for this deploy command would be a REST URL in the format as below :</p><p><a href="https://us-central1-YOUR_PROJECT_ID.cloudfunctions.net/gemini-fn-calling"><strong>https://us-central1-YOUR_PROJECT_ID.cloudfunctions.net/gemini-fn-calling</strong></a></p><p>11. Test this Cloud Function by running the following command from the terminal:</p><pre>gcloud functions call gemini-fn-calling --region=us-central1 --gen2 --data &#39;{&quot;calls&quot;:[[&quot;40.714224,-73.961452&quot;]]}&#39;</pre><p>Response for a random sample prompt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*L2ME8WPaWun8XBIX" /><figcaption>Cloud Shell Terminal response for the Cloud Function call</figcaption></figure><pre>  &#39;{&quot;replies&quot;:[&quot;{ \&quot;DOOR_NUMBER\&quot;: \&quot;277\&quot;, \&quot;STREET_ADDRESS\&quot;: \&quot;Bedford Ave\&quot;, \&quot;AREA\&quot;:<br>  null, \&quot;CITY\&quot;: \&quot;Brooklyn\&quot;, \&quot;TOWN\&quot;: null, \&quot;COUNTY\&quot;: \&quot;Kings County\&quot;, \&quot;STATE\&quot;:<br>  \&quot;NY\&quot;, \&quot;COUNTRY\&quot;: \&quot;USA\&quot;, \&quot;ZIPCODE\&quot;: \&quot;11211\&quot;, \&quot;LANDMARK\&quot;: null}}```&quot;]}&#39;</pre><p>The request and response parameters of this Cloud Function are implemented in a way that is compatible with BigQuery’s remote function invocation. It can be directly consumed from BigQuery data in-place. It means that if your data input (lat and long data) lives in BigQuery then you can call the remote function on the data and get the function response which can be stored or processed within BigQuery directly. To learn how to leverage this Cloud Function in performing in-place LLM insights on your database, refer to this <a href="https://medium.com/google-cloud/in-place-llm-insights-bigquery-gemini-for-structured-unstructured-data-analytics-fdfac0421626">blog</a>.</p><h3>Conclusion</h3><p>This project has demonstrated the power of Gemini Function Calling, transforming a generative AI task into a deterministic, reliable process. If you work with generative AI, don’t let its sometimes-unpredictable nature hold you back. Use the power of Gemini 1.0 Pro Function Calling feature and create applications that are as innovative as they are dependable. Start exploring how you can incorporate this feature into your own work! Are there datasets you could validate, information gaps you could fill, or tasks that could be automated with structured calls embedded within your generative AI responses? Here is the link to the <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling">repo</a> and for further reading, refer to the documentation for <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>, <a href="https://cloud.google.com/bigquery/docs/remote-functions">BigQuery Remote Functions</a>, and <a href="https://cloud.google.com/functions">Cloud Functions</a> for more in-depth guidance in these areas.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4c86a5ab80a9" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-gemini-function-calling-in-java-for-deterministic-generative-ai-responses-4c86a5ab80a9">Deterministic Generative AI with Gemini Function Calling in Java</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Multi-Stage Builds in Kubernetes]]></title>
            <description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://medium.com/google-cloud/multi-stage-builds-in-kubernetes-83d9916c8ffa?source=rss----e52cf94d98af---4"><img src="https://cdn-images-1.medium.com/max/1400/1*rjQc0TCvBrqfDsDO-xlXGA.png" width="1400"></a></p><p class="medium-feed-snippet">As developers constantly seek ways to optimize our workflows without compromising on quality or security. Enter multi-stage builds in&#x2026;</p><p class="medium-feed-link"><a href="https://medium.com/google-cloud/multi-stage-builds-in-kubernetes-83d9916c8ffa?source=rss----e52cf94d98af---4">Continue reading on Google Cloud - Community »</a></p></div>]]></description>
            <link>https://medium.com/google-cloud/multi-stage-builds-in-kubernetes-83d9916c8ffa?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/83d9916c8ffa</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[devops]]></category>
            <dc:creator><![CDATA[The kube guy]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:48:07 GMT</pubDate>
            <atom:updated>2024-04-03T04:48:07.575Z</atom:updated>
        </item>
        <item>
            <title><![CDATA[How Google Admins can Save Money by Understanding the Relationship between Google Cloud Identity…]]></title>
            <link>https://medium.com/google-cloud/how-google-admins-can-save-money-by-understanding-the-relationship-between-google-cloud-identity-e4ad6d3f844c?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/e4ad6d3f844c</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-workspace]]></category>
            <category><![CDATA[identity-management]]></category>
            <category><![CDATA[cloud-identity]]></category>
            <category><![CDATA[google-cloud-identity]]></category>
            <dc:creator><![CDATA[Allan Alfonso]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:47:47 GMT</pubDate>
            <atom:updated>2024-04-03T04:47:47.094Z</atom:updated>
            <content:encoded><![CDATA[<h3>How Google Admins can Save Money by Understanding the Relationship between Google Cloud Identity, Google Workspace, and Google Cloud</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/694/1*M8Tafmr1Dp7vaKFyAQZ8eQ.png" /><figcaption>Google Cloud Identity</figcaption></figure><h3>What is Google Cloud Identity?</h3><p>Google Cloud Identity is Google’s identity provider (idP) that is used by both Workspace and Google Cloud.</p><p>To access it, goto <a href="http://admin.google.com">admin.google.com</a>. There are <a href="https://support.google.com/cloudidentity/answer/7431902">two versions</a>: <strong>Cloud Identity Free and Cloud Identity Premium</strong>. The premium version includes all the features of the free version and adds endpoint device and security capabilities.</p><p>By default, <strong>Cloud Identity Free includes 50 free licenses</strong> but you can request more free licenses by following these <a href="https://cloud.google.com/identity/pricing">instructions</a>.</p><h3>Do I need to purchase a Google Workspace license to use Google Cloud?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Bl2gkJCR4jIX-yfbC7D10Q.png" /><figcaption>The Licenses Assigned to a User</figcaption></figure><p>No.</p><p>Users think they need a Google Workspace license to use Google Cloud because Google asks you to try Google Workspace when you sign up using a <a href="mailto:you@your-company.com"><strong>you@your-company.com</strong></a> email address. You will have to select a Google Workspace trial but you can cancel it before the trial period ends so you are not charged. Behind the scenes, a Cloud Identity license is provisioned and you can verify the licenses a user has by going to “<strong>Directory → Users → &lt;user&gt; → Licenses</strong>”.</p><p>Google Workspace and Cloud Identity licenses are separated so Google Workspace does not become a requirement to use Google Cloud.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/468/1*d4Gc71k1hISpOy1JQRECyA.png" /><figcaption>No Licenses Error</figcaption></figure><p>Another source of confusion is when you get a “<strong>No license available for new user</strong>” error when creating a new Google Cloud user in Cloud Identity.</p><p>The error message says to purchase more Google Workspace licenses so users think they need more Google Workspace licenses to use Google Cloud. To resolve this error, goto “<strong>Billing → License settings</strong>” and <a href="https://knowledge.workspace.google.com/kb/how-to-manage-automatic-licensing-settings-000006369"><strong>disable “Automatic Licensing</strong></a><strong>”</strong>. Automatic Licensing automatically assigns a Google Workspace license and throws an error if there are no more Google Workspace licenses left. By disabling automatic licensing, only Cloud Identity Free licenses will be assigned to new users so you do not have to purchase additional Google Workspace licenses.</p><p>If you need a mix of Workspace and non-Workspace users, manually assign the Workspace and Cloud Identity licenses to the respective users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*avDprGdypI6xMAIFl-uWlg.png" /><figcaption>Disable Automatic Licensing</figcaption></figure><h3>Can I get Cloud Identity Premium for Free?</h3><p>A lot of people don’t know that <a href="https://support.google.com/a/answer/6043385?co=DASHER._Family=Enterprise&amp;sjid=17574536566002910901-NA"><strong>Cloud Identity Premium is included with Google Workspace Enterprise Editions</strong></a><strong>.</strong></p><p>If you’re already a Google Workspace customer and are interested in the security and device management features of <a href="https://support.google.com/cloudidentity/answer/7431902">Cloud Identity Premium</a>, then talk to your account team about upgrading to <a href="https://support.google.com/a/answer/6043385?co=DASHER._Family%3DEnterprise&amp;oco=0">Google Workspace Enterprise Edition</a>. You can save money and get additional features by bundling Google Workspace and Cloud Identity Premium together.</p><p>From my experience, the additional features of Google Workspace Enterprise Edition that users find most valuable are data loss prevention, context-aware access (aka Zero Trust access), and a larger number of participants in Google Meet.</p><h3>Summary</h3><ul><li>Google Cloud Identity is Google’s Identity Provider for Google Services.</li><li>Google Workspace is not required to use Google Cloud. Save money by not purchasing unnecessary Workspace licenses.</li><li>Disable Automatic Licensing and manually assign Workspace and Cloud Identity licenses to have a mix of Workspace and Non-Workspace users. Save money by only purchasing the correct number of licenses for your Workspace users.</li><li>Save money by getting Cloud Identity Premium free with a Google Workspace Enterprise Edition subscription.</li></ul><h3>Free Training</h3><p>Upgrade your Cloud Identity Skills with this FREE course from Coursera.</p><p><a href="https://www.coursera.org/learn/cloud-identity">Introduction to Cloud Identity</a></p><p>And this FREE YouTube course from <a href="https://www.goldyarora.com/">Goldy Arora</a>.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fs149kAiuCns%3Flist%3DPLTLVpWeD3u6GnRvDXAGt-oDNIS3gds-3R&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Ds149kAiuCns&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fs149kAiuCns%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/96db9f22dc1ac6679bc5b0f36c3f18ed/href">https://medium.com/media/96db9f22dc1ac6679bc5b0f36c3f18ed/href</a></iframe><h3>Resources</h3><ul><li><a href="https://cloud.google.com/files/10909_Cloud_Identity_OnePager_V6.pdf">https://cloud.google.com/files/10909_Cloud_Identity_OnePager_V6.pdf</a></li><li><a href="https://support.google.com/a/answer/6043385">https://support.google.com/a/answer/6043385</a></li><li><a href="https://www.goldyarora.com/blog/ci-gcp-ws">https://www.goldyarora.com/blog/ci-gcp-ws</a></li><li><a href="https://support.google.com/cloudidentity/answer/7338389?hl=en">https://support.google.com/cloudidentity/answer/7338389?hl=en</a></li><li><a href="https://support.google.com/a/answer/6342682">https://support.google.com/a/answer/6342682</a></li><li><a href="https://knowledge.workspace.google.com/kb/how-to-manage-automatic-licensing-settings-000006369">https://knowledge.workspace.google.com/kb/how-to-manage-automatic-licensing-settings-000006369</a></li><li><a href="https://cloud.google.com/resource-manager/docs/creating-managing-organization">https://cloud.google.com/resource-manager/docs/creating-managing-organization</a></li><li><a href="https://cloud.google.com/blog/topics/developers-practitioners/identity-access-management-authentication-cloud-identity">https://cloud.google.com/blog/topics/developers-practitioners/identity-access-management-authentication-cloud-identity</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e4ad6d3f844c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/how-google-admins-can-save-money-by-understanding-the-relationship-between-google-cloud-identity-e4ad6d3f844c">How Google Admins can Save Money by Understanding the Relationship between Google Cloud Identity…</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Shh, It’s Free: But Let’s Not Tell Google! Exploring Gemini’s Multimodal Capabilities on Vertex AI]]></title>
            <link>https://medium.com/google-cloud/shh-its-free-but-let-s-not-tell-google-exploring-gemini-s-multimodal-capabilities-on-vertex-ai-ca3cb7e33c3e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ca3cb7e33c3e</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[large-language-models]]></category>
            <category><![CDATA[google-gemini]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Rif Kiamil]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:47:38 GMT</pubDate>
            <atom:updated>2024-04-03T04:47:38.099Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QNpoBmHA_tukDSw-Wt70QQ.jpeg" /></figure><p>Google might not trumpet the news, but there’s a quiet revolution unfolding on their platform. The Gemini Experimental model is our unheralded entry into the world of AI without the cost. This is where the multimodal capabilities of AI come to life, won’t cost you a dime. Consider this your backdoor pass into a free club, where the only membership requirement is your curiosity.</p><h3>Here’s the Whispered Truth:</h3><p>Google might play it cool, but they’ve hinted at the excitement themselves:</p><blockquote><em>“By using the Gemini Experimental model, you are contributing to the development of even better responses. Results may be genius or delightfully unpredictable, all at no cost.”</em></blockquote><p>🌟 Try it out here: <a href="https://console.cloud.google.com/vertex-ai/generative/multimodal/create/">Google Vertex AI</a></p><h3>How to Get Involved</h3><p>Navigating through Google Vertex AI to tap into the powers of the Gemini Experimental model is your first step into a broader world of AI possibilities. Here’s a straightforward guide to get you started:</p><h4>1. Select the Model:</h4><p>1.1. Navigate to the model selection area of the Google Vertex AI platform. 1.2. Look for and select the ‘Gemini Experimental’ option. Once selected, you’ll notice in the results section of the screen an “EXPERIMENTAL” label in blue, clearly indicating that you are using the Gemini Experimental mode. <br>1.3. To understand the current terms of use, hover your mouse over the information (i) icon. <br>1.4. At the time of writing, the terms stated: “<strong><em>By using the Gemini Experimental model, you are contributing to the development of even better responses. Results may be genius or delightfully unpredictable, all at no cost.</em></strong>”</p><figure><img alt="1.1 Navigate to the model selection area of the Google Vertex AI platform. 1.2 Look for the ‘Gemini Experimental’ option. You’ll recognize it by the “EXPERIMENTAL” label in blue, indicating that it’s a model currently in testing." src="https://cdn-images-1.medium.com/max/1024/0*eUmRK39n7jDY_V56" /></figure><h4>2. Understand What You’re Getting Into:</h4><p>Before you dive in, take a moment to understand what ‘Experimental’ entails. As per the screenshot you provided, Google informs us that using the Gemini Experimental model allows you to contribute to the enhancement of the AI’s responses. The outcomes you receive may range from “genius” to “delightfully unpredictable.”</p><h4>3. Contribute to Development:</h4><p>By choosing the Gemini Experimental model, you’re not just accessing an AI tool; you’re part of a collaborative development effort. Your usage and feedback can help improve the model for future users.</p><h4>4. Set Your Expectations:</h4><p>As with any experimental model, prepare for a range of results. They may not always be what you expect, but that’s part of the excitement and the learning experience.</p><h4>5. Monitor the Fine Print:</h4><p>Stay informed about any updates to the terms and conditions related to the use of the experimental model. These can impact your usage and data.</p><h4>6. Rate Limitations</h4><p>As an experimental model, Gemini Experimental is not designed for production-level tasks. It may come with rate limitations</p><h4>7. No documentation</h4><p>As a Google Developer Expert, I’ve inquired about official documentation for the Gemini Experimental model within Google’s internal channels. It appears that, as of now, there is no formal documentation available. So, dive in, enjoy the novelty while it’s available, and remember — always read the label for the most current information.</p><h4>Community</h4><p>Let the community know about your findings and experiences by using the hashtag #BuildWithGemini. Whether your results were unexpectedly brilliant or whimsically unpredictable, your insights are valuable to the ongoing development and understanding of Gemini’s potential.</p><p>Begin Your AI Journey 🛤️: Navigate to the Gemini Experimental model via the <a href="https://console.cloud.google.com/vertex-ai/generative/multimodal/create/">Google Vertex AI</a></p><h3>What else is free on Google?</h3><p>BigQuery, Google Cloud’s serverless data warehouse, offers a powerful platform for data analysis. The best part? You can get started for free (like Google Gemini Experimental) without even having to provide a credit card. BigQuery’s Sandbox allows you to explore public datasets and run queries, making it perfect for learning and experimentation.</p><p>Ready to dive in? Here are some exciting public datasets and sample queries to get you started:</p><p><strong>1. Hacker News: (bigquery-public-data.hacker_news.full)</strong></p><pre>-- This query finds the number of posts made by each author.<br>SELECT author, COUNT(*) AS post_count FROM `bigquery-public-data.hacker_news.full` GROUP BY author;</pre><p><strong>2. NOAA Weather Data: (bigquery-public-data.noaa_gsod.gsod2023)</strong></p><pre>-- This query calculates the average temperature for each weather station.<br>SELECT station_number, AVG(mean_temp) AS avg_temp FROM `bigquery-public-data.noaa_gsod.gsod2023` GROUP BY station_number;</pre><p><strong>3. Chicago Crime Data: (bigquery-public-data.chicago.crimes_2001_to_present)</strong></p><pre>-- This query shows the count of crimes for each primary type.<br>SELECT primary_type, COUNT(*) AS crime_count FROM `bigquery-public-data.chicago.crimes_2001_to_present` GROUP BY primary_type;</pre><p><strong>4. Ethereum Blockchain: (crypto_ethereum)</strong></p><pre>-- This query finds the number of transactions in each block.<br>SELECT block_number, COUNT(*) AS transaction_count FROM `crypto_ethereum.blocks` GROUP BY block_number;</pre><p>Ready to start exploring? Follow my <a href="https://rifkiamil.medium.com/step-by-step-guide-of-bigquery-sandbox-4429d9655d8e">step-by-step guide</a> on setting up your free BigQuery Sandbox:</p><p>With just a few clicks, you’ll be ready to analyze these datasets and many more, unlocking valuable insights with BigQuery’s powerful analytics engine.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ca3cb7e33c3e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/shh-its-free-but-let-s-not-tell-google-exploring-gemini-s-multimodal-capabilities-on-vertex-ai-ca3cb7e33c3e">Shh, It’s Free: But Let’s Not Tell Google! Exploring Gemini’s Multimodal Capabilities on Vertex AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unlock the Power of Machine Learning Without Coding: A Beginner’s Guide to BigQuery ML]]></title>
            <link>https://medium.com/google-cloud/unlock-the-power-of-machine-learning-without-coding-a-beginners-guide-to-bigquery-ml-0ae9ba293544?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/0ae9ba293544</guid>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[bigquery-ml]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Brian Ling]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:46:56 GMT</pubDate>
            <atom:updated>2024-04-03T04:46:56.417Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>Demystifying machine learning for data analysts — build predictive models directly within your data warehouse</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Vi1P_vFVVIMno9Yd" /><figcaption><em>A vibrant illustration of a data warehouse with colorful machine learning algorithms swirling around it, representing the ease of integration</em></figcaption></figure><h4>As a data analyst, you’re constantly seeking insights to drive better business decisions. But traditional machine learning often means complex coding, separate environments, and a reliance on specialized skills that your team might not possess. What if you could tap into the power of predictive modeling without leaving the comfort of your familiar data warehouse?</h4><h3>Introduction</h3><p>BigQuery ML (BQML) opens the door to machine learning for those who are experts in SQL. It bridges the gap between data analysts and machine learning specialists, allowing you to create, train, and deploy a variety of powerful machine learning models directly within Google Cloud’s BigQuery.</p><h3>Purpose</h3><p>This blog post will guide you through a hands-on exploration of BigQuery ML. We’ll cover the basics, walk you through a practical use case, and discuss its potential to revolutionize how you use your data.</p><h3>Use Cases</h3><ul><li><strong>Predicting customer churn:</strong> Identify customers at risk of leaving.</li><li><strong>Fraud detection:</strong> Uncover unusual patterns in financial transactions.</li><li><strong>Demand forecasting:</strong> Predict future sales to optimize inventory.</li><li><strong>Sentiment analysis:</strong> Understand customer feedback trends.</li></ul><h3>Skill Prerequisites</h3><ul><li>Basic understanding of SQL.</li><li>Familiarity with BigQuery and Google Cloud Platform (GCP).</li></ul><h3>Disclaimer</h3><p>BigQuery ML is a powerful tool, but it’s important to use it responsibly. Ensure your data is unbiased and representative of real-world scenarios to avoid inaccurate or discriminatory predictions.</p><h3>Step-by-Step Walkthrough</h3><h3>Prerequisites</h3><ul><li>A Google Cloud Platform project with billing enabled.</li><li>BigQuery access and the necessary IAM permissions.</li><li>A dataset in BigQuery to train your model.</li></ul><h3><strong>Architecture Diagram</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hTT3JmEa0zyDZN_V" /></figure><h3>Creating a Machine Learning Model in BigQuery ML</h3><ol><li><strong>Create your dataset</strong></li></ol><ul><li>To create a dataset, click on the <strong>View actions</strong> icon next to your project ID and select <strong>Create dataset</strong>.</li><li>Name your Dataset ID <strong><em>bqml_lab</em></strong> and click <strong>Create dataset</strong>.</li></ul><p><strong>2. Create a model</strong></p><ul><li>Go to BigQuery <strong>EDITOR</strong>, paste the following query to create a model that predicts purchase likelihood:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Nor09QiVnXUSVHP2" /></figure><pre>CREATE OR REPLACE MODEL bqml_lab.sample_model<br>OPTIONS(model_type=&#39;logistic_reg&#39;) AS<br>SELECT<br>  IF(totals.transactions IS NULL, 0, 1) AS label,<br>  IFNULL(device.operatingSystem, &quot;&quot;) AS os,<br>  device.isMobile AS is_mobile,<br>  IFNULL(geoNetwork.country, &quot;&quot;) AS country,<br>  IFNULL(totals.pageviews, 0) AS pageviews<br>FROM<br>  bigquery-public-data.google_analytics_sample.ga_sessions_*<br>WHERE<br>  _TABLE_SUFFIX BETWEEN &#39;20160801&#39; AND &#39;20170631&#39;<br>LIMIT 100000;</pre><p><strong>Explanations:</strong></p><ul><li><em>bqml_lab</em> is the dataset, sample_model is the model name.</li><li>We’re using binary logistic regression (model_type=’<em>logistic_reg</em>’).</li><li>label is what we aim to predict (purchases).</li><li>Features include device OS, mobile status, country, and pageviews.</li></ul><p>3. <strong>Evaluate your model:</strong></p><ul><li>Replace the previous query with the following and click Run:</li></ul><pre>SELECT<br>  *<br>FROM<br>  ML.EVALUATE(MODEL `bqml_lab.sample_model`, (<br>SELECT<br>  IF(totals.transactions IS NULL, 0, 1) AS label,<br>  IFNULL(device.operatingSystem, &quot;&quot;) AS os,<br>  device.isMobile AS is_mobile,<br>  IFNULL(geoNetwork.country, &quot;&quot;) AS country,<br>  IFNULL(totals.pageviews, 0) AS pageviews<br>FROM<br>  `bigquery-public-data.google_analytics_sample.ga_sessions_*`<br>WHERE<br>  _TABLE_SUFFIX BETWEEN &#39;20170701&#39; AND &#39;20170801&#39;));</pre><ul><li>When the query is complete, click the Results tab below the query text area. You should see a table similar to this:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aeXd0JLNNxsqOQCn" /></figure><p><strong>Explanations:</strong></p><ul><li>Want to know how well your model performs? Check out these key terms: <strong>precision, recall, accuracy, f1_score, log_loss, roc_auc:</strong> You can consult the <a href="https://developers.google.com/machine-learning/glossary/">machine learning glossary</a> for definitions.</li></ul><p><strong>4. Use your model to predict outcomes</strong></p><ul><li>With this query you will try to predict the number of transactions made by visitors of each country, sort the results, and select the top 10 countries by purchases:</li></ul><pre>SELECT<br>  country,<br>  SUM(predicted_label) as total_predicted_purchases<br>FROM<br>  ML.PREDICT(MODEL `bqml_lab.sample_model`, (<br>SELECT<br>  IFNULL(device.operatingSystem, &quot;&quot;) AS os,<br>  device.isMobile AS is_mobile,<br>  IFNULL(totals.pageviews, 0) AS pageviews,<br>  IFNULL(geoNetwork.country, &quot;&quot;) AS country<br>FROM<br>  `bigquery-public-data.google_analytics_sample.ga_sessions_*`<br>WHERE<br>  _TABLE_SUFFIX BETWEEN &#39;20170701&#39; AND &#39;20170801&#39;))<br>GROUP BY country<br>ORDER BY total_predicted_purchases DESC<br>LIMIT 10;</pre><ul><li>When the query is complete, click the Results tab below the query text area. The results should look like the following:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*390uRFMb4ttUw_5I" /></figure><p>🎊Congratulations! You used BigQuery ML to create a binary logistic regression model, evaluate the model, and use the model to make predictions.</p><h3>Outro</h3><h3>Recap key takeaways:</h3><ul><li>BigQuery ML allows you to build machine learning models using SQL.</li><li>No specialized machine learning expertise is required.</li><li>BQML models are easily integrated into your existing BigQuery workflows.</li></ul><h3>Call to Action/Next Steps</h3><ul><li>Explore other BQML model types: Experiment with classification, time series forecasting, and more.</li><li>Dive deeper into model evaluation and optimization techniques.</li></ul><p><strong>👋 </strong>By the way, if you happen to be a startup owner who is actively seeking to propel your business to new heights with Cloud:</p><p>We invite you to join our exclusive virtual live workshops (links below), where you’ll gain hands-on guidance from Google Cloud experts and discover how to seamlessly integrate GCP into your operations. Don’t miss this limited-time opportunity to empower your startup with the knowledge and expertise needed to thrive in the cloud-driven world. <a href="https://rsvp.withgoogle.com/events/let-s-learn-about-google-cloud">Register now</a> and secure your spot!</p><p>⚒️<a href="https://rsvp.withgoogle.com/events/onboarding-workshop">Startup Onboarding Workshop</a></p><p>⛑️<a href="https://rsvp.withgoogle.com/events/gen-ai-workshop">Generative AI Workshop</a></p><p>🔑<a href="https://rsvp.withgoogle.com/events/data-analytics">Data Analytics Workshop</a></p><p>🔐<a href="https://rsvp.withgoogle.com/events/security-workshop">Security Workshop</a></p><p>📠<a href="https://rsvp.withgoogle.com/events/app-mod-workshops">Modern Applications Workshop</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0ae9ba293544" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/unlock-the-power-of-machine-learning-without-coding-a-beginners-guide-to-bigquery-ml-0ae9ba293544">Unlock the Power of Machine Learning Without Coding: A Beginner’s Guide to BigQuery ML</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Parsing Invoices using Gemini 1.5 API with Google Apps Script]]></title>
            <link>https://medium.com/google-cloud/parsing-invoices-using-gemini-1-5-api-with-google-apps-script-1f32af1678f2?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/1f32af1678f2</guid>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[llm]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 04:46:29 GMT</pubDate>
            <atom:updated>2024-04-03T04:46:29.223Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*7a-Tam_sovNuq9gr.png" /></figure><h3>Abstract</h3><p>This report explores using Gemini, a new AI model, to parse invoices in Gmail attachments. Traditional text searching proved unreliable due to invoice format variations. Gemini’s capabilities can potentially overcome this inconsistency and improve invoice data extraction.</p><h3>Introduction</h3><p>After Gemini, a large language model from Google AI, has been released, it has the potential to be used for modifying various situations, including information extraction from documents. In my specific case, I work with invoices in PDF format. Until now, I relied on the direct search by a Google Apps Script to achieve this task. The script’s process involved:</p><ol><li>Converting each invoice PDF file into a temporary Google Doc.</li><li>Utilizing text searching within the Google Doc to extract required values.</li><li>Deleting the temporary Google Doc after successful extraction.</li></ol><p>However, this approach proved unreliable due to variations in the invoice formats of each invoice. The text-searching method often failed to capture the required values consistently across different invoice layouts. This inconsistency led me to believe that Gemini’s capabilities could be a valuable asset for invoice parsing.</p><p>In this report, I aim to introduce a method for parsing various invoice types and retrieving the required values using Gemini. Here, I chose to leverage Google Apps Script for this project because the invoices are received as email attachments in Gmail. Google Apps Script provides a convenient way to access and process these PDF files directly within the Gmail environment.</p><h3>Usage</h3><p>In order to test this script, please do the following steps.</p><h3>1. Create an API key</h3><p>Please access <a href="https://makersuite.google.com/app/apikey">https://makersuite.google.com/app/apikey</a> and create your API key. At that time, please enable Generative Language API at the API console. This API key is used for this sample script.</p><p>This official document can be also seen. <a href="https://ai.google.dev/">Ref</a>.</p><h3>2. Create a Google Apps Script project</h3><p>In this report, Google Apps Script is used. Of course, the method introducing this report can be also used in other languages.</p><p>Please create a standalone Google Apps Script project. Of course, this script can be also used with the container-bound script.</p><p>And, please open the script editor of the Google Apps Script project.</p><h3>IMPORTANT</h3><p><strong>This script uses a Google Apps Script library for converting PDF data to image data. So, please install </strong><a href="https://github.com/tanaikech/PDFApp"><strong>PDFApp</strong></a><strong>. I created this library.</strong></p><h3>3. Base script</h3><p>This is the base script as Class InvoiceApp. This is used from a work function.</p><pre>/**<br> * Parsing invoice with Gemini API.<br> */<br>class InvoiceApp {<br>  /**<br>   * @param {Object} object Object using this library.<br>   */<br>  constructor(object = {}) {<br>    this.model = &quot;models/gemini-1.5-pro-latest&quot;;<br>    this.version = &quot;v1beta&quot;;<br>    this.baseUrl = &quot;https://generativelanguage.googleapis.com&quot;;<br>    this.apiKey = object.apiKey || null;<br>    this.headers = object.apiKey<br>      ? null<br>      : {<br>          authorization: `Bearer ${object.token || ScriptApp.getOAuthToken()}`,<br>        };<br>    this.retry = 5;<br>    this.folderId = object.folderId || &quot;root&quot;;<br>    this.object = object;<br>  }<br><br>  /**<br>   * ### Description<br>   * Main method.<br>   *<br>   * @returns {Promise} Response from API is returned as Promise object.<br>   */<br>  async run() {<br>    if (!this.object?.blob) {<br>      throw new Error(&quot;Please set the PDF blob of invoice on Google Drive.&quot;);<br>    }<br>    const blob = this.object.blob;<br>    console.log(<br>      `--- Converting PDF blob to PNG images and uploading images to Gemini.`<br>    );<br>    const obj = await this.uploadFileByBlob_(blob);<br><br>    console.log(`--- Processing Gemini using the uploaded images.`);<br>    const q = [<br>      `Create a table from the given image of the invoice as a JSON object.`,<br>      `The giving image is the invoice.`,<br>      `Return a created table as a JSON object.`,<br>      `No descriptions and explanations. Return only raw JSON object without markdown. No markdown format.`,<br>      `The required properties in JSON object are as follows.`,<br>      ``,<br>      `[Properties in JSON object]`,<br>      `&quot;invoiceTitle&quot;: &quot;title of invoice&quot;`,<br>      `&quot;invoiceDate&quot;: &quot;date of invoice&quot;`,<br>      `&quot;invoiceNumber&quot;: &quot;number of the invoice&quot;`,<br>      `&quot;invoiceDestinationName&quot;: &quot;Name of destination of invoice&quot;`,<br>      `&quot;invoiceDestinationAddress&quot;: &quot;address of the destination of invoice&quot;`,<br>      `&quot;totalCost&quot;: &quot;total cost of all costs&quot;`,<br>      `&quot;table&quot;: &quot;Table of invoice. This is a 2-dimensional array. Add the first header row to the table in the 2-dimensional array.&quot;`,<br>      ``,<br>      `[Format of 2-dimensional array of &quot;table&quot;]`,<br>      `&quot;title or description of item&quot;, &quot;number of items&quot;, &quot;unit cost&quot;, &quot;total cost&quot;`,<br>      ``,<br>      `If the requirement information is not found, set &quot;no value&quot;.`,<br>      `Return only raw JSON object without markdown. No markdown format. No markcodn tags.`,<br>    ].join(&quot;\n&quot;);<br>    const res = this.doGemini_({ q, obj });<br><br>    console.log(`--- Deleting the uploaded images from Gemini.`);<br>    obj.forEach(({ name }) =&gt; this.deleteFile_(name));<br><br>    console.log(`--- Done.`);<br>    return res;<br>  }<br><br>  /**<br>   * ### Description<br>   * Upload image files to Gemini.<br>   *<br>   * @param {Blob} Blob PDF blob of invoice on Google Drive.<br>   * @returns {Promise} An array including uri, name, mimeType<br>   */<br>  async uploadFileByBlob_(blob) {<br>    if (blob.getContentType() != MimeType.PDF) {<br>      throw new Error(<br>        `Please set PDF blob. The mimeType of this blob is &#39;${blob.getContentType()}&#39;`<br>      );<br>    }<br>    const imageBlobs = await PDFApp.setPDFBlob(blob)<br>      .convertPDFToPng()<br>      .catch((err) =&gt; {<br>        throw new Error(err);<br>      });<br>    const ar = [];<br>    let url = `${this.baseUrl}/upload/${this.version}/files?uploadType=multipart`;<br>    for (let blob of imageBlobs) {<br>      const metadata = { file: { displayName: blob.getName() } };<br>      const payload = {<br>        metadata: Utilities.newBlob(<br>          JSON.stringify(metadata),<br>          &quot;application/json&quot;<br>        ),<br>        file: blob,<br>      };<br>      const options = { method: &quot;post&quot;, payload: payload };<br>      if (this.apiKey) {<br>        url += `&amp;key=${this.apiKey}`;<br>      } else {<br>        options.headers = this.headers;<br>      }<br>      const res = this.fetch_({ url, ...options });<br>      const o = JSON.parse(res.getContentText());<br>      ar.push({<br>        uri: o.file.uri,<br>        name: o.file.name,<br>        mimeType: o.file.mimeType,<br>      });<br>    }<br>    return ar;<br>  }<br><br>  /**<br>   * ### Description<br>   * Parsing invoice of image data by Gemini API.<br>   *<br>   * @param {Object} object Object including q and fileUri.<br>   * @returns {String|Object} Return parsed invoice data from Gemini API.<br>   */<br>  doGemini_(object) {<br>    const { q, obj } = object;<br>    const text = q;<br>    let url = `${this.baseUrl}/${this.version}/${this.model}:generateContent`;<br>    const options = {<br>      contentType: &quot;application/json&quot;,<br>      muteHttpExceptions: true,<br>    };<br>    if (this.apiKey) {<br>      url += `?key=${this.apiKey}`;<br>    } else {<br>      options.headers = this.headers;<br>    }<br>    const fileData = obj.map(({ uri, mimeType }) =&gt; ({<br>      fileData: { fileUri: uri, mimeType },<br>    }));<br>    const contents = [{ parts: [{ text }, ...fileData], role: &quot;user&quot; }];<br>    const temp = [];<br>    let result = null;<br>    let retry = 5;<br>    do {<br>      retry--;<br>      options.payload = JSON.stringify({ contents });<br>      const res = this.fetch_({ url, ...options });<br>      if (res.getResponseCode() == 500 &amp;&amp; retry &gt; 0) {<br>        console.warn(&quot;Retry by the status code 500.&quot;);<br>        Utilities.sleep(3000); // wait<br>        this.doGemini_(object);<br>      } else if (res.getResponseCode() != 200) {<br>        throw new Error(res.getContentText());<br>      }<br>      const { candidates } = JSON.parse(res.getContentText());<br>      if (candidates &amp;&amp; !candidates[0]?.content?.parts) {<br>        temp.push(candidates[0]);<br>        break;<br>      }<br>      const parts = (candidates &amp;&amp; candidates[0]?.content?.parts) || [];<br>      if (parts[0].text) {<br>        const t = parts[0].text.match(/\`\`\`json\n([\w\s\S]*)\`\`\`/);<br>        if (t) {<br>          try {<br>            result = JSON.parse(t[1].trim());<br>          } catch ({ stack }) {<br>            result = null;<br>            console.error(stack);<br>            console.error(t[1].trim());<br>            this.doGemini_(object);<br>          }<br>        }<br>      } else {<br>        result = &quot;No parts[0].text.&quot;;<br>        console.warn(&quot;No parts[0].text.&quot;);<br>        console.warn(parts);<br>      }<br>      temp.push(...parts);<br>    } while (!result &amp;&amp; retry &gt; 0);<br>    return result || &quot;No values. Please try it again.&quot;;<br>  }<br><br>  /**<br>   * ### Description<br>   * Delete file from Gemini.<br>   *<br>   * @param {String} name Name of file.<br>   * @return {void}<br>   */<br>  deleteFile_(name) {<br>    let url = `${this.baseUrl}/${this.version}/${name}`;<br>    const options = { method: &quot;delete&quot;, muteHttpExceptions: true };<br>    if (this.apiKey) {<br>      url += `?key=${this.apiKey}`;<br>    } else {<br>      options.headers = this.headers;<br>    }<br>    this.fetch_({ url, ...options });<br>    return null;<br>  }<br><br>  /**<br>   * ### Description<br>   * Request Gemini API.<br>   *<br>   * @param {Object} obj Object for using UrlFetchApp.fetchAll.<br>   * @returns {UrlFetchApp.HTTPResponse} Response from API.<br>   */<br>  fetch_(obj) {<br>    obj.muteHttpExceptions = true;<br>    const res = UrlFetchApp.fetchAll([obj])[0];<br>    if (res.getResponseCode() != 200) {<br>      throw new Error(res.getContentText());<br>    }<br>    return res;<br>  }<br>}</pre><h3>4. Sample script 1</h3><p>In this sample, the PDF file of invoice is retrieved from Gmail. And, the PDF invoice is parsed by Gemini 1.5 API.</p><p>This is a simple script. So, please modify this to your actual situation.</p><pre>async function sample1() {<br>  const apiKey = &quot;###&quot;; // Please set your API key.<br><br>  // Reading PDF file from Gmail and save it in Google Drive.<br>  const threads = GmailApp.search(&#39;subject:&quot;invoice&quot;&#39;, 0, 1);<br>  if (threads.length == 0) {<br>    console.log(&quot;No threads.&quot;);<br>    return;<br>  }<br>  const blob = threads<br>    .pop()<br>    .getMessages()<br>    .pop()<br>    .getAttachments()[0]<br>    .setContentTypeFromExtension();<br>  if (blob.getContentType() != MimeType.PDF) {<br>    console.log(&quot;This attachment is not PDF.&quot;);<br>    return;<br>  }<br>  const pdfFile = DriveApp.createFile(blob);<br><br>  // Parsing invoice of PDF file and retrieve values.<br>  const ip = new InvoiceApp({ apiKey, blob: pdfFile.getBlob() });<br>  const res = await ip.run();<br>  if (typeof res == &quot;object&quot;) {<br>    console.log(&quot;--- Valid values.&quot;);<br>    console.log(JSON.stringify(res));<br><br>    // do something.<br>  } else {<br>    console.log(&quot;--- Invalid values.&quot;);<br>    console.log(res);<br>  }<br><br>  // pdfFile.setTrashed(true); // If you want to remove the PDF file, please use this line.<br>}</pre><h3>5. Sample script 2</h3><p>In this sample, the PDF invoice file on Google Drive is directly used.</p><pre>async function sample2() {<br>  const apiKey = &quot;###&quot;; // Please set your API key.<br>  const fileId = &quot;###&quot;; // File ID of PDF file of invoice file.<br><br>  // Parsing invoice of PDF file and retrieve values.<br>  const ip = new InvoiceApp({<br>    apiKey,<br>    blob: DriveApp.getFileById(fileId).getBlob(),<br>  });<br>  const res = await ip.run();<br>  if (typeof res == &quot;object&quot;) {<br>    console.log(&quot;--- Valid values.&quot;);<br>    console.log(JSON.stringify(res));<br><br>    // do something.<br>  } else {<br>    console.log(&quot;--- Invalid values.&quot;);<br>    console.log(res);<br>  }<br>}</pre><h3>Testing</h3><p>This is a sample invoice. <a href="https://create.microsoft.com/en-us/template/service-invoice-(simple-lines-design-worksheet)-c10068f0-7a64-423b-abad-dced024877b0">This sample</a> is from <a href="https://create.microsoft.com/en-us/templates/invoices">Invoice design templates of Microsoft</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/707/0*hw8L_l6ab05R1DMW.png" /></figure><p>When the above sample invoice is used, the following result is obtained.</p><pre>{<br>  &quot;invoiceTitle&quot;: &quot;Invoice&quot;,<br>  &quot;invoiceDate&quot;: &quot;January 1, 2024&quot;,<br>  &quot;invoiceNumber&quot;: &quot;100&quot;,<br>  &quot;invoiceDestinationName&quot;: &quot;Maria Sullivan\nThe Palm Tree Nursery\n987 6th Ave\nSanta Fe, NM 11121&quot;,<br>  &quot;invoiceDestinationAddress&quot;: &quot;no value&quot;,<br>  &quot;totalCost&quot;: &quot;$192.50&quot;,<br>  &quot;table&quot;: [<br>    [&quot;Qty&quot;, &quot;Description&quot;, &quot;Unit Price&quot;, &quot;Line Total&quot;],<br>    [&quot;20.00&quot;, &quot;Areca palm&quot;, &quot;$2.50&quot;, &quot;$50.00&quot;],<br>    [&quot;35.00&quot;, &quot;Majesty palm&quot;, &quot;$3.00&quot;, &quot;$105.00&quot;],<br>    [&quot;15.00&quot;, &quot;Bismarck palm&quot;, &quot;$2.50&quot;, &quot;$37.50&quot;]<br>  ]<br>}</pre><p>Another sample is as follows. <a href="https://create.microsoft.com/en-us/template/service-invoice-with-tax-calculations-9330a1fe-20ae-4590-ac01-54c53ed1f3ba">This sample</a> is from <a href="https://create.microsoft.com/en-us/templates/invoices">Invoice design templates of Microsoft</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/773/0*UKZSDhX7Kvownha0.png" /></figure><p>When the above sample invoice is used, the following result is obtained.</p><pre>{<br>  &quot;invoiceTitle&quot;: &quot;INVOICE&quot;,<br>  &quot;invoiceDate&quot;: &quot;April 1, 2024&quot;,<br>  &quot;invoiceNumber&quot;: &quot;100&quot;,<br>  &quot;invoiceDestinationName&quot;: &quot;Nazar Neili&quot;,<br>  &quot;invoiceDestinationAddress&quot;: &quot;Downtown Pets\n123 South Street\nManhattan, NY 15161&quot;,<br>  &quot;totalCost&quot;: &quot;$4350&quot;,<br>  &quot;table&quot;: [<br>    [&quot;DESCRIPTION&quot;, &quot;HOURS&quot;, &quot;RATE&quot;, &quot;AMOUNT&quot;],<br>    [&quot;Pour cement foundation&quot;, &quot;4.00&quot;, &quot;$150.00&quot;, &quot;$600&quot;],<br>    [&quot;Framing and drywall&quot;, &quot;16.00&quot;, &quot;$190.00&quot;, &quot;$3040&quot;],<br>    [&quot;Tiling and flooring install&quot;, &quot;9.00&quot;, &quot;$150.00&quot;, &quot;$1350&quot;]<br>  ]<br>}</pre><h3>Note</h3><ul><li>When this method is used, not only the invoices but also the receipts can be parsed.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1f32af1678f2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/parsing-invoices-using-gemini-1-5-api-with-google-apps-script-1f32af1678f2">Parsing Invoices using Gemini 1.5 API with Google Apps Script</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From GenAI to Insights from Your Customers (Part 1)]]></title>
            <link>https://medium.com/google-cloud/from-genai-to-insights-from-your-customers-part-1-b14213a6f288?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/b14213a6f288</guid>
            <category><![CDATA[text-summarization]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[large-language-models]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <dc:creator><![CDATA[Tara Pourhabibi]]></dc:creator>
            <pubDate>Tue, 02 Apr 2024 01:05:25 GMT</pubDate>
            <atom:updated>2024-04-02T01:05:25.817Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/797/1*KUZsLKGRzS9-M2dH8GbPbw.png" /></figure><p>Analyzing customer complaints is crucial for businesses as it enhances customer experience and fosters trust by providing insights into areas that need improvement.</p><p>Summarization adds value by condensing vast amounts of feedback into actionable insights, enabling businesses to quickly identify trends, prioritize issues, and implement targeted solutions. This efficient process empowers businesses to proactively address customer concerns, improve products or services, and ultimately, improve customer satisfaction and gain more loyalty.</p><p>In this post, my main goal is to condense lengthy customer complaints (<a href="https://www.consumerfinance.gov/">Consumer Finance Protection Bureau </a>(CFPB) data) and extract relevant important information from them efficiently. I guide you through my utilization of Vertex AI PaLM2 along with LangChain and compare the results of the summarized complaint with an open source LLM (LaMini-Flan-T5–248M) alongside LangChain.</p><pre>#install required libraries<br><br>!pip install huggingface-hub<br>!pip install langchain<br>!pip install transformers</pre><pre>#load require librraies<br>import pandas as pd<br><br>#import hface pipeline from langchain and summarize chain<br>from langchain.llms import HuggingFacePipeline<br>from langchain.chains.summarize import load_summarize_chain<br>from langchain.text_splitter import RecursiveCharacterTextSplitter<br>from langchain import PromptTemplate, LLMChain<br><br># load Vertex AI<br>from langchain.llms import VertexAI<br></pre><p><strong>Define LLM Model</strong></p><p><strong><em>LLM Model: Vertext AI PaLM 2</em></strong></p><p><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text">PaLM 2</a> is Google’s LLM approach to responsible Generative AI and is fine-tuned for different NLP tasks such as classification, summarization, and entity extraction.</p><pre>#Define Vertex AI PaLM 2 llm to generate response<br>llm = VertexAI(model_name=&#39;text-bison@001&#39;,<br>                 batch_size=100, #set this if you are using batch processing<br>                 model_kwargs={&quot;temperature&quot;:0, &quot;max_length&quot;:512}<br>                  )</pre><p><strong><em>LLM Model: LaMini-Flan-T5</em></strong></p><p>LaMini-Flan-T5–248 is an open source LLM; a refined iteration of google/flan-t5-base trained on the LaMini-instruction dataset with 2.58M samples.</p><pre>#defining the lamini model chackpoint in langchain<br>checkpoint = &#39;MBZUAI/LaMini-Flan-T5-248M&#39;<br><br>#huggingfacepipeline details<br># Define llm to generate response<br>llm = HuggingFacePipeline.from_model_id(model_id=checkpoint,<br>                                        batch_size=100 #set this if you are using batch processing<br>                                        task =&#39;text2text-generation&#39;<br>                                        model_kwargs={&quot;temperature&quot;:0, &quot;max_length&quot;:512})<br> </pre><p><strong>Define Text Splitter and Summarizer Chain</strong></p><p>Since some of the complaints have long description (that exceed the maximum allowed token size in LLM models), I use LangChain to split them into separate chunks using a “map_reduce” chain type. This will send each chunk separately to LLM in “Map” process, and then “Reduce” function will integrate all summaries together at the end. This is one way to summarize large documents but requires several calls to the LLM. It however, may impact the accuracy and performance.</p><p>See bellow how I defined a recursive text splitter and a prompt with <a href="https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/">PromptTemplate</a> to guide the LLM to summarize the text.</p><pre>#define a recursive text spitter to chucnk the complaints<br>text_splitter = RecursiveCharacterTextSplitter(   <br>    chunk_size = 1000, #I set a to chunck size of 1000  <br>    chunk_overlap  = 40,<br>    length_function = len,<br>)<br><br><br>#set prompt template<br>prompt_template =&quot;&quot;&quot;<br>summarize the given text by high lighting most important information<br><br>{text}<br><br>Summary:<br>    &quot;&quot;&quot;   <br><br>#define prompt template<br>prompt = PromptTemplate(template=prompt_template, input_variables=[&quot;text&quot;])<br><br>#define chain with a map_reduce type<br>chain = load_summarize_chain(llm, map_prompt=prompt, combine_prompt=prompt, verbose=True,chain_type=&quot;map_reduce&quot;)<br><br></pre><p>Summarization can be done in an “online” (for one complaint at a time) or “batch” for a batch/chunk of complaints.</p><p><strong>Online Summarization:</strong></p><pre>#for an online mode, just pass one complaint text<br><br>texts = text_splitter.create_documents([complaint_text])<br>summary = llm_chain.run(texts)<br>print(summary)</pre><p>Here you can see an example of a splitted complaint description:</p><pre>#example of output splitted texts<br>[<br>    Document(<br>        page_content=&#39;I am writing to formally complain about inaccurate and illegal reporting of transactions on <br>my credit report, which I believe violates the Fair Credit Reporting Act ( FCRA ) specifically 15 U.S. Code 1681a.I<br>have carefully reviewed my credit report, and I have identified several inaccuracies in the reporting of late <br>payments and utilization of credit. As per 15 U.S. Code 1681a, The term consumer reporting agency means any person <br>which, for monetary fees, dues, or on a cooperative nonprofit basis, regularly engages in whole or in part in the <br>practice of assembling or evaluating consumer credit information or other information on consumers for the purpose <br>of furnishing consumer reports to third parties, and which uses any means or facility of interstate commerce for <br>the purpose of preparing or furnishing consumer reports. \\n The term consumer means an individual. \\n The term <br>consumer report means any written, oral, or other communication of \\nany information by a consumer reporting&#39;<br>    ),<br>    Document(<br>        page_content=&quot;information by a consumer reporting agency bearing on a consumers credit worthiness, credit <br>standing, credit capacity, character, general reputation, personal characteristics, or mode of living. * ( 2 ) <br>Exclusions \\n ( A ) ( i ) report containing information solely as to transactions or experiences between the <br>consumer and the person making the report ; \\n It is illegal to report inaccurate information that adversely <br>affects a consumer &#39;s creditworthiness. Below are the specific discrepancies that I have identified Account number <br>Account type : Home Equity. Date opened Late payments recognized on of . Account number Account type : Credit card.<br>Date opened Late payments recognized on of Account number Account type : Auto Loan. Date opened Late payments <br>recognized on of Account number Account type : Home Equity. Date opened 104 % credit utilization . Account number <br>Account type : Credit card. Date opened 1 % utilization Account number Account type : Credit Card. Date opened 1 %&quot;<br>    ),<br>    Document(<br>        page_content=&#39;type : Credit Card. Date opened 1 % Utilization.\\n I am formally requesting that you conduct<br>a thorough investigation into these matters, as required by the FCRA. I kindly request that you promptly correct <br>the inaccurate information on my credit report by removing the incorrect late payments and adjusting the reported <br>credit utilization. I understand that under 15 U.S. Code 1681i, you are required to conduct a reasonable <br>investigation within 30 days of receiving a dispute. I urge you to adhere to this statutory requirement and provide<br>me with written notification of the results of your investigation. If the investigation confirms the inaccuracies, <br>I request that you update my credit report accordingly and provide me with a revised copy. Additionally, I would <br>appreciate it if you could provide me with information on the steps taken to prevent such errors in the future. If <br>my concerns are not addressed within the stipulated time frame, I will have no choice but to escalate this matter&#39;<br>    ),<br>    Document(<br>        page_content=&#39;no choice but to escalate this matter to the Consumer Financial Protection Bureau. Please <br>treat this matter with the urgency it deserves.&#39;<br>    )<br>]</pre><p>and here a pretty good overall summary of important information in the complaint has been highlighted:</p><pre>#Example of output summary using LaMini-Flan-T5<br><br>The person is expressing a complaint about inaccurate and illegal reporting of transactions on their credit report,<br>which violates the Fair Credit Reporting Act (FCRA) specifically 15 U.S. Code 1681a. The person identified several <br>discrepancies in the reporting of late payments and utilization of credit, and is requesting a thorough <br>investigation into the credit card utilization and the FCRA&#39;s requirement to conduct a reasonable investigation <br>within 30 days of receiving a dispute. They request to update their credit report and provide information on steps <br>to prevent future errors.</pre><pre>#Example of output summary using Vertext AI PaLM 2 LLM<br><br>The writer is writing to formally complain about inaccurate and illegal <br>reporting of transactions on their credit report. The writer believes that <br>this violates the Fair Credit Reporting Act (FCRA). The writer has carefully reviewed their credit report and has identified several inaccuracies in the reporting of late payments and utilization of credit. The writer is requesting that the consumer reporting agency correct the inaccuracies in their credit report.\n\nIt is illegal to report inaccurate information that adversely affects a consumer&#39;s creditworthiness.<br>The specific discrepancies are:\n\n- Account number Account type: Home Equity. Date opened Late payments recognized on of .\n- Account number Account type: Credit card</pre><p><strong>Batch Summarization:</strong></p><p>For batch processing, “batch” execution mode of LangChain should be called:</p><pre>def split_doc(text_splitter,doc):<br>    &quot;&quot;&quot;<br>    function to split an input document using Langchain<br>    Args:<br>        text_splitter: a langchain text splitter<br>        doc: string text <br>    Output:<br>        texts: a dictionary of splitted text<br>    &quot;&quot;&quot;<br>    texts = text_splitter.create_documents([doc])<br>    <br>    return texts<br><br>def summarize_docs(docs,llm_chain):<br>    &quot;&quot;&quot;<br>    function to summarize chunked documents<br>    Args:<br>        llm_chain: a langchain summarize chain<br>        docs: chunked documents<br>    Output:<br>        summaries: list of summarized documents<br>    &quot;&quot;&quot;<br>    #summarize all chunks in one go<br>    summary = llm_chain.batch(docs)<br>    <br>    summaries=[]<br>    #extract summaries <br>    for summarized_doc in summary:<br>        summaries.append(summarized_doc[&#39;output_text&#39;])<br>    <br>    return summaries</pre><pre><br>#load complaints data<br>#read data from file storage <br>df_complaints=read_data()<br><br>#set the complaint description column for summarizing<br>desc_col=&#39;Consumer complaint narrative&#39;<br><br>#chunk all complaints<br>docs=df_complaints[desc_col].apply(lambda doc: split_doc(text_splitter,doc) )<br><br>#extract and concatenate summaries to the original data<br>df_complaints[&#39;summarized_narrative&#39;]=summarize_docs(docs,llm_chain)</pre><p><strong>Evaluation:</strong></p><p>Human assessment plays a crucial role in evaluating summarization tasks. It is essential to thoroughly review the generated output to ensure it is concise and also maintains the core objectives of the original text.</p><p>To ensure that summaries are aligned closely with human perception, selected samples of summarized documents can be compared with human interpretations, and their<em> </em><a href="https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/#:~:text=ROUGE%20stands%20for%20Recall%2DOriented,as%20well%20as%20machine%20translations.&amp;text=If%20we%20consider%20just%20the,and%20reference%20summary%20is%206."><em>ROUGE</em> </a>(Recall-Oriented Understudy for Gisting Evaluation) score can be calculated. ROUGE comprises metrics that help effective evaluation of automatic text summarization and machine translations.</p><p><strong>Final Note:</strong></p><p>In this article, I’ve showcased the development of a scalable summarization solution. Both LaMini-Flan-T5 and Vertex AI PaLM 2 API (taking into account the<a href="https://cloud.google.com/vertex-ai/generative-ai/pricing"> associated costs</a>) models along with LangChain exhibited strong performance in extracting important highlights from complaints, showcasing robust capabilities in generating AI-powered summaries.</p><p>In upcoming posts, I’ll employ BERTopic and LLM to identify the predominant trends in customer complaints and uncover the root causes behind these issues. This analysis aims to provide valuable insights for businesses.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b14213a6f288" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/from-genai-to-insights-from-your-customers-part-1-b14213a6f288">From GenAI to Insights from Your Customers (Part 1)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Design your Landing Zone — Design Considerations Part 4— IaC, GitOps and CI/CD (Google Cloud…]]></title>
            <link>https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-4-iac-gitops-and-ci-cd-google-cloud-ae3f533c6dbd?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/ae3f533c6dbd</guid>
            <category><![CDATA[gitops]]></category>
            <category><![CDATA[landingzone]]></category>
            <category><![CDATA[infrastructure-as-code]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[cloud-foundation]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Mon, 01 Apr 2024 11:51:00 GMT</pubDate>
            <atom:updated>2024-04-01T11:54:07.419Z</atom:updated>
            <content:encoded><![CDATA[<h3>Design your Landing Zone — Design Considerations Part 4— IaC, GitOps and CI/CD (Google Cloud Adoption Series)</h3><p>Welcome to LZ Design Considerations Part 4, where we’ll wrap up the Landing Zone Design Considerations. This is part of the <a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Google Cloud Adoption and Migration: From Strategy to Operation</a> series.</p><p>Previously, we covered <a href="https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-3-monitoring-logging-billing-and-7b40189a3c81">LZ design decisions relating to monitoring, logging, billing and labelling</a>. In this part, we’ll focus on all things related to automated infra and application deployment, using IaC, GitOps, and CI/CD.</p><h3>12. IaC, GitOps and CI/CD Strategy</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/677/1*y6Mt0Sgz5pxQ_nwHTvlN1g.png" /><figcaption>Darren’s quote of the day</figcaption></figure><h4>Principles Recap</h4><p>I recommended a number of principles in a previous article, called <a href="https://medium.com/google-cloud/cloud-adoption-and-cloud-engineering-principles-google-cloud-adoption-part-3-660bdb78cebb">Cloud Adoption and Cloud Consumption Principles</a>. I want to recap a couple here:</p><ul><li><em>Automate deployments and installs</em></li><li><em>Immutable infrastructure</em></li></ul><p>Let me review the rationale for these. In our legacy on-prem world, we had:</p><ul><li>Fewer, larger machines.</li><li>Machines tended to built once, and rarely rebuilt.</li><li>VMs needed to be looked after. They were treated as <em>pets</em>.</li><li>We had limited scale and limited elasticity.</li><li>We had to care about the underlying physical hardware.</li></ul><p>But now, in Cloud:</p><ul><li>We have many, smaller machines.</li><li>VMs, applications and services tend to be rebuilt frequently. It’s more effective to replace poorly services, than to try to fix them. We treat them as <em>cattle</em>.</li><li>Applications are designed to be fault-tolerant.</li><li>We have virtually unlimited scale, and most services are extremely elastic.</li><li>We want services to scale down (or be turned off) when not in use.</li><li>For the most part, we don’t care about the underlying hardware.</li></ul><p>So, we want our LZ to align to these principles. <strong>We need an automated, repeatable way to build immutable, replaceable infrastructure.</strong></p><h4>Infrastructure-as-Code (IaC) to the Rescue</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*N--Ib58wG_NpNZtm" /><figcaption>IaC, creating infra resources in the Cloud</figcaption></figure><p><strong>IaC is about automated provisioning of infrastructure resources, using code.</strong> It allows us to rapidly provision (and tear down) infrastructure environments in a repeatable, consistent way.</p><p>Some key tenets of IaC:</p><ul><li><strong>All the infrastructure provisioning and dependencies are defined in code</strong>.</li><li>The code should be stored in <strong>source control</strong>, such as GitHub. This way it is managed, versioned, and supports collaboration.</li><li>Our code can be imperative — i.e. follow a set of steps to achieve the outcome. Or our code can be declarative — i.e. <em>“here’s the outcome I need, now you work out how to do it.”</em> <strong>Declarative is best!</strong></li><li>It is easily deployed as part of an automated <strong>CI/CD pipeline</strong>.</li><li>It is <strong>idempotent </strong>— meaning that <strong>we can always repeat</strong> our deployment, regardless of the current state, and end up with the state that we wanted.</li><li>We can use it to <strong>build multiple environments</strong>, and we can be sure they look the same. We can always pass in parameters, to apply environment-specific configuration.</li><li>We can use it to deploy <strong>DR environments on-demand</strong>. (If that is our chosen DR strategy.)</li><li>The code is <strong>self-documenting</strong>. (And supports comments.) This means that an infrastructure engineer can look at our IaC, and understand what it will do. As such, it provides the implementation of our high level design, and eliminates the need for a significant amount of low level design documentation. Why? Because the IaC<em> is the low level design documentation</em>, for the cloud infrastructure.</li><li>It <strong>eliminates configuration drift</strong> — not only between the HLD and the deployed environments, but between the environments themselves.</li><li>It <strong>eliminates human errors</strong>. We don’t have human operators building stuff manually, or tweaking stuff in individual environments.</li></ul><h4>Some Tips and Best Practices for IaC</h4><p>Here’s the thing… If you’re not using IaC, you’re doing Cloud wrong. So here are a few key takeaways:</p><ul><li>For initial resource deployment in a Dev environment, you can provision resources using the Google Cloud Console. But <strong>when you build <em>any </em>cloud infra resources in any other environments (e.g. UAT, OAT, Staging, Prod, whatever), your should be doing so with IaC. </strong>This way, you’re using the same code to deploy to all environments.</li><li>Ensure that your<strong> LZ project factory provides service accounts to your tenants</strong>, and use those service accounts to actually deploy the resources.</li><li><strong>Don’t allow manual (human) infrastructure tweaking in any environments other than Dev.</strong> You can enforce this through policy and IAM. If you allow operators to tweak configuration by hand then you will get configuration drift, and you’ll kill your automation and repeatability benefit.</li><li>If you have any engineers or integrators that say things like, <em>“Let’s just build it manually, and worry about the IaC later”</em> then educate them, or get rid of them. This sort of legacy on-prem thinking kills your cloud agility. I’ve been on projects where system integrators have refused to build the IaC upfront. And the detrimental impact on the project is staggering!</li><li>Create a <strong>CI/CD pipeline</strong> to automate your IaC deployments.</li></ul><h4>The GitOps Approach</h4><p>Building our infrastructure using IaC is a good start. But we need to build a CI/CD pipeline, such that IaC changes can be automatically deployed to our target environment. <strong>Google advocates for the use of GitOps</strong>, which requires that:</p><ul><li>All resources are deployed using <strong>declarative code</strong>.</li><li>Our <strong>code is hosted in a Git repository</strong>.</li><li>All operational changes are made by developers who make a pull request.</li><li>Merging of the pull request results in execution of a build and release pipeline.</li></ul><p>Here’s a sketch of the overall GitOps pipeline, along with some products and tools you might use at each stage:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ANWEEAo6sXrznpDVvMEFAg.png" /><figcaption>GitOps Pipeline</figcaption></figure><p>In the context of deploying cloud infrastructure, our declarative code will be in the form of IaC. Google’s documentation shows this reference example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9srXojWGJMB5Hm1oMK4lQQ.png" /><figcaption>Google’s reference architecture for GitOps</figcaption></figure><p>In this example:</p><ul><li>Our IaC is written in <strong>Terraform</strong>. Terraform is an open source cloud-agnostic IaC tool that uses a declarative IaC language.</li><li>We use <strong>GitHub </strong>to store our Git repo. (But we could use other Git hosting services like GitLab or BitBucket. If we want to stay fully within the Google Cloud ecosystem, we can also use <a href="http://Google Cloud Source Repositories">Google Cloud Source Repositories</a>, CSR. We can use CSR to host our master Git repos; but we could also synchronise our CSR repos from an upstream repo, like GitHub.)</li><li>Infra developers push IaC changes into a feature branch, triggering <strong>Google Cloud Build </strong>to execute terraform plan. This results in a Terraform manifest, but does not actually apply it. (We could use an alternative tool for executing terraform. For example, if we’re using GitHub for our Git repos, we could use GitHub Actions to trigger terraform.)</li><li>Then the developer raises a <strong>pull request</strong> for the dev branch. When it is <strong>merged</strong>, Cloud Build executes terraform apply, thus deploying our Terraform manifest to the dev environment.</li><li>Once the dev build has been validated, the changes are merged into prod, causing the Terraform manifest to be deployed to the prod environment.</li></ul><h4>Pipeline Layers</h4><p>Google recommends separate pipeline layers, with different teams responsible for each. For example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*e6FxEv3FJai3-WkWf5g9Fw.png" /><figcaption>Pipeline layers</figcaption></figure><p>Here:</p><ul><li>The <strong><em>foundation pipeline</em></strong><em> </em>deploys the foundation resources that make up the LZ. This pipeline will typically be the responsibility of a single <strong>Platform Team</strong>.</li><li>The <strong><em>infrastructure pipeline</em></strong> deploys infrastructure that is used by individual tenants and applications. The pipeline can only be executed by a tenant service account, and this service account can only deploy to resources under this tenant’s folder. Google has example code for creating an application infrastructure pipeline in the GitHub Terraform Example Foundation repo, <a href="https://github.com/terraform-google-modules/terraform-example-foundation/tree/master/4-projects">here</a>.</li><li>The <strong><em>application pipeline</em></strong> deploys application resources, such as images, and GKE application resources.</li></ul><h4>Summary of IaC and GitOps Design Decisions</h4><ul><li><strong>Which IaC tool?</strong> I would recommend Terraform, unless you have a compelling reason not to. Terraform is declarative, open source, is cloud agnostic, and works in the enterprise. It is also Google’s recommended tool for IaC with Google Cloud.</li><li>Assuming you’re using Terraform, <strong>where will you persist your Terraform state? </strong>In the enterprise, you should be storing Terraform state in a remote backend that supports collaborative working, automatic state locking, and granular access control. In the Google Cloud ecosystem, <strong>Google GCS is a great choice</strong>. But other options include Terraform Cloud (Terraform SaaS), and Terraform Enterprise (self-hosted).</li><li><strong>Which source code platform? </strong>E.g. GitHub, GitLab, Google Cloud Source Repos, etc.</li><li><strong>What will be your git branching strategy?</strong> Google recommends having a protected main branch, feature and bug-fix branches, plus a <strong>separate persistent branch for each environment</strong>. This way, changes can be promoted through the environments by merging the changes between the environment branches.</li><li><strong>How will you separate these environments in your IaC repo? </strong>Google recommends using a separate folder in the repo for each environment. Each folder will map to a branch, and each branch will deploy to a specific environment.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/789/1*CHnBud_J5muryzzRMLm74w.png" /><figcaption>Separating environments in your repo and in branches</figcaption></figure><ul><li><strong>How many environments</strong> will you manage? E.g. dev, uat, staging and prod?</li><li><strong>What Google Cloud resource naming conventions will you adopt?</strong> You need to document your naming standards. But before you invent your own, Google has a set of recommended naming conventions <a href="https://cloud.google.com/architecture/security-foundations/summary#naming-conventions">here</a>.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0DBrkSxFyd3cst32asUw3g.png" /><figcaption>Google’s recommended resource naming conventions</figcaption></figure><ul><li><strong>Will you use IaC to deploy your landing zone?</strong> If so, will you use an existing IaC LZ blueprint, or create your own? Google provides a couple of open source organisation LZ blueprints and implementations which can be used to rapidly <strong>accelerate your LZ deployment</strong>. These are <a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/tree/master/fast">Google Cloud Foundation Fabric FAST</a> and <a href="https://github.com/terraform-google-modules/terraform-example-foundation">Cloud Foundation Toolkit (CFT) Terraform Example Foundation</a>. <strong>Google Cloud Foundation Fabric FAST</strong> is intended to be a pre-composed end-to-end example, which is forked, cloned and modified as required. Whereas <strong>CFT </strong>is intended to be used a library of opinionated Terraform modules which should be composed as required. Google describes the differences between these two approaches <a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/master/FABRIC-AND-CFT.md">here</a>.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wlX4xPvM5yR0TFaQ.png" /><figcaption>Google’s Enterprise LZ IaC Accelerator — Google Cloud Foundation Fabric FAST</figcaption></figure><ul><li><strong>How will you organise and manage access to IaC repos?</strong> Google recommends using the design principle that configurations with different approval and management requirements are separated into different source control repositories. For example, a central platform team may be responsible for the LZ IaC, shared resources, and the <a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/tree/master/modules/project-factory">tenant factory</a>. Whereas application teams may be responsible for all infrastructure resources deployed within their own folders. This approach — making use of <strong>pipeline layers</strong> — is recommended in enterprises, as it delegates control to application teams.</li><li><strong>What CI/CD tools will you use in your GitOps pipeline</strong>. For example, you might choose Google Cloud Build for seamless integration with Google Cloud, if Google Cloud is the only infrastructure target of your IaC. Alternatively, if you’re already using GitHub and want more cloud agnosticity, then GitHub Actions might be a good choice.</li><li><strong>How will tenants execute their IaC?</strong> Best practice is to only allow tenants to execute IaC using provided tenant service accounts.</li><li><strong>IaC standards and best practices?</strong> Establish and document your organisation’s IaC and Terraform standards and best practices. And don’t reinvent the wheel. Google has <a href="https://cloud.google.com/docs/terraform/best-practices-for-terraform">great guidance</a> on this already.</li><li><strong>How will you enforce IaC policies and standards? </strong>Since all your cloud infrastructure will be deployed using IaC, it is important to ensure that the IaC you execute adheres to your organisation’s policies. For example, you might want to prevent deployment of certain resources, enforce use of labels with a limited set of values, or enforce customer-managed encryption keys on GCS buckets. Consider using an automated policy validation tool, such as Hashicorp Sentinel (but only if you’re using a Terraform Cloud or Terraform Enterprise backend), Terratest (which is open source), or Google’s gcloud terraform vet.</li></ul><h3>Wrap-Up</h3><p>After four articles on the topic of Google Cloud LZ design considerations, I think you’ll agree that the design phase is not entirely trivial! There are a lot of considerations, and many implications of your choices!</p><p>In the next part, I’ll show you how to go about making informed decisions. I’ll guide you through the LZ design process, show you how to capture your decisions, and tell you how to get the help you need, so you don’t make any troublesome mistakes!</p><h3>Before You Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helps me!</li><li>Feel free to <strong>leave a comment</strong> 💬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on these icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*fF62z2-FT03ui0O5.png" /><figcaption>Follow and Subscribe</figcaption></figure><h3>Links</h3><ul><li><a href="https://medium.com/google-cloud/landing-zones-on-google-cloud-b42b08e1abaa">Landing Zones on Google Cloud: What It Is, Why You Need One, and How to Create One</a></li><li><a href="https://cloud.google.com/architecture/landing-zones">Landing zone design in Google Cloud</a></li><li><a href="https://cloud.google.com/docs/terraform/resource-management/managing-infrastructure-as-code">Managing infrastructure-as-code with Terraform, Cloud Build, and GitOps</a></li><li><a href="https://cloud.google.com/docs/terraform">Terraform on Google Cloud</a></li><li><a href="https://cloud.google.com/docs/terraform/best-practices-for-terraform">Best practices for Terraform</a></li><li><a href="https://cloud.google.com/build/docs">Google Cloud Build</a></li><li><a href="https://cloud.google.com/architecture/framework/operational-excellence/automate-your-deployments">Automate your deployments</a></li><li><a href="https://cloud.google.com/architecture/security-foundations/deployment-methodology">Deployment methodology</a></li><li><a href="https://cloud.google.com/source-repositories/docs/features">Google Cloud Source Repositories</a></li><li><a href="https://cloud.google.com/docs/terraform/policy-validation">Google Cloud Terraform policy validation with terraform vet</a></li><li><a href="https://cloud.google.com/docs/terraform/policy-validation/create-policy-library">Google Cloud: creating a Terraform policy library</a></li><li><a href="https://cloud.google.com/architecture/security-foundations/summary#naming-conventions">Google Cloud Resource Naming Standards</a></li><li><a href="https://github.com/terraform-google-modules/terraform-example-foundation">Google CFT Terraform Example Foundation</a></li><li><a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/tree/master/fast">Google Cloud Foundation Fabric FAST</a></li><li><a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/master/FABRIC-AND-CFT.md">Cloud Foundation Fabric FAST vs CFT</a></li><li><a href="https://medium.com/google-cloud/resource-factories-a-descriptive-approach-to-terraform-581b3ebb59c">Resource factories</a></li><li><a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/tree/master/blueprints/factories">Resource factories in Cloud Foundation Fabric FAST</a></li><li><a href="https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/tree/master/modules/project-factory">Cloud Foundation Fabric FAST: project and folder factory</a></li><li><a href="https://registry.terraform.io/modules/terraform-google-modules/project-factory/google/latest">Terraform Google Cloud Project-Factory</a></li><li><a href="https://cloud.google.com/architecture/framework">Google Cloud Architecture Framework</a></li><li><a href="https://cloud.google.com/architecture/security-foundations">Enterprise Foundations Blueprint</a></li></ul><h3>Series Navigation</h3><ul><li><a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Series overview and structure</a></li><li>Previous: <a href="https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-3-monitoring-logging-billing-and-7b40189a3c81">Design your Landing Zone — Design Considerations Part 3: Monitoring, Logging, Billing and Labelling</a></li><li>Next: Design your Landing Zone — How To</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ae3f533c6dbd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-4-iac-gitops-and-ci-cd-google-cloud-ae3f533c6dbd">Design your Landing Zone — Design Considerations Part 4— IaC, GitOps and CI/CD (Google Cloud…</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>