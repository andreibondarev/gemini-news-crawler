<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 18 Apr 2024 07:23:29 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Automating data extraction from SEC 10-K forms using Document AI and Generative AI]]></title>
            <link>https://medium.com/google-cloud/automating-data-extraction-from-sec-10-k-forms-using-document-ai-and-generative-ai-6b2a086d6167?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6b2a086d6167</guid>
            <category><![CDATA[genai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[automation]]></category>
            <category><![CDATA[document-ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Harish Verma]]></dc:creator>
            <pubDate>Thu, 18 Apr 2024 06:15:18 GMT</pubDate>
            <atom:updated>2024-04-18T06:15:17.975Z</atom:updated>
            <content:encoded><![CDATA[<p>SEC10K forms are comprehensive financial reports that public companies file with the U.S. Securities and Exchange Commission (SEC) to disclose their financial performance. However, SEC 10-K forms can be very large, ranging from 50 to over 200 pages. Extracting data from these forms can be time-consuming and challenging due to their large size and complex format.</p><p>In this blog post, we will show you how to use Google Cloud’s Document AI and Generative AI to parse SEC 10-K forms and extract key information. This solution can save you time and effort, and it can help you to make more informed investment decisions quickly.</p><h3>Solution Architecture</h3><p>The solution architecture for Sec10k Form Parser using Document AI and Generative AI is shown below. The solution consumes a pdf document and extracts predefined fields.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zoXK4BtnBH0mRdUC" /><figcaption><em>Solution Architecture</em></figcaption></figure><p>The solution consists of the following components:</p><ul><li>Document AI <a href="https://cloud.google.com/document-ai/docs/workbench/build-custom-splitter-processor">Custom Document Splitter (CDS)</a>: Given a Sec 10-K document it splits the SEC 10-K form into individual sections.</li><li>Document AI <a href="https://cloud.google.com/document-ai/docs/workbench/build-custom-processor">Custom Document Extractor (CDE)</a>: Extracts key information present in tabular form from different sections of the SEC 10-K form.</li><li>Generative AI: Extracts text-based information from the SEC 10-K form.</li><li>BigQuery: Stores the extracted data</li></ul><h3>Data and Model Training</h3><p>The solution was trained on a dataset of SEC10K forms. You can find Kaggle Dataset <a href="https://www.kaggle.com/datasets/pranjalverma08/sec-edgar-annual-financial-filings-2021?resource=download">SEC Edgar Annual Financial Filings — 2021</a> for Sec10K form dataset.</p><p>For Generative AI, fields like company names, addresses, year end date are extracted by providing relevant content to the text-bison model.</p><p>For Custom Document Splitter, we divided the document into sections like Introduction and Signature along with identifying important tables like Consolidated Balance Sheet and Statement of Operations. We labeled and trained on 50+ numbers of training documents.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*A7xu-j3WS4zCOgxZ" /><figcaption><em>Snapshot of Custom Document Splitter developed</em></figcaption></figure><p>For Custom Document Extractor, the documents were labeled to identify the relevant fields. Examples of labels from tables of Consolidated Balance Sheet and Statement of Operations are total current liabilities and assets, total net sales and operating expenses with year wise mapping. We labeled and trained on 50+ numbers of training documents.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IiDpojwfGtVvG5nP" /><figcaption><em>Snapshot of fields for Custom Document Extractor developed</em></figcaption></figure><p>Below is a sample page having a Consolidated Balance Sheet table in a Sec10k form.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rBJZwtTJjYax9op4" /><figcaption><em>Consolidated Balance Sheet table in Sec10K form (</em><a href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000045/goog-20230331.htm#i0c9da2ad630a471ab2611da204d142c8_19"><em>Source</em></a><em>)</em></figcaption></figure><h3>Results</h3><p>The solution was evaluated on a test set of 20 documents and has demonstrated impressive results.</p><ul><li><strong>95%+</strong> accuracy on Document Splitter to identify different sections of the forms</li><li><strong>90%+</strong> accuracy on field extraction of tabular data using Custom Document Extractor</li><li><strong>99%+</strong> accuracy on field extraction of textual data using Generative AI</li></ul><p>We tried our solution developed on the latest filing of Sec10k form by Alphabet Inc. which is publicly available <a href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000045/goog-20230331.htm">here</a>. Below is the snapshot of the 50 pager document.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YhZdo6JY_JR1u9CG" /><figcaption><a href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000045/goog-20230331.htm#i0c9da2ad630a471ab2611da204d142c8_19">Source</a></figcaption></figure><p>Here is the output produced from the solution developed by directly ingesting the pdf shared by Alphabet.</p><pre>{&#39;company_address&#39;: &#39;1600 Amphitheatre Parkway Mountain View, CA 94043&#39;,<br>&#39;company_name&#39;: &#39;Alphabet Inc.&#39;,<br>&#39;company_phone&#39;: &#39;(650) 253-0000&#39;,<br>&#39;fiscal_year&#39;: &#39;March 31, 2023&#39;,<br>&#39;form_type&#39;: &#39;10-Q&#39;,<br>&quot;chief_financial_officer&quot;: &quot;Ruth M. Porat&quot;,<br>&#39;current_assets&#39;: {&#39;previous&#39;: &#39;164,795&#39;, &#39;current&#39;: &#39;161,985&#39; ,&#39;description&#39;: &#39;Total current assets&#39;}<br>&#39;current_liabilities&#39;: {&#39;previous&#39;: &#39;69,300&#39;, &#39;current&#39;: &#39;68,854&#39; ,&#39;description&#39;: &#39;Total current liabilities&#39;}<br>&#39;net_income&#39;: {&#39;previous&#39;: &#39;16,436&#39;, &#39;current&#39;: &#39;15,051&#39;, &#39;description&#39;: &#39;Net income&#39;}<br>&#39;total_net_sales&#39;: {&#39;previous&#39;: &#39;68,011&#39;,   &#39;current&#39;: &#39;69,787&#39;,  &#39;description&#39;: &#39;Revenues&#39;}</pre><h3>Conclusion</h3><p>The integration of Document AI and Generative AI offers a powerful solution for automating and enhancing SEC Form 10-K parsing. By leveraging machine learning and natural language processing capabilities, investors, analysts, and stakeholders can extract structured data with high accuracy, gain contextual understanding, and unlock data insights that are crucial for making informed decisions.</p><p>Learn more about the products used in the solution from links below:</p><ul><li><a href="https://cloud.google.com/document-ai?hl=en">Document AI</a></li><li><a href="https://cloud.google.com/vertex-ai?hl=en">Vertex AI</a></li><li><a href="https://cloud.google.com/storage?hl=en">Cloud Storage</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6b2a086d6167" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/automating-data-extraction-from-sec-10-k-forms-using-document-ai-and-generative-ai-6b2a086d6167">Automating data extraction from SEC 10-K forms using Document AI and Generative AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google APIs from Windows DNS Server]]></title>
            <link>https://medium.com/google-cloud/google-apis-from-windows-dns-server-874aa2cc1a35?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/874aa2cc1a35</guid>
            <category><![CDATA[dns]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[windows]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Julio Diez]]></dc:creator>
            <pubDate>Thu, 18 Apr 2024 06:14:53 GMT</pubDate>
            <atom:updated>2024-04-18T06:14:53.422Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*j8DUdJ5W5vY70kgVGcU5GA.png" /></figure><h3>Background</h3><p>Google Cloud customers often need to route particular Google-bound traffic like <em>storage.googleapis.com</em> from their on-premises network to their private Google Cloud Platform (GCP) connectivity — VPN or interconnect — rather than over the Internet, for efficiency, security, or compliance reasons. For example, this is necessary for cases like when a <a href="https://cloud.google.com/vpc-service-controls/docs/overview">VPC SC perimeter</a> surrounds Cloud Storage buckets since external requests are blocked, allowing only those from within the perimeter. If the on-premises network is part of the VPC SC perimeter, client devices there will need to forward their requests through the private connectivity to GCP to access the bucket.</p><p>In certain situations, redirecting all *.googleapis.com traffic from the on-premises network to GCP may not be feasible, and a more granular approach is required. This is particularly relevant when only a specific subset of clients should be directed to GCP projects within the company, while other clients accessing general Google APIs such as Search, Maps, or even Cloud Storage should retain access to public internet endpoints. This article outlines how to configure GCP and Windows DNS Server to achieve this selective redirection.</p><h3>Solution</h3><p>The solution presented will handle a more complex scenario where various on-premises clients are linked to different GCP environments, <em>production</em> and <em>development</em>, in addition to regular clients accessing Google APIs through public endpoints. The solution assumes that clients can be identified based on their IP subnets, primarily those connecting to GCP.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/771/1*-sRE3m2lByAAz4DR5E9KvQ.png" /></figure><p>Due to the need to consider both DNS records and client-specific fields when responding to DNS queries, modifications to DNS results are necessary. This can be achieved using DNS Response Policies (<a href="https://www.ietf.org/archive/id/draft-vixie-dnsop-dns-rpz-00.txt">RPZ</a>), or in our case <a href="https://learn.microsoft.com/en-us/windows-server/networking/dns/deploy/dns-policies-overview">DNS Policies</a> that Windows Server 2016 introduced to provide similar capabilities to RPZ. DNS Policies provide enhanced flexibility and capabilities, such as Geo-location Traffic Management (ensuring clients receive the IP address of the geographically closest resource) and Split Brain DNS (tailoring responses based on the client’s network location).</p><h4>GCP-only clients</h4><p>To ensure that all traffic to Google APIs from the on-premises network is forwarded to GCP, one solution is for the on-premises network to become authoritative for the googleapis.com zone. With Windows DNS Policies, you can create your own records for your authoritative zones and implement the capabilities mentioned earlier through new objects:</p><ul><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverzonescope?view=windowsserver2022-ps">Zone Scope</a>: a zone comprises one default zone and can have various zone scopes. Each scope may contain an identical set of DNS records, but with different IP addresses assigned to each scope.</li><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverqueryresolutionpolicy?view=windowsserver2022-ps">Query Resolution Policy</a>: a policy applied to zone scopes that establishes procedures for resolving DNS queries using predefined criteria. These criteria could include the client’s subnet or the type of record being queried.</li><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverclientsubnet?view=windowsserver2022-ps">Client Subnet</a>: an object representing subnets corresponding to clients. You can utilize client subnet objects in query resolution policies to match incoming queries to specific clients.</li></ul><p>To access Google APIs that are supported by VPC Service Controls, you must configure <a href="https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid">Private Google Access for on-premises</a>. This involves redirecting traffic to the <em>private.googleapis.com</em> or the <em>restricted.googleapis.com</em> special domain names. The <em>restricted range</em>, 199.36.153.4/30, allows access to Google APIs and services that are supported by VPC Service Controls and blocks access to those services that are not supported. We will use this range in our configuration.</p><p>Within the googleapis.com zone scope, the on-premises DNS server should set up a CNAME record from *.googleapis.com to restricted.googleapis.com. Additionally, an A record should be created, pointing to the IP addresses within the restricted range. Further steps, such as configuring routing from the on-premises network to the restricted range in GCP, are necessary. Refer to the official documentation for more details.</p><p>However, this configuration will not work for regular clients that do not require to be forwarded to GCP. Any client not recognized as being in a particular subnet that requires redirection won’t match any resolution policy and will continue with the usual resolution procedure. As a result, they’ll get a reply with the records from the regular authoritative googleapis.com zone. Of course companies can’t be authoritative for Google APIs and their public endpoints.</p><h4>GCP and non-GCP clients</h4><p>In situations where both GCP and non-GCP or regular clients reside within the on-premises network, it is not feasible for the on-premises infrastructure to serve as the authoritative source for the googleapis.com zone. As a result, any queries made by these clients need to be forwarded to external resolvers. However, it’s important to note that using a resolver for Google APIs will only provide access to public endpoints.</p><p>Fortunately, Windows DNS Policies provide an object that we can use to resolve this issue:</p><ul><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverrecursionscope?view=windowsserver2022-ps">Recursion Scope</a>: similar to zone scopes, DNS recursion can have a default behavior as well as <em>scopes</em> where recursion or resolution behaves differently by directing to a unique set of forwarders.</li></ul><p>By configuring query resolution policies to selectively choose recursion scopes based on criteria such as the client subnet, it becomes possible to resolve the same domains to different IP addresses. This approach provides a flexible solution for managing DNS resolution in complex network environments.</p><h4>High-level diagram</h4><p>The DNS process we’ll implement is outlined in the high-level diagram below. Three types of on-premises clients are used:</p><ul><li>Non-GCP or regular clients, which should continue to access Google APIs and the Internet as usual. They should not be affected by any modifications made to the DNS server.</li><li>GCP clients in a production environment, where Google APIs resolution should be forwarded to Cloud DNS in a production VPC typically through an Interconnect connection.</li><li>GCP clients in a development environment, where Google APIs resolution should be forwarded to Cloud DNS in a development VPC, usually through an Interconnect or VPN connection.</li></ul><p>Each client sends a DNS query for a given domain, such as storage.googleapis.com. Depending on the type of client, the response to the query may vary because different clients use different resolvers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/798/0*-MVssu9jEQuBtR3N" /></figure><p>Cloud DNS will act as the resolver for GCP clients. We will configure private zones to CNAME *googleapis.com to restricted.googleapis.com with A records from the 199.36.153.4/30 range. To accommodate both the production and development environments, the restricted range will be split into two, with each environment receiving two IPs. This setup allows traffic routing from the on-premises network to the appropriate physical connectivity. Additionally, inbound server policies will be created in the production and development VPC networks, and the inbound forwarder IPs will be identified. Refer to the official <a href="https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid">Google Cloud documentation</a> for instructions on configuring the GCP side.</p><p>As shown in the picture, queries are matched based on the client subnet <strong>and</strong> the specified domain. If we didn’t include the domain of the request as a criterion, all GCP client queries for non-authoritative zones would be forwarded to Cloud DNS. While this setup could work, customers typically prefer to redirect only DNS GCP traffic to GCP.</p><h4>Configuration of Windows DNS Server</h4><p>Following is the configuration of the on-premises DNS server. It uses the DNS Policy objects described previously. To create these objects, run PowerShell as an administrator on the server.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*N6cPF_ZFXhFoif-GuHbseQ.png" /></figure><ul><li>Create ClientSubnets objects in the DNS server for production and development clients, to identify DNS queries from each other and from the rest of clients. Let’s assume the range 10.100.0.0/24 is used for production and 10.200.0.0/24 for development. Multiple IP ranges can be specified if necessary. Notably, ClientSubnets objects do not automatically replicate across DNS servers. Therefore, if there are multiple servers, they must be explicitly added to each server.</li></ul><pre>&gt; $dcs = (Get-ADDomainController -Filter *).Name<br>&gt; foreach ($dc in $dcs) {<br> Add-DnsServerClientSubnet -Name &quot;Prod&quot; -IPv4Subnet &quot;10.100.0.0/24&quot; -ComputerName $dc<br> Add-DnsServerClientSubnet -Name &quot;Dev&quot; -IPv4Subnet &quot;10.200.0.0/24&quot; -ComputerName $dc<br>}</pre><ul><li>Create recursion scopes for both production and development to send traffic to Cloud DNS. Utilize the inbound forwarder IP addresses (inbound server policies) specific to each environment. Let’s assume those forwarder IPs are 172.20.0.2 (prod) and 172.30.0.2 (dev).</li></ul><pre>&gt; Add-DnsServerRecursionScope -Name &quot;CloudDNSProd&quot; -Forwarder &quot;172.20.0.2&quot; -EnableRecursion $true<br>&gt; Add-DnsServerRecursionScope -Name &quot;CloudDNSDev&quot; -Forwarder &quot;172.30.0.2&quot; -EnableRecursion $true</pre><ul><li>Now comes a not well-documented aspect. The initial query for googleapis.com from a client populates the global cache of the Windows DNS server, <em>corrupting</em> the cache for subsequent clients. To prevent this issue, it’s necessary to <em>shard</em> or <em>partition</em> the DNS server’s global cache before implementing any query resolution policy for the recursion scopes. The partitioning process is based on creating zone scopes, as the cache is treated as an additional zone.<br>Create zone scopes for the DNS server cache, which will serve as cache shards dedicated to clients meeting the query resolution policies that we will create later.</li></ul><pre>&gt; Add-DnsServerZoneScope -ZoneName &quot;..cache&quot; -Name &quot;CloudDNSProdCache&quot;<br>&gt; Add-DnsServerZoneScope -ZoneName &quot;..cache&quot; -Name &quot;CloudDNSDevCache&quot;</pre><ul><li>Bind the cache zone scopes to query resolution policies for them to take effect. The policies will use FQDN and client subnet criteria to map to the zone scopes. As client subnets, query resolution policy objects do not replicate across DNS servers.</li></ul><pre>&gt; $dcs = (Get-ADDomainController -Filter *).Name<br>&gt; foreach ($dc in $dcs) {<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSProdCache&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Prod&quot; -Action ALLOW -ZoneScope &quot;CloudDNSProdCache&quot; -ZoneName &quot;..cache&quot; -ComputerName $dc<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSDevCache&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Dev&quot; -Action ALLOW -ZoneScope &quot;CloudDNSDevCache&quot; -ZoneName &quot;..cache&quot; -ComputerName $dc<br> }</pre><ul><li>Create query resolution policies with the same criteria to map to the recursion scopes created previously.</li></ul><pre>&gt; $dcs = (Get-ADDomainController -Filter *).Name<br>&gt; foreach ($dc in $dcs) {<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSProd&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Prod&quot; -Action ALLOW -ApplyOnRecursion -RecursionScope &quot;CloudDNSProd&quot; -ComputerName $dc<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSDev&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Dev&quot; -Action ALLOW -ApplyOnRecursion -RecursionScope &quot;CloudDNSDev&quot; -ComputerName $dc<br> }</pre><ul><li>To troubleshoot DNS issues, consider taking these additional steps:<br>- Clear the DNS caches. This may temporarily affect DNS performance, but it can resolve some issues.<br>- Inspect the caches to identify any irregularities or errors.</li></ul><pre>&gt; Clear-DnsServerCache<br>&gt; Clear-DnsServerCache -CacheScope &quot;CloudDNSProdCache&quot;<br>&gt; Clear-DnsServerCache -CacheScope &quot;CloudDNSDevCache&quot;<br>&gt; Show-DnsServerCache<br>&gt; Show-DnsServerCache -CacheScope &quot;CloudDNSProdCache&quot;<br>&gt; Show-DnsServerCache -CacheScope &quot;CloudDNSDevCache&quot;</pre><p>After applying these changes, different types of on-premises clients should now resolve *.googleapis.com to different IP addresses.</p><h3>Final notes</h3><ul><li>The shell commands show how to configure redirection for *.googleapis.com. However, this is usually not the only domain required for GCP services to work from on-premises networks. Domains such as accounts.google.com or gcr.io are also typically needed. You would need to add corresponding Add-DnsServerQueryResolutionPolicy commands with those FQDNs. A list of possible domains can be found in the <a href="https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid#config-choose-domain">documentation</a>.</li><li>Although it is possible to redirect only some services like Google Cloud Storage or BigQuery specifying domains like storage.googleapis.com, using *.googleapis.com makes the DNS configuration simpler and you can still control access to GCP services based on IAM roles (as you should).</li><li>There is no GUI to configure DNS Policies, only PowerShell. However, if you already automate your IT processes with PowerShell this approach can become an advantage, as it allows for easier integration and management of DNS settings.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=874aa2cc1a35" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/google-apis-from-windows-dns-server-874aa2cc1a35">Google APIs from Windows DNS Server</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Platform Technology Nuggets — April 1–15, 2024 Edition]]></title>
            <link>https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-april-1-15-2024-edition-6a6f751a0dbf?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6a6f751a0dbf</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-tech-nuggets]]></category>
            <category><![CDATA[tech-nuggets]]></category>
            <category><![CDATA[gcp-weekly]]></category>
            <dc:creator><![CDATA[Romin Irani]]></dc:creator>
            <pubDate>Wed, 17 Apr 2024 12:48:57 GMT</pubDate>
            <atom:updated>2024-04-17T12:48:56.927Z</atom:updated>
            <content:encoded><![CDATA[<h3>Google Cloud Platform Technology Nuggets — April 1–15, 2024 Edition</h3><p>Welcome to the April 1–15, 2024 edition of Google Cloud Platform Technology Nuggets.</p><p>Please feel free to give <a href="https://forms.gle/UAsAS7YLxYSBTNBy9">feedback</a> on this issue and share the <a href="https://gcptechnuggets.substack.com/">subscription form</a> with your peers.</p><h3>Google Cloud Next 2024</h3><p>Google Cloud Next 2024 was held from April 9–11, 2024 in Las Vegas and Generative AI was centrestage across the keynotes, sessions and more. It is difficult to put down all the blog posts that go into details of what was announced/demonstrated at the conference, but here are few links worth your time to catch up on the announcements:</p><ol><li><a href="https://www.youtube.com/watch?v=M-CzbTUVykg">Cloud Next ’24 Opening Keynote in under 14 minutes</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next24">Welcome to Google Cloud Next 24 </a>: A text form of key announcements made in the Keynote.</li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2024-wrap-up">All 218 things announced at Cloud NEXT ‘24</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/next24-day-1-recap">Day 1 Recap</a>: AI Agents for everyone</li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/next24-day-2-recap">Day 2 Recap</a>: building AI Agents</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nbrSlUJ-lKVOd-oY.jpg" /></figure><p>The sessions are all available on demand at the Cloud NEXT ’24 site. The only requirement is that you will need to register to view the sessions. Check out the <a href="https://cloud.withgoogle.com/next">entire session library</a> from the conference.</p><p>The other sections in this newsletter will highlight key announcements pertaining to those areas.</p><h3>Infrastructure</h3><p>Google Cloud’s Compute portfolio saw some key announcements vis-a-vis workload optimized infrastructure. This new introductions included:</p><ul><li>C4 and N4, new general-purpose VMs powered by 5th Generation Intel Xeon processors</li><li>Upcoming preview of native bare-metal C3 machine types</li><li>X4, Compute Engine’s new memory-optimized instances, now in preview.</li><li>Z3, Google Cloud’s first storage-optimized VM family</li><li>Google Axion Processor, a new Arm-based CPU</li></ul><p>Check out the <a href="https://cloud.google.com/blog/products/compute/compute-and-infrastructure-enhancements-at-next24">blog post</a> for more details.</p><p>Google Axion Processors, our first custom Arm®-based CPUs designed for the data center has been announced. Built using the Arm Neoverse™ V2 CPU, the Axion processors are expected to deliver significant performance improvements for general-purpose workloads like web and app servers, containerized microservices, open-source databases, in-memory caches, data analytics engines, media processing, CPU-based AI training and inferencing, and more. This compute option is likely to be available to customers later in the year. Check out the <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu">blog post</a> for more details.</p><p>Two new Open Source offerings in the area of AI Inferencing have been announced: JetStream and MaxDiffusion. Check out the <a href="https://cloud.google.com/blog/products/compute/accelerating-ai-inference-with-google-cloud-tpus-and-gpus">blog post</a>.</p><p>A suggested read in this space is the key enhancements that Google Cloud has been doing in its AI Hypercomputer architecture space. <a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">Check it out.</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FKeezDA92z9dNg0e_tuDzg.jpeg" /></figure><h3>Containers and Kubernetes</h3><p>Here is a fantastic post that highlights how Container Services on Google Cloud are set to efficiently serve the needs of Gen AI applications in the future. The post nicely highlights to get started with Cloud Run for an easy AI starting point, GKE for training and inference and more. <a href="https://cloud.google.com/blog/products/containers-kubernetes/google-clouds-container-platform-for-the-next-decade-of-ai">Check out</a> how Google Cloud is positioning these services to continue serving key workloads via containers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SJnc-mRDIYmm8qo8.png" /></figure><p>Looking to serve and deploy Gemma on GKE Standard as well as Autopilot clusters, check out a <a href="https://cloud.google.com/blog/products/containers-kubernetes/serving-gemma-on-google-kubernetes-engine-deep-dive">blog post</a> that highlights the key enhancements that have been made to GKE to help you do that. The post includes Integration with Hugging Face, Kaggle and Vertex AI Model Garden, GKE notebook experience using Colab Enterprise and A cost-efficient, reliable and low-latency AI inference stack.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CrH2GwwGyw-4Op48.png" /></figure><p>GKE Autopilot announced support for burstable workload support. As the blog post states, “Bursting allows your Pod to temporarily utilize resources outside of those resources that it requests and is billed for.” Another feature announced was Pods as small as 1/20th of a vCPU can be used now. Additionally, you can create any size of Pod you like between the minimum to the maximum size, for example, 59m CPU, 302m, 808m, 7682m etc. These features when combined together is a great way to run high-density workloads. Check out the <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot-burstable-workloads">blog post</a> for more details.</p><h3>Networking</h3><p>A big announcement was Cloud Service Mesh, a fully managed service mesh across all Google Cloud platform types. This is a single offering that combines Traffic Director’s control plane and Google’s open-source Istio-based service mesh, Anthos Service Mesh. Check out the <a href="https://cloud.google.com/blog/products/networking/introducing-cloud-service-mesh">blog post</a> that highlights what customers get from this new offering and the benefits.</p><p>Google Cloud’s next-generation cloud firewall offering is now available in GA. The product Cloud Firewall Plus, now called Cloud NGFW (Next Gen Firewall) is available in 3 tiers: Essentials, Standard and Enterprise. Check out the <a href="https://cloud.google.com/blog/products/identity-security/announcing-next-gen-firewall-enterprise-now-in-ga-next24">blog post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x_WPDHw3EDCADEAE.png" /></figure><p>If you’d like to get all the details on Whats New in Networking announced at Cloud NEXT 24, check out this <a href="https://cloud.google.com/blog/products/networking/whats-new-for-networking-at-next24">blog post</a>.</p><h3>Identity and Security</h3><p>Security saw some key updates at Google Cloud NEXT ’24. Gemini in Security Operations has a new assisted investigation feature, generally available at the end of this month, which will guide analysts through their workflow wherever they are in Chronicle Enterprise and Chronicle Enterprise Plus. Gemini recommends actions based on the context of an investigation, and can run searches and create detection rules to improve response times. You can also ask Gemini for the latest threat intelligence from Mandiant directly in-line — including any indicators of compromise found in their environment — and Gemini will navigate users to the most relevant pages in the integrated platform for deeper investigation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eedRxvVFALw-ex3gsOljSg.gif" /></figure><p>This is just one of several updates in the Security space. Check out the <a href="https://cloud.google.com/blog/products/identity-security/make-google-part-of-your-security-team-supercharged-by-ai-next24">blog post </a>for more security related announcements.</p><h3>Machine Learning</h3><p>Gemini is now right across Google Cloud. Whether you are a developer, security analyst, data analyst, operator, etc — you are bound to use Gemini now across various services. The names can get a bit confusing and hence it is important that you read this article first to get the product names right and what Gemini does to help work with specific areas. Check out the <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-for-google-cloud-is-here">post</a> that talks more about the diagram that you see below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KPJX_qfeG1CgBpgr.jpg" /></figure><p>Once you are done with the above post, do check out a <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">post</a> that highlights updates to Gemini, Imagen, Gemma and MLOps on Vertex AI. The updates include Gemini Pro 1.5 now available in Public Preview, Imagen’s new text-to-live image capabilities and more.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hcZHb8R7KfOVV4evOklVEg.png" /></figure><p>When it comes to using Generative AI in Enterprise applications, grounding these systems to the truth is essential and an absolute requirement. Google Cloud defined “enterprise truth” as the approach to grounding a foundation model in web information; enterprise data like databases and data warehouses; enterprise applications like ERP, CRM, and HR systems; and other sources of relevant information. And how does it propose to do that? Via a preview of <strong>Ground with Google Search</strong> in Vertex AI. Check out the <a href="https://cloud.google.com/blog/products/ai-machine-learning/grounding-gen-ai-in-enterprise-truth">post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WAKrEtso3C_yAYWwyP98Pg.gif" /></figure><p>Vertex AI Search and Conversation products, along with other developer tools is all coming together under one umbrella: Vertex AI Agent Builder. The key objective is to help build our AI Agents that are grounded in factuality and uses key features like Vertex AI Extensions, function calling and data connectors. Check out the <a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder">blog post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X61k2gIx7kirQjZC7GHuvQ.gif" /></figure><p>If you have been tracking Vertex AI Text Embeddings, at Cloud NEXT ’24, two new text embeddings were <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-announces-new-text-embedding-models">announced</a>:</p><ul><li>English only: text-embedding-preview-0409</li><li>Multilingual: text-multilingual-embedding-preview-0409</li></ul><h3>Storage and Data Transfer</h3><p>Before Cloud NEXT ’24 where several storage innovations were announced, there were a couple of Storage updates:</p><ul><li>You can now leverage the power of Google Cloud tags, including inheritance, to easily configure backup policies for Compute Engine VMs, ensuring consistent protection of your dynamic cloud environments. Check out the <a href="https://cloud.google.com/blog/products/storage-data-transfer/tags-support-in-backup-and-dr-service-simplifies-vm-protection">post</a>.</li><li>You can now look at meeting data retention compliance via the new Object retention lock. This feature helps you set and lock retention configurations on Cloud Storage objects, with a “retain until time.” This means that an object with an object retention lock can not be deleted or replaced until the retain until time has passed. Check out the <a href="https://cloud.google.com/blog/products/storage-data-transfer/introducing-cloud-storage-object-retention-lock">post</a>.</li></ul><p>At Cloud NEXT 24, with GenAI clearly dominating the mindspace, optimized storage solutions and features were announced to address the challenge of decreasing model load, training, and inference times while maximizing accelerator utilization. These included Cloud Storage FUSE with file caching, ParallelStore, Hyperdisk ML and more. Check out the <a href="https://cloud.google.com/blog/products/storage-data-transfer/storage-announcements-at-next24">blog post</a> for more announcements made.</p><h3>Databases</h3><p>Gemini in Databases was a key announcement in the area of Databases at NEXT. This meant AI-powered assistance to simplify all aspects of the database journey including developing, monitoring, optimizing, securing and migrating database-driven applications, vector support across more of the databases and more. Check out the <a href="https://cloud.google.com/blog/products/databases/whats-new-with-databases-at-next24">blog post</a> for more details.</p><p>Check out this additional <a href="https://cloud.google.com/blog/products/databases/a-deep-dive-into-gemini-in-databases">blog post</a> that dives into Gemini for Databases and gives a preview of the AI assisted features in Database Studio.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xZKzdHbRxBz6mxadnGzU-Q.gif" /></figure><p>Expanding on the above, <a href="https://cloud.google.com/blog/products/databases/understanding-natural-language-support-in-alloydb">AlloyDB for PostgreSQL got natural language support</a>. This feature is key in helping developers integrate real-time operational data into generative AI applications. AlloyDB also introduced parameterized secure views, a new kind of database view that locks down access to end-user data at the database level to help you protect against prompt injection attacks.</p><p>Also announced is the new ScaNN index for AlloyDB, bringing 12 years of Google research and innovation in approximate nearest neighbor algorithms to AlloyDB. This index is said to “deliver up to <strong>4x</strong> faster vector queries, up to <strong>8x</strong> faster index build times and typically a <strong>3–4x</strong> smaller memory footprint than the HNSW index in standard PostgreSQL”. At the moment, it is available in technology preview in AlloyDB Omni, and will become available in the AlloyDB for PostgreSQL managed service in Google Cloud later. Check out the <a href="https://cloud.google.com/blog/products/databases/understanding-the-scann-index-in-alloydb">blog post</a> for more details.</p><p>I love the statement “How do you store the entire Internet?” in the following <a href="https://cloud.google.com/blog/products/databases/bigtable-enhancements-at-next24">blog post</a>, that highlights the goal that the team set out with, designed BigTable for that and now 20 years later, continue to boost BigTable with features that are expected to bring it to a more widestream set of users.</p><p>Looking to do migrations of SQL Server to the Cloud SQL for SQL Server managed service? A preview of support for SQL Server migrations to Cloud SQL for SQL Server in Database Migration Service is now available. DMS is a fully managed serverless cloud service that performs database migrations with minimal downtime. Check out a blog post how Database Migration Service works.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QaK7NlTLnPGyFIfK.jpg" /></figure><p>Speaking of migrations, Gemini in Databases and specifically when it comes to migrations can help out in multiple ways. How about explainability when it comes to understanding existing queries that you need to migrate, schema conversions and more. Check out an interesting <a href="https://cloud.google.com/blog/products/databases/gemini-helps-migrate-oracle-to-postgresql-on-google-cloud">blog post</a> that highlights how you can use Gemini to help migrate Oracle to Cloud SQL for PostgreSQL on Google Cloud.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dzgjz_qXDoAhEj7I.jpg" /></figure><p>Private Service Connect is a capability of Google Cloud networking that allows consumers to access managed services privately from inside their VPC network. Private Service Connect is now fully integrated with <a href="https://cloud.google.com/sql">Cloud SQL</a>, Google Cloud’s fully managed database service for PostgreSQL, MySQL, and SQL Server. Check out the <a href="https://cloud.google.com/blog/products/databases/private-service-connect-for-cloud-sql-databases">blog post</a> for details on how to get started, configuring Private Service Connect and deployment architectures.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/520/0*xMM66CL5v2XhWshS.png" /></figure><p>Memorystore for Redis Cluster saw some major announcements at Cloud NEXT:</p><ul><li>Public preview of data persistence for both RDB (Redis Database) and AOF (Append Only File)</li><li>General availability of new nodes types of 1.4 GB, 6.5 GB and 58 GB</li><li>General availability of ultra-fast vector search on Memorystore for Redis</li><li>Public preview of new configuration options</li></ul><p>Check out the <a href="https://cloud.google.com/blog/products/databases/memorystore-for-redis-cluster-updates-at-next24">blog post</a> for more details.</p><h3>Data Analytics</h3><p>If you are looking at scanning just the key announcements, check out this <a href="https://cloud.google.com/blog/products/data-analytics/data-analytics-at-next24">post</a>. Now let’s dive into some of those announcements.</p><p>BigQuery is now the single, AI-ready data analytics platform. A single product that helps you manage structured data in BigQuery tables, unstructured data like images, audience and documents, and streaming workloads, all with the best price-performance. Dive into this <a href="https://cloud.google.com/blog/products/data-analytics/bigquery-is-a-unified-ai-ready-data-analytics-platform">post</a> to understand how.</p><p>Duet AI in BigQuery is now Gemini in BigQuery. Key assistance is now available in AI augmented data preparation that helps users to cleanse and wrangle their data. Another interesting feature is the new semantic search capabilities to help you pinpoint the most relevant tables for your tasks. Leveraging the metadata and profiling information of these tables from <a href="https://cloud.google.com/dataplex?e=48754805&amp;hl=en">Dataplex</a>, Gemini in BigQuery surfaces relevant, executable queries that you can run with just one click. Check out the <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24">post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FWNlIa3m4bAkWkJfyID_wg.gif" /></figure><p>The new <strong>BigQuery data canvas </strong>provides<strong> </strong>a reimagined natural language-based experience for data exploration, curation, wrangling, analysis, and visualization, allowing you to explore and scaffold your data journeys in a graphical workflow that mirrors your mental model. Check out this <a href="https://cloud.google.com/blog/products/data-analytics/get-to-know-bigquery-data-canvas">post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yA1LiRSthQ49CcKBuNvWvw.gif" /></figure><p>The deep integration now of Gemini models into Looker is also going to open up multiple possibilities in the realm of Looker as a BI Platform. Check out this <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-looker-at-next24">post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rbu-dumyV74OC-w6Sg43Mg.gif" /></figure><p>A couple of other posts that are interesting and which give a glimpse into how BigQuery and GenAI capabilities have merged are:</p><ul><li><a href="https://cloud.google.com/blog/products/data-analytics/how-to-use-gemini-pro-vision-in-bigquery">Analyze images and videos in BigQuery using Gemini 1.0 Pro Vision</a></li><li><a href="https://cloud.google.com/blog/products/data-analytics/bigquery-multimodal-embeddings-generation">Introducing multimodal and structured data embedding support in BigQuery</a></li></ul><h3><strong>De</strong>velopers and Practitioners</h3><p>Duet AI for Developers got rebranded as Gemini Code Assist, which is set to supercharge your development workflow. There were key demos provided at Cloud NEXT Developer Keynote that showed extended capabilities that include full codebase awareness, increase in local context, code transformation support (refactoring, etc), connecting to existing source repositories and various partner integrations. Check out the <a href="https://cloud.google.com/blog/products/application-development/gen-ai-and-app-development-tools-and-partnerships">blog post</a> that highlights each of these areas.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HQ5Rd6TSsZwda3tlvHe6Vw.gif" /></figure><p><a href="https://cloud.google.com/blog/products/databases/firestore-launches-at-next24">Firestore saw some key announcements at NEXT 24</a>. These include:</p><ul><li>Use Gemini Code Assist in your favorite Integrated Development Environment (IDE) to use natural language to define your Firestore data models and write queries.</li><li>Firestore now has built-in support for vector search using exact nearest neighbors, the ability to automatically generate vector embeddings using popular embedding models via a turn-key extension, and integrations with popular generative AI libraries such as LangChain and LlamaIndex. Check out a detailed article on using <a href="https://cloud.google.com/blog/products/databases/get-started-with-firestore-vector-similarity-search">Firestore vector similarity search</a>.</li><li>Firestore now supports Customer Managed Encryption Keys (CMEK) in preview, which allows you to encrypt data stored at-rest using your own specified encryption key.</li><li>Retain daily backups using Firestore’s Scheduled Backup feature for up to 98 days, up from seven days.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0rbVbd1WHXzgpdoyJzT0sA.gif" /></figure><p>An interesting new service has been announced at Cloud NEXT 24 : App Hub. Think of one or more applications that you have deployed on Google Cloud. It is a challenge to understand visualizing that application in terms of not just the cloud resources that it uses but dependencies on other services, across project services and more. App Hub is targetted to address that by introducing abstractions in the form of Applications, Workloads and Services. It is able to injest automatically key Google Cloud services that Applications would use and then build out a dependency graph that is kept updated all the time. The current resources that it supports are various Load Balancing services (Services) and Compute Engine MIGs (Workloads). As the service grows to support GKE and Cloud Run, it could get very useful. Check out the <a href="https://cloud.google.com/blog/products/application-development/introducing-app-hub">blog post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/837/0*5XM3_D4twH2WlPqq.png" /></figure><p>If you are using Apigee, Gemini Code Assist is making its way into the product suite to help ease the task of developing API specifications, API integrations and more. For example, when it comes to building out APIs, you can build out an API Specification using the Gemini Code Assist integration inside of the Cloud Code VS Code extension. These specifications can then be published to the API Hub. Not just that but Gemini offers step-by-step guidance for adding new policy configurations while creating an API proxy. Lastly, Gemini also provides explanations for your existing configurations, reducing the learning curve during updates and maintenance. Check out the <a href="https://cloud.google.com/blog/products/api-management/gemini-code-assist-for-apigee-api-management">post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*F3_-Swqf_VlNZ1NguXo5xw.gif" /></figure><h3>Learn Google Cloud</h3><p>When it comes to service discovery and DNS resolution in your GKE clusters, you have a choice with kube-dns, Cloud DNS, etc. Deep dive into this options via this informative <a href="https://cloud.google.com/blog/products/networking/understanding-dns-options-for-gke">blog post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4jraBWptrpXidDTE.png" /></figure><h3>Stay in Touch</h3><p>Have questions, comments, or other feedback on this newsletter? Please send <a href="https://forms.gle/UAsAS7YLxYSBTNBy9">Feedback</a>.</p><p>If any of your peers are interested in receiving this newsletter, send them the <a href="https://gcptechnuggets.substack.com/">Subscribe</a> link.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6a6f751a0dbf" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-april-1-15-2024-edition-6a6f751a0dbf">Google Cloud Platform Technology Nuggets — April 1–15, 2024 Edition</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Master Data Management Simplified: Match & Merge with Generative AI!]]></title>
            <link>https://medium.com/google-cloud/master-data-management-simplified-match-merge-with-generative-ai-35fd35d306c2?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/35fd35d306c2</guid>
            <category><![CDATA[mdm]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai]]></category>
            <dc:creator><![CDATA[Abirami Sukumaran]]></dc:creator>
            <pubDate>Wed, 17 Apr 2024 01:27:38 GMT</pubDate>
            <atom:updated>2024-04-17T01:27:38.029Z</atom:updated>
            <content:encoded><![CDATA[<p>It is my dream to accelerate some of the tedious MDM processes with Generative AI, Embeddings, Vector Search and more… Read on.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/740/1*zccJwrLqwmYQ38V2yeCtRw.png" /><figcaption>Image showing a bike station with docked bikes to represent the use case</figcaption></figure><h3>Introduction</h3><p>At its core, Master Data Management (MDM) is about creating a single, reliable source of truth for your organization’s most critical data. Think of MDM as a meticulously curated library where every book (data point) is correctly labeled, up-to-date, and easy to find. Implementing robust MDM has always been a strategic necessity, but it often comes with complexities and resource demands. This is where the transformative power of Generative AI, particularly models like Gemini 1.0 Pro, Gemini 1.0 Pro Vision, Gemini 1.5 Pro, comes into play.</p><blockquote><strong>In this article</strong>, we will demonstrate how Gemini 1.0 Pro simplifies master data management applications like enrichment and deduplication, for the citibike_stations data available in the BigQuery public dataset. For this, we will use:<br>1. BigQuery public dataset bigquery-public-data.new_york_citibike.<br>2. Gemini Function Calling (a Java Cloud Function that gets the address information using the reverse Geocoding API for the coordinates available with the citibike_stations data) that we have already created in <a href="https://medium.com/google-cloud/using-gemini-function-calling-in-java-for-deterministic-generative-ai-responses-4c86a5ab80a9">one</a> of our previous articles.<br>3. Vertex AI Embeddings API and Vector Search in BigQuery to identify duplicates.</blockquote><h3>Master Data Management (MDM): The Foundation for Data-Driven Decisions</h3><p>The key elements of master data are business entities (like customers, products, suppliers, locations, etc. which are the nouns that your business revolves around), identifiers (Unique identifiers ensure each entity is distinct and traceable across systems), attributes (These are the characteristics that describe each entity e.g. a customer’s address, a product’s price etc.). I am going to take a little space here with it because understanding the importance of MDM is paramount in the generative AI era more than ever. Why MDM Matters:</p><ul><li><strong>Adds clarity to data:</strong></li></ul><p>Without MDM, businesses struggle with fragmented data scattered across various systems. This leads to inconsistencies, duplicates, and sometimes ambiguous understanding of relationships between data points (e.g. is “Abirami Sukumaran” in one system the same customer as “Abirami S.” in another?).</p><ul><li><strong>Helps in decision making:</strong></li></ul><p>Accurate, unified master data is the foundation of informed business decisions. It answers questions like — Who are our most valuable customers? Which products are underperforming? What suppliers provide the best value?</p><ul><li><strong>Serves data as a Product:</strong></li></ul><p>MDM is essential for treating data as a valuable asset. Clean, integrated, and up-to-date master data forms the basis for building insightful data products that drive business outcomes.</p><p><strong>Let me paint a picture to help you understand master data better by comparing it with transactional data: </strong>Transactional data captures individual events (a purchase, a shipment etc.). Master data provides the context for those events by defining the entities involved. For example, a sales transaction links to master data for the customer, product, and salesperson. Without MDM, data tends to remain in silos hindering a holistic view, prone to quality issues and relatively heavy resource-spend on data reconciliation and consolidation on demand.</p><h3>The powerful synergy of MDM and Generative AI</h3><p>Below are some of the industry grade applications and advantages of empowering and simplifying MDM with Generative AI:</p><ul><li><strong>Automated Data Cleansing:</strong> Gemini 1.0 Pro can intelligently identify and rectify inconsistencies, duplicates, and errors in master data, significantly reducing manual effort.</li><li><strong>Intelligent Matching:</strong> Leveraging its language understanding capabilities, Generative AI capabilities, embeddings and Vector Search, come together to accurately match and merge records from disparate sources, even with variations in formatting or naming conventions.</li><li><strong>Contextual Enrichment:</strong> Generative AI models can generate missing attributes or provide additional context to master data, enhancing its value for analysis and decision-making.</li><li><strong>Adaptive Learning</strong>: You can leverage fine tuning and reinforcement learning to enable generative AI to continuously improve its understanding of your business entities and relationships, leading to more accurate and efficient MDM over time.</li></ul><h3>High Level Flow Diagram</h3><p>This diagram represents the flow of data and steps involved in the implementation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1001/1*pMW7YoxbqYbJjYXgucff9g.png" /><figcaption>High level flow of the use case</figcaption></figure><h3>Demo</h3><p>The steps loosely are as follows:</p><ol><li>Create a BigQuery dataset for the use case. Create a landing table with data from the public dataset table bigquery-public-data.new_york_citibike.citibike_stations.</li><li>Make sure the Cloud Function that includes Gemini Function Calling for address standardization is deployed.</li><li>Store the enriched address data in the landing tables (from 2 sources for demo purpose).</li><li>Invoke Vertex AI Embeddings API from BigQuery on the address data.</li><li>Use BigQuery Vector Search to identify duplicate records.</li></ol><h3>Setup</h3><ol><li>In the <a href="https://console.cloud.google.com/">Google Cloud Console</a>, on the project selector page, select or create a Google Cloud <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects">project</a>.</li><li>Make sure that billing is enabled for your Cloud project. Learn how to <a href="https://cloud.google.com/billing/docs/how-to/verify-billing-enabled">check if billing is enabled on a project</a>.</li><li>You will use Cloud Shell, a command-line environment running in Google Cloud that comes preloaded with bq. From the Cloud Console, click Activate Cloud Shell on the top right corner.</li><li>Enable necessary APIs for this implementation if you haven’t already. Alternative to the gcloud command is through the console using this <a href="https://pantheon.corp.google.com/apis/enableflow?apiid=cloudfunctions.googleapis.com,run.googleapis.com,bigquery.googleapis.com,bigqueryconnection.googleapis.com,aiplatform.googleapis.com">link</a>.</li></ol><h3>Step 1: Create BigQuery Dataset and External Connection</h3><p>BigQuery <a href="https://cloud.google.com/bigquery/docs/datasets-intro">dataset</a> is a container for all the tables and objects for your application. BigQuery <a href="https://cloud.google.com/bigquery/docs/remote-functions#create_a_connection">connection</a> is used to interact with your Cloud Function. In order to create a remote function, you must create a BigQuery connection. Let’s begin with creating the dataset and the connection.</p><ol><li>From the Google Cloud Console, go to the <a href="https://console.cloud.google.com/bigquery">BigQuery page</a> and click the 3 vertical dots icon next to your project id. From the list of options, select “Create data set”.</li><li>In Create data set pop up, enter the data set ID “mdm_gemini” with the region set to the default value “US (multiple regions…)”</li><li><a href="https://cloud.google.com/bigquery/docs/remote-functions#create_a_connection">BigLake Connection</a> allows us to connect the external data source while retaining fine-grained BigQuery access control and security, which in our case is the Vertex AI Gemini Pro API. We will use this connection to access the model from BigQuery via the Cloud Function. Follow steps below to create the BigLake Connection:</li></ol><p>a. Click ADD on the Explorer pane of the BigQuery page:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/0*mqX5rzEHWjkzEQIE" /><figcaption>ADD button to create connection</figcaption></figure><p>b. Click Connections to external data sources in the sources page.</p><p>c. Enter external data source details as below in the pop up that shows up and click CREATE CONNECTION:</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*b31hiO4ynbDLRrXWEFF4aQ.png" /><figcaption>Create Connection</figcaption></figure><p>d. Once the connection is created, go to the connection configuration page and copy the Service account ID for access provisioning:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xh8uToOATp-TmC5O" /><figcaption>Connection Info</figcaption></figure><p>e. Open <a href="https://console.cloud.google.com/iam-admin/iam">IAM and admin</a> page, click GRANT ACCESS, enter the service account id in the new principals tab and roles as shown below and click SAVE.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fCExrL0trAtqirew" /><figcaption>Grant Access</figcaption></figure><h3>Step 2: Deploy Function Calling (Java Cloud Function)</h3><ol><li>Clone the github <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling">repo</a> from your Cloud Shell Terminal and change the YOUR_API_KEY and YOUR_PROJECT_ID with your values</li></ol><pre>git clone https://github.com/AbiramiSukumaran/GeminiFunctionCalling</pre><p>2. Go to Cloud Shell terminal, navigate into the newly cloned project directory “GeminiFunctionCalling” and execute the below statement build and deploy the Cloud Function:</p><pre>gcloud functions deploy gemini-fn-calling --gen2 --region=us-central1 --runtime=java11 --source=. --entry-point=cloudcode.helloworld.HelloWorld --trigger-http</pre><p>The result for this deploy command would be a REST URL in the format as below :</p><p><a href="https://us-central1-abis-345004.cloudfunctions.net/gemini-bq-fn"><strong>https://us-central1-</strong>YOUR_PROJECT_ID<strong>.cloudfunctions.net/gemini-fn-calling</strong></a></p><p>3. Test this Cloud Function by running the following command from the terminal:</p><pre>gcloud functions call gemini-fn-calling - region=us-central1 - gen2 - data &#39;{&quot;calls&quot;:[[&quot;40.714224,-73.961452&quot;]]}&#39;</pre><p>Response for a random sample prompt:</p><pre>  &#39;{&quot;replies&quot;:[&quot;{ \&quot;DOOR_NUMBER\&quot;: \&quot;277\&quot;, \&quot;STREET_ADDRESS\&quot;: \&quot;Bedford Ave\&quot;, \&quot;AREA\&quot;:<br>  null, \&quot;CITY\&quot;: \&quot;Brooklyn\&quot;, \&quot;TOWN\&quot;: null, \&quot;COUNTY\&quot;: \&quot;Kings County\&quot;, \&quot;STATE\&quot;:<br>  \&quot;NY\&quot;, \&quot;COUNTRY\&quot;: \&quot;USA\&quot;, \&quot;ZIPCODE\&quot;: \&quot;11211\&quot;, \&quot;LANDMARK\&quot;: null}}```&quot;]}&#39;</pre><p>The request and response parameters of this Cloud Function are implemented in a way that is compatible with BigQuery’s remote function invocation. It can be directly consumed from BigQuery data in-place. It means that if your data input (lat and long data) lives in BigQuery then you can call the remote function on the data and get the function response which can be stored or processed within BigQuery directly.</p><p>4. Run the following DDL from BigQuery to create a remote function that invokes this deployed Cloud Function:</p><pre> CREATE OR REPLACE FUNCTION <br>  `mdm_gemini.MDM_GEMINI` (latlng STRING) RETURNS STRING<br>  REMOTE WITH CONNECTION `us.gemini-bq-conn`<br>  OPTIONS (<br>    endpoint = &#39;https://us-central1-YOUR_PROJECT_ID.cloudfunctions.net/gemini-fn-calling&#39;, max_batching_rows = 1<br>  );</pre><p><strong>WORKAROUND</strong></p><p>If you do not have the necessary key or Cloud Function deployed, feel free to jump to the landing table directly by exporting the data from the csv into your new BigQuery dataset mdm_gemini using the following command in the Cloud Shell Terminal. <strong>Remember</strong> to download the file <a href="https://github.com/AbiramiSukumaran/MDMwithGemini/blob/main/CITIBIKE_STATIONS.csv">CITIBIKE_STATIONS.csv</a> from the <a href="https://github.com/AbiramiSukumaran/MDMwithGemini">repo</a> into your Cloud Shell project folder and navigate into that folder before executing the following command:</p><pre>bq load --source_format=CSV --skip_leading_rows=1 mdm_gemini.CITIBIKE_STATIONS ./CITIBIKE_STATIONS.csv \ name:string,latlng:string,capacity:numeric,num_bikes_available:numeric,num_docks_available:numeric,last_reported:timestamp,full_address_string:string</pre><h3>Step 3: Create Table and Enrich Address data</h3><p>a. If you have used the WORKAROUND approach from the last step, you can skip this step, since you have already created the table there. If NOT, proceed to the running the following DDL in BigQuery SQL Editor:</p><pre>CREATE TABLE mdm_gemini.CITIBIKE_STATIONS as (<br>select  name, latitude || &#39;,&#39; || longitude as latlong, capacity, num_bikes_available, num_docks_available,last_reported,<br>&#39;&#39; as full_address_string <br> from bigquery-public-data.new_york_citibike.citibike_stations) ;</pre><p>Let’s enrich the address data by invoking the remote function on the latitude and longitude coordinates available in the table. Please note that we will update this only for data reported for the year 2024 and where number of bikes available &gt; 0 and capacity &gt; 100:</p><pre>update `mdm_gemini.CITIBIKE_STATIONS` <br>set full_address_string = `mdm_gemini.MDM_GEMINI`(latlong) <br>where EXTRACT(YEAR FROM last_reported) = 2024 and num_bikes_available &gt; 0 and capacity &gt; 100;</pre><p>b. Do not skip this step even if you used the WORKAROUND approach in the last step. This is where we will create a second source of bike station location data for the purpose of this use case. Afterall, MDM is bringing data from multiple sources together and identifying the golden truth.</p><p>Run the following DDLs in BigQuery SQL Editor for creating the second source of location data with 2 records in it. Let’s name this table mdm_gemini.CITIBIKE_STATIONS_SOURCE2 and insert 2 records into it:</p><pre>CREATE TABLE mdm_gemini.CITIBIKE_STATIONS_SOURCE2 (name STRING(55), address STRING(1000), embeddings_src ARRAY&lt;FLOAT64&gt;);<br><br>insert into mdm_gemini.CITIBIKE_STATIONS_SOURCE2 VALUES (&#39;Location broadway and 29&#39;,&#39;{ &quot;DOOR_NUMBER&quot;: &quot;1593&quot;, &quot;STREET_ADDRESS&quot;: &quot;Broadway&quot;, &quot;AREA&quot;: null, &quot;CITY&quot;: &quot;New York&quot;, &quot;TOWN&quot;: null, &quot;COUNTY&quot;: &quot;New York County&quot;, &quot;STATE&quot;: &quot;NY&quot;, &quot;COUNTRY&quot;: &quot;USA&quot;, &quot;ZIPCODE&quot;: &quot;10019&quot;, &quot;LANDMARK&quot;: null}&#39;, null);<br><br>insert into mdm_gemini.CITIBIKE_STATIONS_SOURCE2 VALUES (&#39;Allen St &amp; Hester&#39;,&#39;{ &quot;DOOR_NUMBER&quot;: &quot;36&quot;, &quot;STREET_ADDRESS&quot;: &quot;Allen St&quot;, &quot;AREA&quot;: null, &quot;CITY&quot;: &quot;New York&quot;, &quot;TOWN&quot;: null, &quot;COUNTY&quot;: &quot;New York County&quot;, &quot;STATE&quot;: &quot;NY&quot;, &quot;COUNTRY&quot;: &quot;USA&quot;, &quot;ZIPCODE&quot;: &quot;10002&quot;, &quot;LANDMARK&quot;: null}&#39;, null);</pre><h3>Step 4: Generate Embeddings for Address Data</h3><p>Embeddings are high-dimensional numerical vectors that represent a given entity, like a piece of text or an audio file. Machine learning (ML) models use embeddings to encode semantics about such entities to make it easier to reason about and compare them. For example, a common operation in clustering, classification, and recommendation models is to measure the distance between vectors in an embedding space to find items that are most semantically similar. The Vertex AI text-embeddings API lets you create a text embedding using Generative AI on Vertex AI. Text embeddings are numerical representations of text that capture relationships between words and phrases. Read more about Vertex AI Text Embeddings <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings">here</a>.</p><p>Run the below DDL to create a remote model for Vertex AI text embeddings API:</p><pre>CREATE OR REPLACE MODEL `mdm_gemini.CITIBIKE_STATIONS_ADDRESS_EMB`<br>REMOTE WITH CONNECTION `us.gemini-bq-conn`<br>OPTIONS (ENDPOINT = &#39;textembedding-gecko@latest&#39;);</pre><p>Now that the remote embeddings model is ready, let’s generate embeddings for the first source and store it in a table. You can store the embeddings result field in the same mdm_gemini.CITIBIKE_STATIONS table as before, but I am choosing to create a new one for clarity:</p><pre>CREATE TABLE `mdm_gemini.CITIBIKE_STATIONS_SOURCE1` AS (<br>SELECT *<br>FROM ML.GENERATE_EMBEDDING(<br>  MODEL `mdm_gemini.CITIBIKE_STATIONS_ADDRESS_EMB`,<br>  ( select name, full_address_string as content from `mdm_gemini.CITIBIKE_STATIONS` <br>  where full_address_string is not null )<br>   )<br>);</pre><p>Let’s generate embeddings for address data in table CITIBIKE_STATIONS_SOURCE2:</p><pre>update `mdm_gemini.CITIBIKE_STATIONS_SOURCE2` a set embeddings_src =<br>(<br>SELECT  ml_generate_embedding_result<br>FROM ML.GENERATE_EMBEDDING(<br>  MODEL `mdm_gemini.CITIBIKE_STATIONS_ADDRESS_EMB`,<br>  ( select name, address as content from `mdm_gemini.CITIBIKE_STATIONS_SOURCE2` ))<br>where name = a.name) where name is not null;<br>This should create embeddings for the second source, note that we have created the embeddings field in the same table CITIBIKE_STATIONS_SOURCE2.</pre><p>To visualize the embeddings are generated for the source data tables 1 and 2, run the below queries:</p><pre>select name,address,embeddings_src from `mdm_gemini.CITIBIKE_STATIONS_SOURCE2`;<br>select name,content,ml_generate_embedding_result from `mdm_gemini.CITIBIKE_STATIONS_SOURCE1`;</pre><p>Let’s go ahead and perform vector search to identify duplicates.</p><h3>Step 5: Vector Search for Flagging Duplicate Addresses</h3><p>In this step, we will search the address embeddings ml_generate_embedding_result column of the `mdm_gemini.CITIBIKE_STATIONS_SOURCE1` table for the top 2 embeddings that match each row of data in the embeddings_src column of the `mdm_gemini.CITIBIKE_STATIONS_SOURCE2` table:</p><pre>select query.name name1,base.name name2, <br>/* (select address from mdm_gemini.CITIBIKE_STATIONS_SOURCE2 where name = query.name) content1, base.content content2, */<br> distance<br> from VECTOR_SEARCH(<br>  TABLE mdm_gemini.CITIBIKE_STATIONS_SOURCE1,<br>  &#39;ml_generate_embedding_result&#39;,<br>  (SELECT * FROM mdm_gemini.CITIBIKE_STATIONS_SOURCE2),<br>  &#39;embeddings_src&#39;,<br>  top_k =&gt; 2 <br>) where query.name &lt;&gt; base.name<br>order by distance desc;</pre><p><strong>Table that we are querying:</strong> mdm_gemini.CITIBIKE_STATIONS_EMBEDDINGS on the field ‘ml_generate_embedding_result’</p><p><strong>Table that we use as base:</strong> mdm_gemini.CITIBIKE_STATIONS_SOURCE2 on the field ‘embeddings_src’</p><p><strong>top_k:</strong> specifies the number of nearest neighbors to return. The default is 10. A negative value is treated as infinity, meaning that all values are counted as neighbors and returned.</p><p><strong>distance_type:</strong> specifies the type of metric to use to compute the distance between two vectors. Supported distance types are EUCLIDEAN and COSINE. The default is EUCLIDEAN.</p><p>The result of the query is as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EQJgLI91pzmTsXnW" /><figcaption>Result Set</figcaption></figure><p>As you can see, we have listed 2 nearest neighbors (in other words, closest duplicates) for the 2 rows in CITIBIKE_STATIONS_SOURCE2 from CITIBIKE_STATIONS_SOURCE1. Since the distance_type is unspecified, it assumes that it is EUCLIDEAN and the distance is read as the distances in address TEXT values between the two sources, lowest being the most similar address texts.</p><p>Let’s set distance_type to COSINE:</p><pre>select query.name name1,base.name name2, <br>/* (select address from mdm_gemini.CITIBIKE_STATIONS_SOURCE2 where name = query.name) content1, base.content content2, */<br> distance<br> from VECTOR_SEARCH(<br>  TABLE mdm_gemini.CITIBIKE_STATIONS_SOURCE1,<br>  &#39;ml_generate_embedding_result&#39;,<br>  (SELECT * FROM mdm_gemini.CITIBIKE_STATIONS_SOURCE2),<br>  &#39;embeddings_src&#39;,<br>  top_k =&gt; 2,distance_type =&gt; &#39;COSINE&#39;<br>) where query.name &lt;&gt; base.name<br>order by distance desc;</pre><p>Result:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/905/0*lkeIwXC98QwSoqoq" /><figcaption>Result set</figcaption></figure><p>Both queries (of both distance types) are ordered by distance DESCENDING which means we want to list the results in the order of decreasing distance. But you will notice that the second query’s distance order is reversed. Guess why?</p><p>Yes!! You got it right! When we say COSINE, it returns the similarity. So bigger the number, closer the similarity and hence lesser the distance. In EUCLIDEAN, bigger the number, farther the distance of values (in this case text).</p><blockquote><strong>Tips to understand the difference and applications of EUCLIDEAN and COSINE:</strong></blockquote><blockquote><strong>Euclidean Distance</strong> measures the straight-line distance between two points in a multi-dimensional space and<strong> Cosine Similarity</strong> measures the cosine of the angle between two vectors.</blockquote><blockquote>Scale Invariance: Cosine similarity is not affected by the magnitude (length) of vectors, only their direction. This is useful when comparing documents of different lengths or user preferences with varying intensities.</blockquote><blockquote>Curse of Dimensionality: Euclidean distance can become less informative in very high-dimensional spaces, while cosine similarity tends to hold up better.</blockquote><blockquote>When to Use Which:<br>Similar Scale, Absolute Differences Matter: Euclidean distanceDifferent Scales, Direction Matters More: Cosine similarity</blockquote><blockquote>5. Example:<br>Imagine two users’ movie ratings:<br>User 1: [5, 4, 3] (action, comedy, drama)<br>User 2: [10, 8, 6] (same genres, but rated higher)</blockquote><blockquote>Euclidean distance would be large due to the difference in rating scale.<br>Cosine similarity would be high, indicating similar taste despite the rating difference.</blockquote><blockquote>Please note in our example, we have used location text similarity — it doesn’t mean we are tracing the absolute geocoding distance between the 2 locations by value. We are only finding their similarity by the address text values.</blockquote><p>I built a UI around the Vector Search data from BigQuery for visualizing nearest neighbors based on their similarity in address string, NOT THE PHYSICAL DISTANCE, for a slightly larger dataset in a simple Java Spring Boot application and deployed it in Cloud Run. I created a <a href="https://github.com/AbiramiSukumaran/mdm_gemini_web">sample app</a>, edit the SQL in the controller class to match your distance results table:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X5vJaFouSO6L2e97C-fnoA.gif" /><figcaption>A demo of Bike Station Location UI that lists stations with similar addresses (not actual distance)</figcaption></figure><h3>Conclusion</h3><p>This project has demonstrated the power of using Gemini 1.0 Pro and Function Calling in transforming a few MDM activities into simplified yet powerful, deterministic and reliable generative AI capabilities. Now that you know, feel free to identify other ways of implementing the same use case or other MDM functionalities. Are there datasets you could validate, information gaps you could fill, or tasks that could be automated with structured calls embedded within your generative AI responses? Here is the link to the <a href="https://github.com/AbiramiSukumaran/MDMwithGemini">repo</a> and for further reading, refer to the documentation for <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>, <a href="https://cloud.google.com/bigquery/docs/remote-functions">BigQuery Remote Functions</a>, and <a href="https://cloud.google.com/functions">Cloud Functions</a>, <a href="https://cloud.google.com/bigquery/docs/vector-index-text-search-tutorial#create_the_remote_model_for_text_embedding_generation">Embeddings</a>, <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/search_functions#vector_search">Vector Search</a> for more in-depth guidance.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=35fd35d306c2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/master-data-management-simplified-match-merge-with-generative-ai-35fd35d306c2">Master Data Management Simplified: Match &amp; Merge with Generative AI!</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps…]]></title>
            <link>https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/c9ce870dcb99</guid>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[google-workspace]]></category>
            <category><![CDATA[google-spreadsheets]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-sheets]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 09:29:21 GMT</pubDate>
            <atom:updated>2024-04-16T09:29:21.906Z</atom:updated>
            <content:encoded><![CDATA[<h3>Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps Script</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*rqD6sT_95bXtlahR.jpg" /></figure><h3>Abstract</h3><p>Consolidate scattered cell references (A1Notation) in Google Sheets for efficiency. This script helps select cells by background color or update values/formats, overcoming limitations of large range lists.</p><h3>Introduction</h3><p>When working with Google Spreadsheets, there might be a scenario where you need to process scattered A1Notations (cell addresses in the format “A1”). This could involve selecting cells with specific background colors, updating cell values, or modifying cell formats.</p><p>One approach to handle scattered A1Notations is to create a range list containing the individual cell coordinates and activate it. However, this method becomes inefficient when dealing with a large number of cells due to the high processing cost associated with activating each cell individually.</p><p>To address this limitation, consolidating scattered A1Notations into continuous ranges offers a significant performance improvement. While a previous report discussed expanding consolidated A1Notations back into individual cells Ref: <a href="https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/">https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/</a>, consolidating them for processing efficiency had not been covered.</p><p>During the development of a script to achieve consolidation, it became apparent that existing solutions were not straightforward. To ensure clarity and facilitate debugging in the initial stages, the script was created by splitting each step into smaller, testable functions. While this approach might appear less elegant, it prioritizes understandability during the development process.</p><p>The provided script offers a solution for consolidating scattered A1Notations, as illustrated in the demonstration image. By consolidating the notations, the script can efficiently select cells with a specific background color, reducing the overall processing cost.</p><p>Furthermore, the script’s functionality can be extended to other use cases. For instance, it can be used to update the values or formats of scattered cells across the spreadsheet.</p><h3>Principle</h3><p>In this script, the process of consolidating A1Notations into rectangles is achieved by calculating the maximum rectangle size for all given A1Notations. This essentially combines scattered A1Notations into a single, most efficient rectangle.</p><p>The sample situation is as follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*53lMnCPL-j25cyO-.png" /></figure><p>The cells with a red background color are used in this example. When the A1Notations are retrieved from those cells, it is as follows.</p><pre>[<br>  &quot;C2&quot;,<br>  &quot;D2&quot;,<br>  &quot;E2&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;C3&quot;,<br>  &quot;D3&quot;,<br>  &quot;E3&quot;,<br>  &quot;C4&quot;,<br>  &quot;D4&quot;,<br>  &quot;C6&quot;,<br>  &quot;D6&quot;,<br>  &quot;C7&quot;,<br>  &quot;D7&quot;,<br>  &quot;B8&quot;,<br>  &quot;C8&quot;,<br>  &quot;D8&quot;,<br>  &quot;E8&quot;,<br>  &quot;F8&quot;<br>]</pre><p>When these A1Notations are consolidated, it becomes as follows.</p><pre>[<br>  &quot;C2:E3&quot;,<br>  &quot;C6:D8&quot;,<br>  &quot;C4:D4&quot;,<br>  &quot;E8:F8&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;B8&quot;<br>]</pre><p>Here, the maximum size of the rectangle is calculated starting from the top-left cell (C2 in this example). This approach determines the result values in the above output order.</p><p>It’s important to note that if the situation is changed, the maximum size of the rectangle might not always be obtainable using this method. While it’s possible to modify the starting cell for calculation to ensure the maximum rectangle size is always found, this can significantly increase the processing cost. Therefore, this script adopts the top-left cell as the starting point for calculation to strike a balance between efficiency and accuracy.</p><p>The image provides a visual representation of the consolidation process applied to a sample set of A1Notations.</p><p>The script can be seen at <a href="https://github.com/tanaikech/UtlApp/blob/master/forStringProcessing.js#L466">my repository</a>.</p><h3>Usage</h3><h3>1. Create a Google Spreadsheet</h3><p>Please create a new Google Spreadsheet. And, please set the background color as the above image. In this sample, the background colors are set in “B2:F8”.</p><p>And, please open the script editor of this Spreadsheet.</p><h3>2. Install library</h3><p>In this case, the script is a bit complicated. So, I added this script to my existing library <a href="https://github.com/tanaikech/UtlApp">UtlApp</a>. By this, this library can expand and consolidate the A1Notations.</p><p>You can see how to install this library at <a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#1-install-library">here</a>.</p><h3>3. Sample script 1</h3><p>In this sample, the situation of the above image is used. The script is as follows.</p><pre>function sampl1() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  Browser.msgBox(JSON.stringify(res));<br>}</pre><p>When this script is run, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/909/0*HGSpkIuucZ75_3Xh.gif" /></figure><h3>4. Sample script 2</h3><p>In this sample, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/0*xTvZlCIhIhj0z6mH.gif" /></figure><p>The cells of the red background color are selected by this script. And, the background color of the selected cells is manually changed.</p><p>The script is as follows.</p><pre>function sampl2() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  sheet.getRangeList(res).activate();<br>}</pre><h3>IMPORTANT</h3><ul><li>I’m worried that this method might not be able to be used on a Google Spreadsheet with a large size because of the process cost.</li></ul><h3>Note</h3><ul><li>The top abstract image was created by <a href="https://gemini.google.com/">Gemini</a> from the section of “Introduction”.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9ce870dcb99" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99">Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps…</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FHIR Whistle Data Mappings Validation]]></title>
            <link>https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/cd62c8613a92</guid>
            <category><![CDATA[healthcare-data-engine]]></category>
            <category><![CDATA[fhir-mapping]]></category>
            <category><![CDATA[whistle-data-mapping]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ashwinshetty]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:06:38 GMT</pubDate>
            <atom:updated>2024-04-16T06:26:24.849Z</atom:updated>
            <content:encoded><![CDATA[<h3>Business Scenario</h3><p>Healthcare Data Engine(HDE) is a popular GCP based solution to help Healthcare stakeholders transition to FHIR (Fast Healthcare Interoperability Resources). HDE provides pipelines which helps convert non FHIR data to FHIR and reconciles them to form a single Longitudinal Patient Record, which then makes deriving insights from patient data easy and quick.</p><p>One of the core components of HDE is the Data Mapping Language known as Whistle. This Open Source Data Mapping language is used for converting complex, nested data from one schema to another. For Example, from HL7 to FHIR.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/538/1*ODs0XPOT-aUALiRqchds9Q.png" /></figure><p>This article talks about how to use Whistle to write sample mappings for HL7 data. Run the mapping code locally and then test the resultant converted FHIR format data against a FHIR store.</p><h3><strong>What do we need</strong></h3><p>We will test the Whistle Mappings to convert sample HL7 data to FHIR. We will be leveraging APIs provided by GCP Healthcare API to ingest some sample HL7 messages to a HL7 store provided by GCP Healthcare API. We will use the schematized variant of this HL7 message from HL7 store and run Whistle mapping code to convert it to FHIR on our local machines. We will then test this converted FHIR data by ingesting it into GCP Healthcare API FHIR store</p><h3>Steps</h3><p><strong>Step 1</strong> — Enable GCP Cloud Healthcare API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/953/1*4QU0vG0lwQMn5RSS-yC2Fw.png" /></figure><p><strong>Step 2</strong> — Follow documentation in <a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">git repo — Healthcare Data Harmonization</a> to set up Whistle Engine on our local machines. This would need us to install below softwares on our local machines, as we will be using ‘<strong>gradle</strong>’ to run our Whistle engine application.</p><ul><li><a href="https://git-scm.com/">Git</a></li><li><a href="https://www.azul.com/downloads/?version=java-11-lts&amp;package=jdk#zulu">JDK 11.x</a></li><li><a href="https://gradle.org/next-steps/?version=7.6&amp;format=bin">Gradle 7.x</a></li></ul><p><strong>Step 3 </strong>— Create a <a href="https://cloud.google.com/healthcare-api/docs/datasets#create-dataset">Healthcare API Dataset</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2#creating_an_hl7v2_store">HL7 store</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/fhir#creating_a_fhir_store">Fhir store</a> inside that dataset. We will use these resources for our testing.</p><p>Once created we should see an output like below, where ‘<strong>datastore</strong>’ is our Healthcare API Dataset, ‘<strong>hl7v2store</strong>’ is the HL7 store and ‘<strong>fhirstore</strong>’ is the FHIR store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/793/1*FfLztPEw6stASDDU3c6C3w.png" /></figure><p><strong>Step 4</strong> — Let us <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2-messages#ingesting_hl7v2_messages">ingest a sample HL7 message</a> into our HL7 store. Save the below sample message in a file named ‘<strong>sample-hl7-msg.hl7</strong>’.</p><pre>MSH|^~\&amp;|FROM_APP|FROM_FACILITY|TO_APP|TO_FACILITY|20170703223000||ADT^A01|20170703223000|P|2.5|<br>EVN|A01|20210713083617|<br>PID|1||21004033^^^^MRN||SULIE^BRAN||19941208|M|||444 MAIN ST^^MOUNTAIN SPRINGS^CO^80444||1111111144|2222222244|<br>PV1||I|H44 RM4^1^^HIGHWAY 44 CLINIC||||5144^MARRIE QUINIE|||||||||Y||||||||||||||||||||||||||||20170703223000|</pre><p>The default segment separator in HL7v2 is a carriage return (\r). Most text editors use newline (\n) characters as segment separators. So we will use the below command to replace any \n with \r.</p><pre>sed -z &#39;s/\n/\r/g&#39; sample-hl7-msg.hl7 &gt; sample-hl7-msg-fixed.hl7</pre><p>HL7 store expects input messages to be in base64 encoded string format. So let us use the below command to encode the sample HL7 message.</p><pre>openssl base64 -A -in ./sample-hl7-msg-fixed.hl7 -out ./sample-hl7-msg-base64.txt</pre><p>Copy the encoded string from ‘<strong>sample-hl7-msg-base64.txt</strong>’ in the below format and save it in a file named ‘<strong>hl7v2-sample.json’</strong>.</p><pre>{<br>  &quot;message&quot;: {<br>    &quot;data&quot;: &quot;&lt;base64-encoded-string&gt;&quot;<br>  }<br>}</pre><p>We will run the below CURL command in a terminal to ingest this message to an HL7 store.</p><pre>curl -X POST      \<br>    -H &quot;Authorization: Bearer $(gcloud auth application-default print-access-token)&quot;      \<br>    -H &quot;Content-Type: application/json; charset=utf-8&quot;      \<br>    --data-binary @hl7v2-sample.json      \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages:ingest&quot;</pre><p>Once the command is successful, we will get a ‘<strong>message.name</strong>’ field in the response as shown below.</p><pre>{<br>  &quot;hl7Ack&quot;: &quot;&lt;base64-encoded-string&gt;&quot;,<br>  &quot;message&quot;: {<br>    &quot;name&quot;: &quot;&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;MESSAGE_ID&gt;&quot;,<br>    }<br>}</pre><p>Using the ‘<strong>message.name</strong>’ field we will next fetch the schematized message into an output json file. This file will act as an input for our whistle mappings.</p><pre>curl -X GET \<br>     -H &quot;Authorization: Bearer &quot;$(gcloud auth print-access-token) \<br>     -H &quot;Content-Type: application/json; charset=utf-8&quot; \<br>     &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;message-name&gt;&quot; \<br>     | jq &#39;.schematizedData.data | fromjson&#39; &gt; &lt;output-filename.json&gt;</pre><p><strong>Step 5</strong> — Let us open any IDE or terminal. We will run below gradle command to trigger mapping, in the directory where github repo was cloned.</p><pre>gradle :runtime:run -q --args=&quot;-m $HOME/wstl_codelab/codelab.wstl -i $HOME/wstl_cod<br>elab/&lt;output-filename.json&gt;&quot; &gt; converted-fhir.json</pre><blockquote>Explanation of the above <strong>gradle</strong> command:</blockquote><blockquote><strong>gradle</strong>: This invokes the Gradle build automation tool.<br><strong>:runtime:run</strong>: This tells Gradle to execute the run task to start the Whistle application.<br><strong>-q</strong>: This flag tells Gradle to run in “quiet” mode, suppressing most of the output except for errors.<br> <strong>— args</strong>: This introduces arguments that will be passed to the run task (and ultimately to the application it starts).<br><strong>-m $HOME/wstl_codelab/codelab.wstl</strong>: This argument specifies the path to a whistle file that the application will use for data mapping.<br><strong>-i $HOME/wstl_codelab/&lt;output-filename.json&gt;</strong>: This argument points to a JSON file containing input data for the Whistle mapping.</blockquote><p><strong>Sample Patient Whistle Mapping:</strong></p><blockquote>This code is just for demo purposes and does not represent the actual FHIR structure. It maps Patient fields like ‘<strong>identifier</strong>’, ‘<strong>name</strong>’ and ‘<strong>address</strong>’ from the PID segment in our input file. These mappings are structured into functions like ‘<strong>Build_Identifier</strong>’, ‘<strong>Build_Name</strong>’ and ‘<strong>Build_Address</strong>’ for better readability.</blockquote><pre>PID_Patient($root.ADT_A01.PID)<br><br>def PID_Patient(PID){<br>  identifier[]: Build_Identifier(PID.3[])<br>  name[]: Build_Name(PID.5[])<br>  address[]: Build_Address(PID.11[])<br>  active: true<br>  resourceType: &quot;Patient&quot;<br>}<br><br>def Build_Identifier(CX) {<br>  value: CX.1<br>}<br><br>def Build_Name(XPN) {<br>  family: XPN.1.1<br>  given[]: XPN.2<br>  given[]: XPN.3<br>}<br><br>def Build_Address(XAD) {<br>  line[]: XAD.2<br>  city: XAD.3<br>  state: XAD.4<br>  postalCode: XAD.5<br>}</pre><p><strong>Step 6</strong> — Once we have the mapped output, we can check if all the fields were converted as per our requirements. Once confirmed, we can try and load this to a FHIR store using the below command.</p><pre>curl -X POST \<br>    -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>    -H &quot;Content-Type: application/fhir+json&quot; \<br>    -d @converted-fhir.json \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/fhirStores/&lt;fhirstore-name&gt;/fhir/Patient&quot;</pre><p>Post successful completion of the above command, we should be able to see the record in our FHIR store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGeHlfu01LH9IC1oCEeN7Q.png" /></figure><h3>Conclusion</h3><p>By following the steps outlined above, we explored a method to validate the HL7 to FHIR conversion workflow utilizing the Open Source Whistle Data Mapping repository. This approach can be readily adapted to validate data conversion workflows involving any other data format to FHIR. This technique proves useful for conducting quick tests, proofs of concept (POCs), or pilot projects for healthcare data conversion to FHIR. Engaging with this process offers a deeper understanding of the capabilities of the powerful Whistle Data Mapping Language.</p><h3>Reference Links</h3><p><a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">Whistle github repo</a></p><p><a href="https://cloud.google.com/healthcare-api/docs">Cloud Healthcare API documentation</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd62c8613a92" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92">FHIR Whistle Data Mappings Validation</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dazbo’s Google Cloud Next ’24 Recap: Keynote]]></title>
            <link>https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f5518238c9d</guid>
            <category><![CDATA[ai-agent]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[wrap-up]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-next]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:05:10 GMT</pubDate>
            <atom:updated>2024-04-16T07:13:06.235Z</atom:updated>
            <content:encoded><![CDATA[<h3>Shall I? Shan’t I?</h3><p>It’s been a couple of days since Google Cloud Next ’24 wrapped up, and I’ve seen recaps appear on Medium already. So I ask myself: <em>“Should I bother this year?”</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*jvPtfzoKolvQBc2o.jpg" /><figcaption>To recap or not to recap…</figcaption></figure><p>I’ve decided “Yes” for two reasons…</p><ul><li>I’ve been doing recaps of these events for a few years, so I’d hate to break my streak! (Check out <a href="https://medium.com/google-cloud/google-next-2023-experience-and-favourite-sessions-fb00add5f59e">here</a>, and <a href="https://docs.google.com/presentation/d/1gfgijQjlQlvn6CEd29j5uVRXgynNFoBwMz2bUIXvydA/edit#slide=id.g27956d63397_0_230">here</a>.)</li><li>I find writing stuff down helps me learn and remember. So even if no one else finds this useful, I will!</li></ul><p>This year’s Google Cloud Next was in Las Vegas. Alas, this is the Next in the last few that I haven’t been able to attend in person. 😭 So, this wrap-up is based purely on watching the virtual content. And in case you weren’t aware, you can view all the recorded sessions at <a href="https://cloud.withgoogle.com/next">cloud.withgoogle.com/next</a>.</p><h3>Summary of Announcements</h3><p>I’ll update this list as a view more sessions.</p><ul><li>New investments in sub-sea cabling and data centres.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: A3 Mega VMs, powered by NVIDIA H100 Tensorcore GPUs. Twice as powerful has the previous iteration.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: GA of TPU v5p. Google’s most powerful TPU yet. These have 4x the compute capacity of the previous generation of TPUs.</li><li>Preview: Hyperdisk ML — next generation block storage optimised for AI workloads.</li><li>Vertex AI on <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>GKE Enterprise support for <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>AI Model support (including Gemma and Llama) on <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>Preview: <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805">Google Axion</a>. A custom ARM-based CPU. Claims 50% better performance and 60% more energy efficient than comparable current-gen x86 VMs! Google are migrating many services to Axiom.</li><li>Intel 5th Gen Xeon processors.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Gemini AI 1.5 Pro in Vertex AI</a>. Google’s multimodal foundational model. It can parse 1m tokens of information!</li><li>Gemini AI 1.5 Pro now integrated with Gemini Code Assist.</li><li>Supervised tuning for Gemini models.</li><li>Preview: Gemini Cloud Assist, which helps with the entire development lifecycle, including design and optimisation.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Grounding of Gemini models with Google Search</a>! This significantly reduces hallucination.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder">Vertex AI Agent Builder</a>: rapidly speed up the creation of multi-modal AI agents.</li><li><a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations">Google Vids</a> will be released to Workspace labs in June. This is an AI-powered collaborative video creation app, as part of Workspace.</li><li>Imagen 2.0 is now GA in Vertex AI. Google’s most advanced text-to-image model.</li><li>Public preview: Text-to-Live Image. This creates animated video-like images from a text prompt.</li><li>Public preview: Gemini in Looker.</li><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiant’s frontline threat intelligence using using natural language prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths and risks.</li></ul><h3>The Irony Isn’t Wasted On Me</h3><p>I’ve watched the keynote, and I’m summarising it here. Manually. Without AI.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/181/0*z3Pfn-gqrDOZ2lSc.gif" /></figure><h3>Opening Keynote: The New Way to Cloud</h3><p>You can see the full keynote <a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">here</a>.</p><h4>Keynote Quick Thoughts</h4><ul><li>It’s all about AI. Shocking.</li><li>The biggest announcements are around Gen AI capabilities.</li><li>I think the keynote mentioned AI agents 1,806,402 times. (Okay, I’m exaggerating slightly.)</li></ul><h4>Introduction</h4><blockquote>Google are at the forefront of the AI platform shift. More than 60% of funded Gen AI startups, and nearly 90% of Gen AI unicorns are Google Cloud customers.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5JXpAkiOWGrILB4z.jpg" /></figure><p>The keynote opens with an introductory video talking the power of AI today. (<em>“AI you say? I’m shocked. Shocked, I tell you!”</em>) The video talks about things we can do with AI now, like:</p><ul><li>Using satellites to reduce methane emissions.</li><li>Turning DNA into code to make… Crop-resistant corn!</li><li>Spoting and filling potholes.</li><li>Spoting diseases earlier.</li><li>Scanning 100K lines of code in 2 minutes, in order to spot and fix bugs.</li></ul><p>So this is <em>“The new way to Cloud.”</em></p><p>So far, so cool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1PHwomsMjmMnmhFTPJ62dw.png" /><figcaption>Google has announced partnerships with 100s of leading AI partners</figcaption></figure><p>The early keynote includes a brief introduction to some of the topics of this year’s Next:</p><ul><li>Over 300 customers and partners will be sharing their <strong>Gen AI success stories</strong> at this event.</li><li>Some discussion around the launch of <strong>Gemini </strong>and the advancements since its launch.</li><li><strong>Google Distributed Cloud and Edge</strong>, to support highly confidential and edge workloads.</li><li><strong>Cross-cloud networking</strong> now provides secure, low-latency connectivity of Google’s AI services to any application on any cloud.</li><li><strong>Chrome Enterprise Premium Browser</strong>.</li><li><strong>Multimodal Gen AI Agents</strong> will transform how we interact with the applications and the web. Agents are intelligent entities to do things like: customer agents, to help a shopper find the perfect dress; or helping an employee pick the right health benefits.</li></ul><h4>The AI Stack</h4><p>The keynote talks about <strong>Google’s AI stack</strong>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FxbiOgARL1a4-Q__qFFqfA.png" /><figcaption>Google’s AI stack</figcaption></figure><ul><li>Note the rebranding of Duet AI to “<a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-for-google-cloud-is-here"><strong>Gemini for Google Cloud</strong></a>”.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture"><strong>AI Hypercomputer</strong></a>: an integrated AI infrastructure platform for offering AI at scale. There are a number of announcements related to GPUs, TPUs, and AI-optimised storage.</li></ul><p>The keynote includes announcements around:</p><ul><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization/unlock-ai-anywhere-with-google-distributed-cloud?e=48754805"><strong>Google Distributed Cloud</strong></a>, which has a number of capability enhancements around GKE, Vertex AI, and AI model support. GDC now has both “secret” and “top secret” accreditations. Mobile operator “Orange” referenced as an organisation running across 26 countries and using GDC to keep data localised to each country.</li><li><a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805"><strong>Google Axion</strong></a>. A new custom ARM-based CPU that offers considerably higher performance and lowe energy consumption than caparable current gen x86.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates"><strong>Gemini 1.5 Pro in public preview</strong></a>. It has the world’s largest context window. In a single shot, it can process: 1M tokens, 1 hour of video, 11 hours of audio, and over 30K lines of code.</li><li><strong>Grounding of Gemini models with Google Search!</strong> This significantly reduces hallucination. Or you can ground with data from your own databases.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder"><strong>Vertex AI Agent Builder</strong></a> — to rapidly speed up creating AI Agents. Gemino Pro can create free-flowing conversations with text, voice, images and video as inputs. But also, it can even provide real time interactions in voice! Natural language can be used to train the AI agents, e.g. to describe topics that are verboten. You can configure transcription and summarisation. And response quality can be improved using vector search. Also, modular extensions can be integrated to complete standard customer workflows, e.g. booking a flight.</li></ul><h4>Shopping AI Agents</h4><p>The keynote then demonstrates a <strong>shopping AI Agent</strong>, and the ability to upload a video and ask it:</p><blockquote>Find me a checkered shirt like the keyboard player is wearing. I’d like to see prices, where to buy it, and how soon can I be wearing it?</blockquote><p>The response is near instantaneous on the website. And then we see a demo of interacting with a <em>voice </em>AI agent which continues the interaction and completes the transaction. That’s pretty cool!</p><h4>A Few Google Workspace Updates</h4><p>Then the keynote moves onto <strong>Gemini for Google Workspace</strong>. Use it to:</p><ul><li>Answer questions.</li><li>Create notes in meetings.</li><li>Extract insights from reports.</li><li>Create images to insert in presentations.</li><li>Real-time translation.</li></ul><p>Announcements related to <strong>Google Workspace</strong>:</p><ul><li>A recent benchmarking study shows <strong>Google Meet now outperforms Zoom and MS Teams</strong> for overall video performance.</li><li>Chat summarisation and real time translation now available for Google Meet.</li><li><strong>AI Security add-on</strong> can automatically classify and protect company data.</li><li><strong>Gemini in Google Chat</strong> can provide summaries of long conversations.</li></ul><p>We see a demo of reviewing proposals, comparing them, and asking questions, e.g.</p><blockquote>Does this offer comply with our compliance rule book?</blockquote><h4>Employee Agents</h4><p>Next, we talk about how to <strong>create a multi-modal AI employee agents using Vertex AI</strong>:</p><ul><li>Create a custom model with Vertex AI.</li><li>Connect the custom model to your company data and web data.</li><li>Ground in enterprise truth, e.g. with BigQuery and AlloyDB.</li></ul><p>Then we see a demo of how you can use a Vertex AI employee agent to summarise an employee benefits enrollment email, as well as a one hour benefits video. The agent is able to reason across text, video and the prompt, and provide a summary. Furthermore, the agent is able to compare the proposed plan to a previous plan, and make inferences.</p><h4>Creative AI Agents</h4><p>Now we move on to <strong>Creative AI Agents</strong>. Carrefore are using Creative AI Agents for marketing; they built a new marketing studio using Vertex AI, in just five weeks. Now they can build personalised campaigns in just a few clicks.</p><p>Creative agents uses Gemino Pro to look at existing material, documents and brand images, to infer a brand identity. We can generate multi-modal content; we can create live images, and even podcasts!</p><p>Then there was the announcement of <a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations"><strong>Google Vids</strong></a>, the AI-powered collaborative video creation app, as part of Google Workspace. Aparna then demos creating a recap video of the Next event, using Google Vids:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kZo1hMT6Ne-896WcxB3sGQ.png" /><figcaption>Creating a video recap in seconds, using Google Vids</figcaption></figure><p>Then we have announcements of <strong>Imagen 2.0 Text-to-Image</strong>, including new editing modes to edit a generated image. And there’s the new <strong>Text-to-Live Image</strong>, which is now in preview:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*K2OL48TfgPnzrPn6EXgqhw.gif" /><figcaption>Generating a live image from a prompt</figcaption></figure><h4>Data Agents</h4><p>So many agents!!</p><p>AI Data Agents us to ask natural language questions of our data. <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24"><strong>Gemini in BigQuery</strong></a> is now in Preview, and allows AI-powered data preparation, analysis and querying. BigQuery can be integrated directly with Vertex AI. So now we can perform multi-modal analysis across all of documents, images, videos, audio, and structured data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LlwikgKhm7Q-kjaSvSKqig.png" /><figcaption>Querying a data agent</figcaption></figure><p>One extremely cool thing about this demo was that the agent built a forecast dynamically, using BigQuery ML. And then uses vector embeddings to find products that look like a supplied image.</p><h4>Code Agents</h4><p>Surprise… More agents.</p><p>Google’s AI code assistant is now called <strong>Gemini Code Assist</strong>. (No more Duet AI.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/1*pizqI2uBk7FGn_0Y5TBAhg.png" /><figcaption>Benefits of using Code Assist</figcaption></figure><p>The keynote talks about how Gemini Code Assist can be used with a code base anywhere… On-prem, GitLab, GitHub, BitBucket, etc. Furthermore, Gemini Code Assist supports data residency requirements in multiple regions. It is now integrated with Gemini 1.5 Pro, and can leverage the new 1-million token context window.</p><p>The demo was cool… Show the visual mockup of a new UI to Gemini Code Assist, and it generates the code, leveraging our entire (huge) code base, and aligned to our code standards.</p><h4>Security Agents</h4><p>Please… No more agents!</p><p>These AI agents assist security operations teams, radically increasing the speed of security investigation and response.</p><p>There were a number of announcements relating to integration of Gemini into security products:</p><ul><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiant’s frontline threat intelligence using using natural language prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths and risks.</li></ul><h4>Wrap-Up</h4><p>Thomas Kurian wraps-up by saying:</p><blockquote>Our open platform offers choice at every layer.</blockquote><ul><li>Chips (CPUs, TPUs, GPUs) for training and serving.</li><li>Your choice of models.</li><li>Your choice of development environments.</li><li>Databases, including vector.</li><li>Your choice of business applications.</li></ul><blockquote>We’re creating a new era of generative AI agents, built on a new, truly open platform for AI. And we’re reinventing infrastructure to support it.</blockquote><h3>What’s Next?</h3><p>(See what I did there?)</p><p>I’ll watch a bunch of sessions I’m interested in, and provides some useful nuggets and summaries soon. I’ll put these in some separate articles, rather than just adding to this one.</p><h3>Links</h3><ul><li><a href="https://cloud.withgoogle.com/next">Google Cloud Next ‘24</a></li><li><a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">Keynote</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2024-wrap-up">All 218 things we announced at Google Cloud Next ‘24</a></li></ul><h3>Before You Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helps me!</li><li>Please give me claps! You know you clap more than once, right?</li><li>Feel free to <strong>leave a comment</strong> 💬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on these icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*73hF99AvDUGryMuV.png" /><figcaption>Follow and Subscribe</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f5518238c9d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d">Dazbo’s Google Cloud Next ’24 Recap: Keynote</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fine tuning Gemma with LoRA on GCP]]></title>
            <link>https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/5d25dbab9e0e</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[finetune-llm]]></category>
            <category><![CDATA[lora]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gemma]]></category>
            <dc:creator><![CDATA[pritam sahoo]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:52 GMT</pubDate>
            <atom:updated>2024-04-16T00:04:52.701Z</atom:updated>
            <content:encoded><![CDATA[<p>My obsession with Gemma continues. Folks new to the Gemma model can revisit my previous blog <a href="https://medium.com/google-cloud/gemma-open-models-from-google-0045263e53d2">link</a>.</p><p>In brief Gemma is the family of lightweight, state of the art (SOTA) open models powered by the same technology powering one of the most popular Google Cloud Gemini models.</p><p>In this blog we will get started with fine tuning with Gemma with LoRA.</p><p>Lets understand first a bit on fine tuning. One of the reasons finetuning is picking up is the reason Large language Models(LLMs) are not trained on specific tasks or domain related data. Primarily LLMs often called as foundational models are trained on internet scale massive corpus of data, texts etc. Doing a full training of pre-trained LLM models becomes technically challenging due to expensive computational resources as one of the major concerns.</p><p>Let’s understand the benefits of Fine tuning.</p><ol><li>Fine Tuning pre-trained model is much faster and cost effective leading to less computational resources required.</li><li>Better Performances for domain specific tasks especially on industry use cases related to Financial services, Insurance , Healthcare etc.</li><li>Lets not forget about democratization of GenAI models for individual users i.e. developers and others who have less computational power.</li></ol><p>Lets understand Parameter efficient fine tuning <a href="http://a.ka">a.k.a</a>. PEFT. It’s a subset of fine tuning which effectively utilizes parameters/weights with efficient output. Instead of altering all the parameters of the model PEFT selects a subset of them thereby reducing computational and memory requirements. PEFT plays a major role in the fine tuning process thereby improving the performance of base/foundational LLMs on specific tasks. This is super useful when training LLM models like Gemini and its different variants, PALM,even open source Gemma models etc from Google.</p><p>We will explore fine tuning Gemma Models with <strong>LoRA</strong>. <strong>LoRA</strong> stands for Low Rank Adaptation of Large Language Models. It’s a technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights/parameters of the base model and introducing a small number of new weights into the model.</p><p><strong>Crucial Point to consider</strong> In LoRA, the starting point hypothesis is super important . It assumes that the pre-trained model’s weights are already close to the optimal solution for the downstream tasks.</p><p>Advantages of using LoRA as fine tuning technique</p><ol><li>Reduces Parameter and memory footprint. LoRA significantly reduces the number of trainable parameters, making it much more memory-efficient and computationally cheaper.</li><li>Fine tuning and so does inference is faster ~ as it uses less parameters/weights.</li><li>Maintains performance: LoRA has been proved to maintain performance close to traditional fine-tuning methods in several tasks.</li></ol><p><strong>So let’s get started with Fine tuning with LoRA on the Gemma Model.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UkJdQkyIoYu3I-OH3bJYoA.jpeg" /></figure><p>For this demo I will be using Google Collab Notebook to get some horsepower with T4 GPUs.</p><p><strong>Step 1: Get access to Gemma</strong></p><p>To complete this collab, you will first need to complete the setup instructions at <a href="https://ai.google.dev/gemma/docs/setup">Gemma setup</a>. The Gemma setup instructions show you how to do the following:</p><ul><li>Get access to Gemma on <a href="https://kaggle.com/">kaggle.com</a>.</li><li>Select a Colab runtime with sufficient resources to run the Gemma 2B model.</li><li>Generate and configure a Kaggle username and API key.</li></ul><p>After you’ve completed the Gemma setup, move on to the next section, where you’ll set environment variables for your Colab environment.</p><p><strong>Step 2 : Select the Runtime</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/468/0*6QanOb6XI087IwsU" /></figure><h4><strong>Step 3 : Configure your secrets i.e. username and key in Account tab</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/834/0*m_Kg4FsICCsvEH73" /></figure><p><strong>Step 4 : Select the Data for fine tuning from hugging face. </strong><a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k"><strong>Databricks Dolly 15k dataset</strong></a><strong>. </strong>This dataset contains 15,000 high-quality human-generated prompt / response pairs specifically designed for fine-tuning LLMs. Brief screenshot of the datasets</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/997/0*wmgP3pZMciNG5Ix5" /></figure><p><strong>Step 5 : Set the environment variables and run the below commands in Collab</strong></p><p>import os</p><p>from google.colab import userdata</p><p>os.environ[“KAGGLE_USERNAME”] = userdata.get(‘username’)</p><p>os.environ[“KAGGLE_KEY”] = userdata.get(‘key’)</p><p><strong>Step 6 : Install the dependencies</strong></p><p>!pip install -q -U keras-nlp</p><p>!pip install -q -U keras&gt;=3</p><p><strong>Step 7 : Select the backend. You may choose from PyTorch or Tensorflow or Jax</strong></p><p>os.environ[“KERAS_BACKEND”] = “jax”.</p><p># Avoid memory fragmentation on JAX backend.</p><p>os.environ[“XLA_PYTHON_CLIENT_MEM_FRACTION”]=”1.00&quot;</p><p><strong>Step 8 : Import Packages i.e. Keras and KerasNLP.</strong></p><p>import keras</p><p>import keras_nlp</p><p><strong>Step 9 : Load the dataset from hugging face.</strong></p><p>!wget -O databricks-dolly-15k.jsonl <a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl">https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl</a></p><p><strong>Step 10 : For this demo purpose I will be using a subset of 1000 examples instead of 15K examples. For better fine tuning you may use more examples.</strong></p><p>import json</p><p>data = []</p><p>with open(“databricks-dolly-15k.jsonl”) as file:</p><p>for line in file:</p><p>features = json.loads(line)</p><p># Filter out examples with context, to keep it simple.</p><p>if features[“context”]:</p><p>continue</p><p># Format the entire example as a single string.</p><p>template = “Instruction:\n{instruction}\n\nResponse:\n{response}”</p><p>data.append(template.format(**features))</p><p># Only use 1000 training examples, to keep it fast.</p><p>data = data[:1000]</p><p><strong>Step 11 : Now its time to Load the Gemma 2B base Model. You may try using the Gemma 7B base model.</strong></p><p>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(“gemma_2b_en”)</p><p>gemma_lm.summary()</p><p>You will see below summary output if everything is working fine.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/0*df55cDERngDI3ig_" /></figure><p><strong>Step 11: Lets Inference the Model before fine tuning.</strong></p><p>Pass the below prompt i.e. “ What should I do on a trip to Europe?”</p><p>prompt = template.format(</p><p>instruction=”What should I do on a trip to Europe?”,</p><p>response=””,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>You will see very generic blant and not so great output from the base model as mentioned below</strong></p><p>— — — — — — — — — — — — — — — — — — — — —</p><p><strong>Instruction:</strong></p><p><strong>What should I do on a trip to Europe?</strong></p><p><strong>Response:</strong></p><p><strong>It’s easy, you just need to follow these steps:</strong></p><p><strong>First you must book your trip with a travel agency.</strong></p><p><strong>Then you must choose a country and a city.</strong></p><p><strong>Next you must choose your hotel, your flight, and your travel insurance</strong></p><p><strong>And last you must pack for your trip.</strong></p><p><strong>— — — — — — — — — — — — — — —</strong></p><p><strong>Step 12: Lets fine tuning using LoRA using Databricks Dolly 15K dataset.</strong></p><p>LoRA rank. It controls the expressiveness and precision of the fine-tuning adjustments.Lower rank means which requirement of computational power and also less precision adaptation. You may start with 4,8 etc for demo/experimentation purposes.</p><p>&gt;&gt; gemma_lm.backbone.enable_lora(rank=4)</p><p>&gt;&gt; gemma_lm.summary()</p><p><strong>Total params: 2,507,536,384 (9.34 GB)</strong></p><p><strong>Trainable params: 1,363,968 (5.20 MB)</strong></p><p><strong>Non-trainable params: 2,506,172,416 (9.34 GB)</strong></p><p><strong>While you run the below section in the collab notebook be patient as it will take some time and you will see reduction in losses.This step will reduce the number of trainable parameters significantly.Epoch = 1 means it will run for 1 time for 1000 datasets.</strong></p><p>gemma_lm.preprocessor.sequence_length = 512</p><p>optimizer = keras.optimizers.AdamW( // AdamW ~ optimizer for transformer models</p><p>learning_rate=5e-5,</p><p>weight_decay=0.01,</p><p>)</p><p>optimizer.exclude_from_weight_decay(var_names=[“bias”, “scale”])</p><p>gemma_lm.compile(</p><p>loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),</p><p>optimizer=optimizer,</p><p>weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],</p><p>)</p><p>gemma_lm.fit(data, epochs=1, batch_size=1)</p><p><strong>The output from the above step will show significant reduction in loss with just 1000 datasets.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qf-k1hlFro2K4DUh" /></figure><p><strong>Step 13: Let’s get started with Inferencing post fine tuning.</strong></p><p>Pass the below prompt again i.e. “ What should I do on a trip to Europe?”</p><p>prompt = template.format(</p><p>instruction=”What should I do on a trip to Europe?”,</p><p>response=””,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>**** Let me know the results. Must be better than before finetuning.</strong></p><p>Thats’ it folks on Gemma fine tuning with LoRA. Stay tuned for more updates coming your way on QLoRA……..</p><p><strong>Signing off…. Pritam</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d25dbab9e0e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e">Fine tuning Gemma with LoRA on GCP</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use Log Analytics for BigQuery Usage Analysis on Google Cloud]]></title>
            <link>https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f5454626c6c</guid>
            <category><![CDATA[log-analytics]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[observability]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[logs]]></category>
            <dc:creator><![CDATA[Xiang Shen]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:36 GMT</pubDate>
            <atom:updated>2024-04-16T12:18:26.756Z</atom:updated>
            <cc:license>https://creativecommons.org/publicdomain/mark/1.0/</cc:license>
            <content:encoded><![CDATA[<p>On Google Cloud you can use <a href="https://cloud.google.com/logging/docs/log-analytics#analytics">Log Analytics</a> to query and analyze your log data, and then you can view or <a href="https://cloud.google.com/logging/docs/analyze/charts">chart the query results</a>.</p><p><a href="https://cloud.google.com/bigquery">BigQuery</a> is Google Cloud’s fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence.</p><p>While BigQuery offers built-in observability capabilities like the <a href="https://cloud.google.com/bigquery/docs/information-schema-intro">INFORMATION_SCHEMA</a> views, detailed logging remains crucial for in-depth usage analysis, auditing, and troubleshooting potential issues.</p><p>This article will walk you through how to analyze BigQuery logs using log analytics.</p><h3>Upgrade Log bucket</h3><p>First, if you haven’t, you need to configure Cloud Logging to upgrade all the existing log buckets with Log Analytics enabled.</p><p>To upgrade an existing bucket to use Log Analytics, do the following:</p><ol><li>In the navigation panel of the Google Cloud console, select <strong>Logging</strong>, and then select <strong>Logs Storage.</strong></li><li>Locate the bucket that you want to upgrade.</li><li>When the <strong>Log Analytics available</strong> column displays <strong>Upgrade</strong>, you can upgrade the log bucket to use Log Analytics. Click <strong>Upgrade</strong>.<br>A dialog opens. Click <strong>Confirm</strong>.</li></ol><h3>Perform BigQuery Activities</h3><p>Complete the following tasks to generate some BigQuery logs. In the tasks, the BigQuery command line tool <a href="https://cloud.google.com/bigquery/docs/reference/bq-cli-reference">bq</a> is used.</p><p><strong>Task 1. Create datasets</strong></p><p>Use the <strong>bq mk</strong> command to create new datasets named <strong>bq_logs</strong> and <strong>bq_logs_test </strong>in your project:</p><pre>bq mk bq_logs<br>bq mk bq_logs_testbq mk bq_logs_test</pre><p><strong>Task 2. List the datasets</strong></p><p>Use the <strong>bq ls</strong> command to list the datasets:</p><pre>bq ls</pre><p><strong>Task 3. Delete a dataset</strong></p><p>Use the <strong>bq rm</strong> command to delete the a dataset (select <strong>y</strong> when prompted):</p><pre>bq rm bq_logs_test</pre><p><strong>Task 4. Create a new table</strong></p><pre>bq mk \<br> --table \<br> --expiration 3600 \<br> --description &quot;This is a test table&quot; \<br> bq_logs.test_table \<br> id:STRING,name:STRING,address:STRING</pre><p>You should have a new empty table named <strong>test_table</strong> that has been created for your dataset.</p><p><strong>Task 5. Run some example queries</strong></p><p>You can run a simple query like the following to generates a log entry. Copy and paste the following query into the BigQuery Query editor:</p><pre>bq query — use_legacy_sql=false ‘SELECT current_date’</pre><p>The following query will leverage weather data from the <a href="https://cloud.google.com/blog/products/data-analytics/noaa-datasets-on-google-cloud-for-environmental-exploration">National Oceanic and Atmospheric Administration (NOAA)</a>. Copy the query into the BigQuery editor and click <strong>RUN</strong>.</p><pre>bq query --use_legacy_sql=false \<br>&#39;SELECT<br> gsod2021.date,<br> stations.usaf,<br> stations.wban,<br> stations.name,<br> stations.country,<br> stations.state,<br> stations.lat,<br> stations.lon,<br> stations.elev,<br> gsod2021.temp,<br> gsod2021.max,<br> gsod2021.min,<br> gsod2021.mxpsd,<br> gsod2021.gust,<br> gsod2021.fog,<br> gsod2021.hail<br>FROM<br> `bigquery-public-data.noaa_gsod.gsod2021` gsod2021<br>INNER JOIN<br> `bigquery-public-data.noaa_gsod.stations` stations<br>ON<br> gsod2021.stn = stations.usaf<br> AND gsod2021.wban = stations.wban<br>WHERE<br> stations.country = &quot;US&quot;<br> AND gsod2021.date = &quot;2021-12-15&quot;<br> AND stations.state IS NOT NULL<br> AND gsod2021.max != 9999.9<br>ORDER BY<br> gsod2021.min;&#39;</pre><h3>Perform log analysis</h3><p>Now there are some log entries for BigQuery. You can run some queries using Log Analytics.</p><p><strong>Task 1. Open Log Analytics</strong></p><p>On the left side, under <strong>Logging</strong> click <strong>Log Analytics</strong> to access the feature. You should see something like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6PrYdj0eZePE3wF8" /></figure><p>If your query field is empty or you forget which table you want to use, you can click the <strong>Query</strong> button to get the sample query back.</p><p>Now you can run your own queries in the query field. Remember to replace <strong>[Your Project Id]</strong> with the project id you are using.</p><p><strong>Task 2. To find the activities for BigQuery datasets</strong></p><p>You can query the activities that a dataset is created or deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;datasetservice%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QgplyJgYAw8L93Rn" /></figure><p><strong>Task 3. To find the activities for BigQuery tables</strong></p><p>You can query the activities that a dataset is created or deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;%TableService%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*REyZPbwK41Vfotxk" /></figure><p><strong>Task 4. To view the queries completed in BigQuery</strong></p><p>Run the following query:</p><pre>SELECT<br> timestamp,<br> resource.labels.project_id,<br> proto_payload.audit_log.authentication_info.principal_email,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.query) AS query,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.statementType) AS statementType,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatus.error.message) AS message,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS startTime,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS endTime,<br> CAST(TIMESTAMP_DIFF( CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS TIMESTAMP), CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS TIMESTAMP), MILLISECOND)/1000 AS INT64) AS run_seconds,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalProcessedBytes) AS INT64) AS totalProcessedBytes,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalSlotMs) AS INT64) AS totalSlotMs,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.referencedTables) AS tables_ref,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalTablesProcessed) AS INT64) AS totalTablesProcessed,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.queryOutputRowCount) AS INT64) AS queryOutputRowCount,<br> severity<br>FROM<br> `[Your Project Id].global._Default._Default`<br>WHERE<br> log_id = &quot;cloudaudit.googleapis.com/data_access&quot;<br> AND proto_payload.audit_log.service_data.jobCompletedEvent IS NOT NULL<br>ORDER BY<br> startTime</pre><p>After the query completes, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yIhioPqh3mZwXguM" /></figure><p>Scroll through the results of the executed queries.</p><p><strong>Task 5. To chart the query result</strong></p><p>Instead of using a table to see the results, Log Analytics also supports creating charts for visualization. For example, to view a pie chart for the queries that have run, you can click the <strong>Chart</strong> button in the result view, select <strong>Pie chart</strong> as the chart type and <strong>query</strong> as the column. You should see a chart similar to the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HoiWqDgj6gqLAp58" /></figure><p>We’ve only scratched the surface of BigQuery log analysis; you can explore many other queries and charts to enhance your understanding of BigQuery. Feel free to contribute and create samples in <a href="https://github.com/GoogleCloudPlatform/observability-analytics-samples/tree/main/samples/logging">GCP’s sample GitHub repository</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f5454626c6c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c">Use Log Analytics for BigQuery Usage Analysis on Google Cloud</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Landing Zone Technical Onboarding— the “How-To” (Google Cloud Adoption Series)]]></title>
            <link>https://medium.com/google-cloud/landing-zone-technical-onboarding-the-how-to-google-cloud-adoption-series-9b7ba8710e83?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/9b7ba8710e83</guid>
            <category><![CDATA[technical-design]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-partner]]></category>
            <category><![CDATA[landingzone]]></category>
            <category><![CDATA[project-team]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Mon, 15 Apr 2024 08:06:46 GMT</pubDate>
            <atom:updated>2024-04-15T13:45:31.192Z</atom:updated>
            <content:encoded><![CDATA[<p>Welcome to the latest installment in the <a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Google Cloud Adoption and Migration: From Strategy to Operation</a> series.</p><p>Previously, we covered the various considerations that are part of LZ design. In this part we’ll cover:</p><ul><li>Establishing your <strong>LZ Core Project Team</strong>.</li><li>Establishing the <strong>support </strong>you need.</li><li><strong>Workshopping </strong>the design considerations.</li><li>Agreeing and documenting your <strong>LZ design decisions</strong> in the <strong>LZ Design Document</strong>.</li><li><strong>Deploy!</strong></li></ul><h3>Technical Onboarding?</h3><p>Google refers to the overall process of setting up a Cloud environment as <em>“Technical Onboarding”</em>. Historically it was referred to as <em>“Cloud Foundation”</em>. It includes:</p><ol><li>Kick-off, scoping and planning</li><li>Understanding organisational capability needs</li><li>Workshopping</li><li>Implementation (deployment)</li></ol><p>But before you can execute any steps, you’ll need a core project team.</p><h3>Establishing Your LZ Project Team</h3><p>If you’re working through the process of designing your LZ, then it’s probably fair to say that your maturity with Google Cloud is quite low. If you were mature in Google Cloud, you’d already have your well-designed LZ! So the first thing you’ll need to do is <strong>establish the core LZ project team</strong>, who will be responsible for:</p><ul><li>Working through the various LZ design considerations that I have outlined in the previous few parts.</li><li>Capturing your LZ design decisions.</li><li>Deploying your LZ.</li></ul><p><strong>What should this LZ core team look like?</strong> My recommendations are that you include the following:</p><ul><li>A <strong>lead cloud architect</strong>. This needs to be someone who fully understands cloud, and your organisation’s cloud strategy. Possibly, they were the enterprise cloud architect who created the organisation’s cloud strategy in the first place. This person will ultimately own the technical deliverables, and have the final say on the design decisions.</li><li><strong>A project manager.</strong> For obvious reasons!</li><li>A <strong>Platform / DevOps Lead</strong>. This is someone who has a strong understanding of Google Cloud, DevOps, and Terraform IaC. This person will ultimately be responsible for deployment of the LZ. They are likely also a key member of the <strong>Cloud Platform Team</strong>, which <em>may not yet formally exist at this point in the journey. </em>(And it is likely that this person will be a pivotal member of the new Cloud Platform Team.)</li><li>An <strong>SRE Lead</strong>. This person will be concerned with observability and embedding <a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE best practice</a>. They are likely a key member of the <strong>SRE Team</strong>, <em>which may not yet formally exist.</em></li><li>A <strong>network architect</strong>. Someone who understands your existing network topology, switching and routing, and your network strategy. This person will be well placed to describe the options and constraints, particularly when agreeing hybrid connectivity, DNS options, etc.</li><li>A <strong>security architect</strong>. Someone who understands the organisational security requirements, as well as existing capabilities and use cases for technologies like firewalls, IDS/IPS, WAF, and proxies. They will also be aware of the security strategy.</li><li><strong>Google Cloud SMEs</strong>. You need a some Google Cloud specialists who collectively have strong knowledge and experience of Google Cloud LZs, and of all the products and services that are fundamental the LZ, such as IAM, networking, GKE, monitoring and alerting, etc.</li></ul><p>Which brings us on to…</p><h3>Support</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iwfCzdjGNwy9VRke" /></figure><p>Again: if you’re in the process of designing and building your LZ, it’s quite likely that your organisation doesn’t yet have the necessary expertise and experience to design and deploy the LZ. You’re going to need some help.</p><p>For this, I would recommend engaging with Google, or with a <a href="https://cloud.google.com/find-a-partner/">Google Cloud Partner</a> that offers a <strong>Landing Zone design and build service</strong>. During the LZ design and build phase, you can supplement your internal team with all the expertise (and experience) that you need. For example, the Google Cloud partner can supply you with:</p><ul><li>A<strong> Google Cloud Architect</strong> who is an expert in Google Cloud and LZ design.</li><li><strong>Additional experts</strong> who can handle any specific queries that the architect doesn’t have the answers to. These additional resources could be deployed into your core team, or simply be available as “background resources” that your partner can tap into.</li><li><strong>DevOps engineering capability</strong>, to either build and deploy your IaC, or to support and guide your own Platform Team.</li></ul><p>Additionally, such a partner can assist you to:</p><ul><li>Build your <a href="https://medium.com/@derailed.dash/google-cloud-adoption-organisational-change-capabilities-upskilling-and-cloud-centre-of-15bc49ae7ae6">CCoE capability</a>.</li><li>Build your Platform Team.</li><li>Build your <a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE capability</a>.</li><li>Execute an initial migration PoC.</li><li>Help you build an application migration factory team.</li></ul><p>The partner can advise on organisational structure, provide resource augmentation until your organisation is self sufficient, and help upskill your internal staff.</p><p>It would be remiss of me not to mention <a href="https://www.epam.com/?gad_source=1&amp;gclid=Cj0KCQjwlN6wBhCcARIsAKZvD5h7jf_UxQE04IrMPl5G9rxYkX2YhMjUgUVU_jPCnO9AWjz9fnPgXLkaAiX0EALw_wcB"><strong>EPAM</strong></a><strong> </strong>at this stage, because a) they are a <strong>Google Cloud Premier Partner</strong> that offers such a service; and b) I happen to work for them!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*az-PumqTxzgAcYpE.png" /><figcaption>EPAM — A Google premier partner</figcaption></figure><p>They are multinational, with around 50000 engineers and consultants. They have over 1200 Google Cloud certified experts, and they have won <strong>Google Cloud Partner of the Year</strong> in 2018, 2023 and 2024!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/868/1*gv58BYiOq362nTAF4flxgQ.png" /><figcaption>EPAM’s credentials</figcaption></figure><p>I’ve worked with a lot of consultancies over my career (though typically, as the client), and I can honestly say: <strong>EPAM are the gold standard</strong>.</p><p>If you do want to engage with EPAM, you can reach out through the link above, or you can <a href="https://github.com/derailed-dash">connect with me directly</a>.</p><h3>Workshops</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RejtZ3_J4Zzn-rtG.jpg" /><figcaption>Tried-and-Tested</figcaption></figure><p>Now you’ve got your project team and engaged with the support you need, it’s time to crack on with some workshops!</p><p><strong>Google’s tried-and-tested approach is to hold a series of workshops</strong>, aligned to the various LZ design considerations that I’ve previously covered. (Google Cloud partners will typically use a similar approach.) I would recommend a 2 hour workshop for each of the following topics:</p><ul><li><strong>Initiation:</strong> LZ goals; current state; overall project scope; confirming roles and responsibilities; ways-of-working.</li><li><strong>Identity and Access Management:</strong> IAM; roles; master IdP decisions; Google Workspace superadmins; Google Cloud org admins; other groups; IdP integration; SSO and MFA.</li><li><strong>Resource Hierarchy and Management:</strong> org and folder structure; org policies; tenants and project factory principles; environments and sandboxes.</li><li><strong>Network and Security: </strong>hybrid connectivity; shared VPC topology; VPC-SC; firewall; ingress and egress (including Internet connectivity) patterns; current network/appliance considerations; DDoS and WAF (e.g. with Cloud Armor); DNS; DR and region considerations; org policies revisit.</li><li><strong>Compute and GKE:</strong> GKE strategy; multitenant cluster design; fleet design; GKE address ranges; release channels; Workload Identity Federation; Anthos service mesh; IaaS OS management and upgrade/patching strategy.</li><li><strong>Operations and Visibility: </strong>monitoring, logging and alerting; predefined and custom dashboards; metrics, including GKE and Istio metrics, and Ops Agent; metrics scope (aggregation and isolation) design; SIEM considerations; audit logging; network service logging; logging aggegration and organisational log sinks; log archiving and exports; integration with other operations software (e.g. on-prem); SRE considerations.</li><li><strong>Billing and Cost Optimisation: </strong>billing roles; tenant/project cost visibility; billing exports; project budgets; sandbox budgets; budget alerts; labelling strategy and standards.</li><li><strong>Automation, GitOps and Foundation Enablement:</strong> IaC, GitOps and CI/CD; LZ IaC frameworks and accelerators (e.g. Google Fabric FAST and Google CFT); IaC policy enforcement; project factory; tenant onboarding processes and support. (I’m going to cover tenant enablement later in the series.)</li></ul><p>So, that’s eight workshops with the core LZ team, and you can bring in specialists as required.</p><h3>Documenting Your LZ Design</h3><p>As you progress through the workshops, document the design decisions as you go. Capture your overall design and decisions in an artefact that Google calls the <strong>LZ Technical Design Document (TDD)</strong>.</p><p>This doc should include:</p><ul><li>Introduction and scope of the solution.</li><li>A summary of all design decisions. These should point to the relevant sections in the document.</li><li>Sections corresponding to each of the workshops.</li></ul><h3>Deploy!</h3><p>I’m going to cover this in the next installment!</p><h3>Before You Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helps me!</li><li>Please <strong>give me claps</strong>! You know you clap more than once, right?</li><li>Feel free to <strong>leave a comment</strong> 💬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on these icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*fF62z2-FT03ui0O5.png" /><figcaption>Follow and Subscribe</figcaption></figure><h3>Links</h3><ul><li><a href="https://medium.com/google-cloud/landing-zones-on-google-cloud-b42b08e1abaa">Landing Zones on Google Cloud: What It Is, Why You Need One, and How to Create One</a></li><li><a href="https://cloud.google.com/find-a-partner/">Google Cloud Partners</a></li><li><a href="https://www.epam.com/?gad_source=1&amp;gclid=Cj0KCQjwlN6wBhCcARIsAKZvD5h7jf_UxQE04IrMPl5G9rxYkX2YhMjUgUVU_jPCnO9AWjz9fnPgXLkaAiX0EALw_wcB">EPAM</a></li><li><a href="https://cloud.google.com/architecture/framework">Google Cloud Architecture Framework</a></li><li><a href="https://cloud.google.com/architecture/security-foundations">Enterprise Foundations Blueprint</a></li></ul><h3>Series Navigation</h3><ul><li><a href="https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc">Series overview and structure</a></li><li><a href="https://medium.com/@derailed.dash/google-cloud-adoption-organisational-change-capabilities-upskilling-and-cloud-centre-of-15bc49ae7ae6">Org change, upskilling and CCoE Establishment</a></li><li><a href="https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b">SRE best practice</a></li><li>Previous: <a href="https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-4-iac-gitops-and-ci-cd-google-cloud-ae3f533c6dbd">Design your Landing Zone — Design Considerations Part 4: IaC, GitOps and CI/CD</a></li><li>Next: Technical Onboarding and Landing Zone Deployment</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9b7ba8710e83" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/landing-zone-technical-onboarding-the-how-to-google-cloud-adoption-series-9b7ba8710e83">Landing Zone Technical Onboarding— the “How-To” (Google Cloud Adoption Series)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>