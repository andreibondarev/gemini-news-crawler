<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Google Cloud - Community - Medium]]></title>
        <description><![CDATA[A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don&#39;t necessarily reflect those of Google. - Medium]]></description>
        <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Google Cloud - Community - Medium</title>
            <link>https://medium.com/google-cloud?source=rss----e52cf94d98af---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 19 Apr 2024 16:58:06 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/google-cloud" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Multi-region HA in Google Cloud]]></title>
            <link>https://medium.com/google-cloud/multi-region-ha-in-google-cloud-823b9f706578?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/823b9f706578</guid>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Sergey Shcherbakov]]></dc:creator>
            <pubDate>Fri, 19 Apr 2024 05:15:57 GMT</pubDate>
            <atom:updated>2024-04-19T05:15:57.347Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*y-wMkHFcpMgOJtYixfAy_w.png" /></figure><p>Google Cloud is one of the remarkable cloud “hyperscalers”. Hyperscalers are designed for massive capacity. They possess immense data center networks spread globally, allowing them to handle the enormous computing demands of large enterprises and applications with vast user bases. With that, Hyperscalers can enable applications with unsurpassed capabilities in scalability, reliability and global reach.</p><p>In this article we will try to explore the levels of possible application availability in Google Cloud with a focus on private internal networks. We’ll also provide actual infrastructure configuration examples.</p><p>Let’s imagine a business critical web application or API that provides its important service to the, potentially internal, business customers or end users. Often the business needs require the application to minimize its downtime, make it accessible to the users and responsive most of the time. A common measure of success of such metric is the application service uptime metric often aiming for targets like “99.99%” (“four nines”) or even “99.999%” (“five nines”) which translate into very few minutes of allowed downtime per year.</p><p>The typical mechanisms that the application design can rely upon to improve application Availability (as measured by uptime) are</p><ul><li><strong>Redundancy</strong> — run application on multiple independent hardware instances</li><li><strong>Load Balancing </strong>— distribute incoming network traffic across multiple application instances running on multiple independent hardware instances</li><li><strong>Failover</strong> — mechanisms to automatically detect failures and switch operation to a working application instance seamlessly</li><li><strong>Monitoring &amp; Alerting</strong> — robust monitoring systems to detect problems quickly and preferably proactively notify the team responsible for addressing them</li><li><strong>Self-healing —</strong> ability of the application components restart themselves or re-provision failing resources with minimal manual intervention</li></ul><p>In this article we will concentrate on how Google Cloud can help with the first three means of improving cloud application availability: redundancy, load balancing, failover.</p><h3><strong>Redundancy</strong></h3><p>A single application instance or application running in a single failure domain cannot sustain underlying hardware failure and hence the application would not be available to the end users in case of an underlying hardware outage:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mW4jv8zKfyr1rRv9xf-t7g.png" /><figcaption>Fig. 1: Single application instance on single GCE VM</figcaption></figure><p>If our business objectives require addressing only a single Google Compute Engine (GCE) VM outage we would need to apply <strong>Redundancy</strong> and <strong>Load Balancing</strong> in order to improve application availability and resilience to that failure scenario:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DpDuW6cKQnoVzTmeuk35Vw.png" /><figcaption>Fig. 2: Multiple application instances in single GCE Zone</figcaption></figure><p>This setup is addressing the single GCE VM or application instance outage failure scenario.</p><p>Google Cloud hardware is organized into <a href="https://cloud.google.com/compute/docs/regions-zones/zone-virtualization"><em>clusters</em></a>. A cluster represents a set of compute, network, and storage resources supported by building, power, and cooling infrastructure. Infrastructure components typically support a single cluster, ensuring that clusters share few dependencies. However, components with highly demonstrated reliability and downstream redundancy can be shared between clusters. For example, multiple clusters typically share a utility grid substation because substations are extremely reliable and clusters use redundant power systems.</p><p>A <a href="https://cloud.google.com/compute/docs/regions-zones"><em>zone</em></a> is a deployment area within a region and Compute Engine implements a layer of abstraction between zones and the physical clusters where the zones are hosted. Each zone is hosted in one or more clusters and you can check the <a href="https://cloud.google.com/compute/docs/regions-zones/zone-virtualization">Zone virtualization</a> article for more details about that mapping.</p><p>To simplify reasoning without sacrificing accuracy it would be fair to assume that a GCE zone is a deployment area within a geographic region mapped to one or more clusters that can fail together, e.g. because of the power supply outage.</p><p>GCE zone outage is <a href="https://status.cloud.google.com/summary">not an impossible scenario</a> and a highly reliable application on Google Cloud typically seeks to sustain its service during such unfortunate event by running application replicas in multiple zones:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BGaUrlTA6NCfc0RbYaClkQ.png" /><figcaption>Fig. 3: Multiple application instances in multiple GCE Zones</figcaption></figure><p>With high zone availability <a href="https://cloud.google.com/compute/sla">SLA levels</a> provided by the Google Cloud Compute engine, the application setup in Figure 3 should be sufficient for majority of business use cases even for very demanding customers requiring high application service SLA levels.</p><p>Unfortunately, a full region outage is also <a href="https://www.businessinsider.com/google-cloud-data-center-london-outage-hottest-day-record-uk-2022-7">not an impossible scenario</a>.</p><p>The power of cloud hyperscalers is especially in that they provide customers with significantly better tools to survive disasters similar to <a href="https://www.reuters.com/article/idUSKBN2B20NT/">this one</a>, for example, than other cloud providers. Amongst other things, that is what differentiates “Hyperscalers” from small-scale or localized cloud service providers. In Google Cloud an application can run its replicas not only on power independent hardware within one data center or geographic location (probably connected to the same power plant in the neighborhood) but also across geographic location and even across continents!</p><p>So we are coming to the next level of application redundancy that is possible with Google Cloud: multi-regional application deployment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jE7J83TUsHoguxvrK5OOMg.png" /><figcaption>Fig. 4: Multiple application instances in multiple GCE Regions with global external load balancing</figcaption></figure><p>With that, a business critical application can now have a strategy for the entire site (region) failure and promise uptime to its critical clients even in that unlikely case.</p><h3>Load Balancing</h3><p>There needs to be some magic happening in order to seamlessly direct clients from around the world to the application instance replicas running in multiple geographic locations. And not only that. Whenever a VM, GCE zone or even full region goes down that magic needs to seamlessly redirect application clients to the healthy locations in other surviving region.</p><p>What are the options for load balancing that Google Cloud provides?</p><p>On the picture in Figure 4 the load balancer is located in Google Cloud but outside of any particular region. That kind of a global service can be provided by the following <a href="https://cloud.google.com/load-balancing/docs/application-load-balancer">types</a> of Google Cloud Load balancers:</p><ul><li>Global External Application Load Balancer</li><li>Classic Application Load Balancer in Premium Tier</li><li>Global External proxy Network Load Balancer</li><li>Classic Proxy Network Load Balancer</li></ul><p>Load balancers of all of these listed types load balancing traffic coming from the clients on the internet to the workloads running on Google Cloud.</p><p>An enterprise organization on Google Cloud would keep VPC networks private and expose application workloads to the internal company clients, which are also often located across the world.</p><p><em>Internal</em> load balancers on Google Cloud restrict access to the application to the clients in internal networks only. Unlike global external, <em>internal</em> load balancers on Google Cloud currently rely on the regional infrastructure. Availability of the applications exposed by internal load balancers can hence be affected by a single cloud region outage.</p><p>That means that for the internal clients the multi-regional application deployment depicted in Figure 4 logically changes to:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xauNw4H1lLTDmhzblFpk2g.png" /><figcaption>Fig. 5: Multiple application instances in multiple GCE Regions with internal load balancing</figcaption></figure><p>The choice of internal load balancers on Google Cloud is even bigger:</p><ul><li>Regional Internal Application Load Balancer</li><li>Cross-region Internal Application Load Balancer</li><li>Regional internal proxy Network Load Balancer</li><li>Cross-region internal proxy Network Load Balancer</li><li>Internal passthrough Network Load Balancer</li></ul><p>There is an open question with the regional internal load balancing though. How would application clients know and seamlessly failover to the healthy region in case of a full region outage (a high bar challenge we have set us up to)?</p><p>To address that challenge we can revert to a well known technique called <a href="https://en.wikipedia.org/wiki/Round-robin_DNS">DNS load balancing</a> (or round-robin DNS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*17RU8OR_tQTt7IOethwokQ.png" /><figcaption>Fig. 6: Internal DNS load balancing with regional application backends</figcaption></figure><p>Fully managed Google Cloud DNS service offers an important and convenient tool to setup such cross-regional client access, it is called <a href="https://cloud.google.com/dns/docs/policies-overview#geolocation-policy">Geolocation routing policies</a> and it “<em>lets you map traffic originating from source geographies (Google Cloud regions) to specific DNS targets. Use this policy to distribute incoming requests to different service instances based on the traffic’s origin. You can use this feature with the internet, with external traffic, or with traffic originating within Google Cloud and bound for internal passthrough Network Load Balancers. Cloud DNS uses the region where the queries enter Google Cloud as the source geography.</em>”</p><p>Using Cloud DNS Geolocation routing policies in the setup depicted in Figure 6 application clients will automatically receive IP address of the Internal Load Balancer nearest to their geographic location from the Cloud DNS server.</p><p>Please note, that the Google Cloud DNS is a fully managed <em>global</em> service offering impressive <a href="https://cloud.google.com/dns/sla">SLO targets</a>. DNS cache on the application client side helps sustaining the application service available in the rare case of possible Cloud DNS service outage.</p><p>In fact, many parts of the <a href="https://cloud.google.com/load-balancing/docs/l7-internal/setting-up-l7-cross-reg-internal">Cross-region Internal Application Load Balancers </a>are <em>global</em> Google Cloud resources as well. Here is a more detailed diagram borrowed from the public Google Cloud pages:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*PFjluXOPFbz_x475" /><figcaption>Fig. 7: Global resources of the internal cross-region load balancer</figcaption></figure><h3>Failover</h3><p>But what exactly happens to the client connections and overall application availability in case of an individual GCE VM, zone or region outage? How would a DNS service know that it needs to resolve application hostnames to a different IP address to direct clients to another (healthy) region?</p><p>The Cloud DNS service and its Geolocation routing policies in particular has yet another feature which completes the multi-regional application deployment puzzle. It is <a href="https://cloud.google.com/dns/docs/zones/manage-routing-policies#health-checks">Health Checks</a>.</p><p>For Internal Passthrough Network Load Balancers (L4), Cloud DNS checks the health information on the load balancer’s individual backend instances to determine if the load balancer is healthy or unhealthy. Cloud DNS applies a default 20% threshold, and if at least 20% of backend instances are healthy, the load balancer endpoint is considered healthy. DNS routing policies mark the endpoint as healthy or unhealthy based on this threshold, routing traffic accordingly.</p><p>For Internal Application Load Balancers and Cross-region Internal Application Load Balancers, Cloud DNS checks the overall health of the internal Application Load Balancer, and lets the internal Application Load Balancer itself check the health of its backend instances.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*m6FRNR7-8iegZykNfqZcSA.png" /><figcaption>Fig. 8: Internal DNS load balancing with cross-region application backends with health checks</figcaption></figure><p>Cloud DNS health checks are a crucial solution component for achieving maximum application uptime. Without Cloud DNS ability to test the current status of the internal load balancers and application backends it would not be able to reason about which IP address exactly should be returned to the clients on their DNS requests in case of a region outage. Hence the seamless client failover to the health application instances would not be possible.</p><p>Please note that in order to achieve best results, the Time-To-Live parameter of the application A-record in Cloud DNS needs to be set to a minimal value. It could even be zero, in which case applications would contact DNS for a current IP before every call to the application service. The choice of the DNS record TTL value is a tradeoff between the application availability requirements and DNS service load and client response latency.</p><p>Internal load balancers maintain their own application backends health checks (Cloud DNS health checks are using a different mechanism) and in case of the cross-region internal application load balancers a load balancer operating in a particular region can automatically failover and redirect client requests to the application replicas running in another healthy region.</p><p>This setup addresses the “partial” region outage scenario. That is when only application backend instances are not available (e.g. GCE VMs are down or there is a error in the application preventing it from accepting incoming connections) but other services in the affected region (such as networking and load balancing) continue working.</p><h3>Configuration with Managed Instance Groups</h3><p>Let’s combine all pieces of an HA solution discussed before into a single picture and see how the Google Cloud resources need to be configured together to achieve the desired effect.</p><p>GCE Managed Instance Groups based scenarios, discussed in this article, are also relevant to the managed Kubernetes service on Google Cloud, GKE, as well. Kubernetes node pools in GKE are implemented as GCE MIGs. Hence, Kubernetes workload deployed to GKE on Google Cloud can be made multi-regional by deploying the application service to several GKE clusters in distinct regions. The load balancer resources for such set up can be provisioned using</p><ul><li><a href="https://gateway-api.sigs.k8s.io/">Kubernetes Gateway API</a> and <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api#gateway_controller">GKE Gateway Controller</a> in GKE clusters</li><li><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress">Multi Cluster Ingress</a> resources in GKE clusters</li><li>Terraform resources (outside of GKE cluster)</li></ul><p>In this example we will use Terraform to set up load balancers, a common tool for declarative cloud infrastructure definition and provisioning, but it is also possible to achieve the same setup using the other two approaches. You can find the full Terraform example in <a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing">this</a> GitHub project.</p><p>Our first example will be based on the regional <a href="https://cloud.google.com/load-balancing/docs/internal">Internal Network Passthrough Load Balancers</a> and Google Compute Engine (GCE) <a href="https://cloud.google.com/compute/docs/instance-groups#managed_instance_groups">Managed Instance Groups</a> (MIGs).</p><p>In the last section of this article we’ll discus pros and cons of different load balancer and application backends combinations.</p><p><a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing/mig.tf">First</a> we define the GCE MIGs in two Google Cloud regions:</p><pre>// modules/mig/mig.tf:<br>module &quot;gce-container&quot; {<br>  source = &quot;terraform-google-modules/container-vm/google&quot;<br>  container = {<br>    image = var.image<br>    env = [<br>      {<br>        name = &quot;NAME&quot;<br>        value = &quot;hello&quot;<br>      }<br>    ]<br>  }<br>}<br><br>data &quot;google_compute_default_service_account&quot; &quot;default&quot; {<br>}<br>module &quot;mig_template&quot; {<br>  source               = &quot;terraform-google-modules/vm/google//modules/instance_template&quot;<br>  version              = &quot;~&gt; 10.1&quot;<br>  network              = var.network_id<br>  subnetwork           = var.subnetwork_id<br>  name_prefix          = &quot;mig-l4rilb&quot;<br>  service_account      = {<br>    email  = data.google_compute_default_service_account.default.email<br>    scopes = [&quot;cloud-platform&quot;]<br>  }<br>  source_image_family  = &quot;cos-stable&quot;<br>  source_image_project = &quot;cos-cloud&quot;<br>  machine_type         = &quot;e2-small&quot;<br>  source_image         = reverse(split(&quot;/&quot;, module.gce-container.source_image))[0]<br>  metadata             = merge(var.additional_metadata, { &quot;gce-container-declaration&quot; = module.gce-container.metadata_value })<br>  tags = [<br>    &quot;container-vm-test-mig&quot;<br>  ]<br>  labels = {<br>    &quot;container-vm&quot; = module.gce-container.vm_container_label<br>  }<br>}<br><br>module &quot;mig&quot; {<br>  source             = &quot;terraform-google-modules/vm/google//modules/mig&quot;<br>  version            = &quot;~&gt; 10.1&quot;<br>  project_id         = var.project_id<br><br>  region             = var.location<br>  instance_template  = module.mig_template.self_link<br>  hostname           = &quot;${var.name}&quot;<br>  target_size        = &quot;1&quot;<br>  <br>  autoscaling_enabled = &quot;true&quot;<br>  min_replicas = &quot;1&quot;<br>  max_replicas = &quot;1&quot;<br>  named_ports = [{<br>    name = var.lb_proto<br>    port = var.lb_port<br>  }] <br><br>  health_check_name = &quot;${var.name}-http-healthcheck&quot;<br>  health_check = {<br>    type = &quot;http&quot;<br>    initial_delay_sec   = 10<br>    check_interval_sec  = 2<br>    healthy_threshold   = 1<br>    timeout_sec         = 1<br>    unhealthy_threshold = 1<br>    port                = 8080<br>    response            = &quot;&quot;<br>    proxy_header        = &quot;NONE&quot;<br>    request             = &quot;&quot;<br>    request_path        = &quot;/&quot;<br>    host                = &quot;&quot;<br>    enable_logging      = true<br>  }<br>}<br><br>// mig.tf<br>module &quot;mig-l4&quot; {<br>  for_each      = var.locations<br>  source        = &quot;./mig&quot;<br>  project_id    = var.project_id<br>  location      = each.key<br>  network_id    = data.google_compute_network.lb_network.id<br>  subnetwork_id = data.google_compute_subnetwork.lb_subnetwork[each.key].id<br>  name          = &quot;failover-l4-${each.key}&quot;<br>  image         = var.image<br>}</pre><p><a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing/l4-rilb-mig.tf">Then</a>, let’s define two Cross-regional Internal Network Passthrough Load Balancers (L4 ILBs), each in respective region:</p><pre>// modules/l4rilb/l4-rilb.tf<br>locals {<br>  named_ports = [{<br>    name = var.lb_proto<br>    port = var.lb_port<br>  }]<br>  health_check = {<br>    type                = var.lb_proto<br>    check_interval_sec  = 1<br>    healthy_threshold   = 4<br>    timeout_sec         = 1<br>    unhealthy_threshold = 5<br>    response            = &quot;&quot;<br>    proxy_header        = &quot;NONE&quot;<br>    port                = var.lb_port<br>    port_name           = &quot;health-check-port&quot;<br>    request             = &quot;&quot;<br>    request_path        = &quot;/&quot;<br>    host                = &quot;1.2.3.4&quot;<br>    enable_log          = false<br>  }<br>}<br><br>module &quot;l4rilb&quot; {<br>  source        = &quot;GoogleCloudPlatform/lb-internal/google&quot;<br>  project       = var.project_id<br>  region        = var.location<br>  name          = &quot;${var.lb_name}&quot;<br>  ports         = [local.named_ports[0].port]<br>  source_tags   = [&quot;allow-group1&quot;]<br>  target_tags   = [&quot;container-vm-test-mig&quot;]<br>  health_check  = local.health_check<br>  global_access = true<br><br>  backends = [<br>    {<br>      group       = var.mig_instance_group<br>      description = &quot;&quot;<br>      failover    = false<br>    },<br>  ]<br>}<br><br>// l4-rilb-mig.tf<br>module &quot;l4-rilb&quot; {<br>  for_each            = var.locations<br>  source              = &quot;./modules/l4rilb&quot;<br>  project_id          = var.project_id<br>  location            = each.key<br>  lb_name             = &quot;l4-rilb-${each.key}&quot;<br>  mig_instance_group  = module.mig-l4[each.key].instance_group<br>  image               = var.image<br>  network_id          = data.google_compute_network.lb_network.id<br>  subnetwork_id       = data.google_compute_subnetwork.lb_subnetwork[each.key].name<br><br>  depends_on = [ <br>    google_compute_subnetwork.proxy_subnetwork <br>  ]<br>}</pre><p>And now let’s also <a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing/dns-l4-rilb-mig.tf">add</a> the global Cloud DNS record set configuration:</p><pre>// dns-l4-rilb-mig.tf<br>resource &quot;google_dns_record_set&quot; &quot;a_l4_rilb_mig_hello&quot; {<br>  name         = &quot;l4-rilb-mig.${google_dns_managed_zone.hello_zone.dns_name}&quot;<br>  managed_zone = google_dns_managed_zone.hello_zone.name<br>  type         = &quot;A&quot;<br>  ttl          = 1<br><br>  routing_policy {<br>    dynamic &quot;geo&quot; {<br>      for_each  = var.locations<br>      content {<br>        location  = geo.key<br>        health_checked_targets {<br>          internal_load_balancers {<br>              ip_address         = module.l4-rilb[geo.key].lb_ip_address<br>              ip_protocol        = &quot;tcp&quot;<br>              load_balancer_type = &quot;regionalL4ilb&quot;<br>              network_url        = data.google_compute_network.lb_network.id<br>              port               = &quot;8080&quot;<br>              region             = geo.key<br>              project            = var.project_id<br>            }<br>        }<br>      }<br>    }<br>  }  <br>}</pre><p>After we apply the Terraform configuration to the target Google Cloud project:</p><pre>terraform init<br>terraform plan<br>terraform apply</pre><p>we get all solution infrastructure components including a test application running in the GCE VMs in two distinct Google Cloud regions needed to perform end-to-end testing.</p><p>Let’s see how the clients can now access our application.</p><p>For testing of continuous request flow we can use the <a href="https://fortio.org/">Fortio</a> tool, which is a common tool for testing service mesh application performance. We will run it from a GCE VM attached to the same VPC where the load balancers are installed:</p><pre>gcloud compute ssh jumpbox<br><br>docker run fortio/fortio load --https-insecure -t 1m -qps 1 http://l4mig.hello.zone:8080</pre><p>The results after a minute of execution should look similar to the following:</p><pre>IP addresses distribution:<br>10.156.0.11:8080: 1<br>Code 200 : 258 (100.0 %)<br>Response Header Sizes : count 258 avg 390 +/- 0 min 390 max 390 sum 100620<br>Response Body/Total Sizes : count 258 avg 7759.624 +/- 1.497 min 7758 max 7763 sum 2001983<br>All done 258 calls (plus 4 warmup) 233.180 ms avg, 17.1 qps</pre><p>Please note the IP address of the L4 internal regional load balancer in the nearest region that is getting all of the calls.</p><p>In the second console window SSH into the VM in the GCE MIG group in the nearest region</p><pre>export MIG_VM=$(gcloud compute instances list --format=&quot;value[](name)&quot; --filter=&quot;name~l4-europe-west3&quot;)<br>export MIG_VM_ZONE=$(gcloud compute instances list --format=&quot;value[](zone)&quot; --filter=&quot;name=${MIG_VM}&quot;)<br><br>gcloud compute ssh --zone $MIG_VM_ZONE $MIG_VM --tunnel-through-iap --project $PROJECT_ID<br><br>docker ps</pre><p>Now let’s run the load test in the first console window again.</p><p>While the test is running switch to the second console window and execute</p><pre>docker stop ${CONTAINER}</pre><p>Switch to the first console window and notice the failover happening. The output at the end of the execution should look like following</p><pre>IP addresses distribution:<br>10.156.0.11:8080: 16<br>10.199.0.48:8080: 4<br>Code -1 : 12 (10.0 %)<br>Code 200 : 108 (90.0 %)<br>Response Header Sizes : count 258 avg 390 +/- 0 min 390 max 390 sum 100620<br>Response Body/Total Sizes : count 258 avg 7759.624 +/- 1.497 min 7758 max 7763 sum 2001983<br>All done 120 calls (plus 4 warmup) 83.180 ms avg, 2.0 qps</pre><p>Please note that the service VM in the Managed Instance has been automatically restarted. This functionality is provided by the Google Compute Engine Managed Instance groups and implements the forth component of application high availability posture, as you remember from the beginning of the article, it is <strong><em>self-healing</em></strong>.</p><h3>Cloud Run Backends</h3><p>Let’s consider a second scenario and assume that our cloud-native application is implemented as a <a href="https://cloud.google.com/run">Cloud Run</a> service.</p><p>The Cloud Run based scenarios, discussed in this article, are relevant for the <a href="https://cloud.google.com/functions/docs/concepts/version-comparison#new-in-2nd-gen">Cloud Functions</a> (2nd generation) application backends as well. Cloud Functions can be configured as load balancer backends similarly to the Cloud Run instances using the same Serverless Network Endpoint Groups resources.</p><p>The overall multi-region application deployment changes slightly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tBWHPTGg6H1Tne0vW6Ip0w.png" /><figcaption>Fig. 9: Internal DNS load balancing with cross-region application backends in Cloud Run</figcaption></figure><p>We <a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing/modules/cr/cr2.tf">start</a> with defining two regional Cloud Run instances allowing invocations from unauthenticated clients:</p><pre>// modules/cr/cr2.tf<br>resource &quot;google_cloud_run_v2_service&quot; &quot;cr_service&quot; {<br>  project       = var.project_id<br>  name          = &quot;cr2-service&quot;  <br>  location      = var.location<br>  launch_stage  = &quot;GA&quot;<br><br>  ingress = &quot;INGRESS_TRAFFIC_INTERNAL_LOAD_BALANCER&quot;<br>  custom_audiences = [ &quot;cr-service&quot; ]<br><br>  template {<br>    containers {<br>      image = &quot;gcr.io/cloudrun/hello&quot; # public image for your service<br>    }<br>  }<br>  traffic {<br>    percent         = 100<br>    type = &quot;TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST&quot;<br>  }<br>}<br><br>resource &quot;google_compute_region_network_endpoint_group&quot; &quot;cloudrun_v2_sneg&quot; {<br>  name                  = &quot;cloudrun-sneg&quot;<br>  network_endpoint_type = &quot;SERVERLESS&quot;<br>  region                = var.location<br>  cloud_run {<br>    service = google_cloud_run_v2_service.cr_service.name<br>  }<br>}<br><br>resource &quot;google_cloud_run_v2_service_iam_member&quot; &quot;public-access&quot; {<br>  name     = google_cloud_run_v2_service.cr_service.name<br>  location = google_cloud_run_v2_service.cr_service.location<br>  project  = google_cloud_run_v2_service.cr_service.project<br>  role     = &quot;roles/run.invoker&quot;<br>  member   = &quot;allUsers&quot;<br>}<br><br>// cr.tf<br>module &quot;cr-service&quot; {<br>  for_each    = var.locations<br>  source      = &quot;./modules/cr&quot;<br>  project_id  = var.project_id<br>  location    = each.key<br>  image       = var.image<br>}</pre><p>And <a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing/l7-crilb-cr.tf">then</a> define global Internal Cross-Region Application Load Balancer resources:</p><pre>// modules/l7crilb/l7-crilb.tf<br>resource &quot;google_compute_global_forwarding_rule&quot; &quot;forwarding_rule&quot; {<br>  for_each              = var.subnetwork_ids<br>  project               = var.project_id<br>  <br>  name                  = &quot;${var.lb_name}-${each.key}&quot;<br><br>  ip_protocol           = &quot;TCP&quot;<br>  load_balancing_scheme = &quot;INTERNAL_MANAGED&quot;<br>  port_range            = var.lb_port<br>  target                = google_compute_target_https_proxy.https_proxy.self_link<br>  network               = var.network_id<br>  subnetwork            = each.value<br>}<br><br>resource &quot;google_compute_target_https_proxy&quot; &quot;https_proxy&quot; {<br>  project  = var.project_id<br><br>  name    = &quot;${var.lb_name}&quot;<br>  url_map = google_compute_url_map.url_map.self_link<br><br>  certificate_manager_certificates = [<br>    var.certificate_id<br>  ]<br>  lifecycle {<br>    ignore_changes = [<br>      certificate_manager_certificates<br>    ]<br>  }<br>}<br><br>resource &quot;google_compute_url_map&quot; &quot;url_map&quot; {<br>  project  = var.project_id<br><br>  name            = &quot;${var.lb_name}&quot;<br>  default_service = google_compute_backend_service.backend_service.self_link<br>}<br><br>resource &quot;google_compute_backend_service&quot; &quot;backend_service&quot; {<br>  project  = var.project_id<br><br>  load_balancing_scheme = &quot;INTERNAL_MANAGED&quot;<br>  session_affinity = &quot;NONE&quot;<br>  <br>  dynamic &quot;backend&quot; {<br>    for_each          = var.backend_group_ids<br>    content {<br>      group           = backend.value<br>      balancing_mode  = var.balancing_mode<br>      capacity_scaler = 1.0      <br>    }<br>  }<br><br>  name        = &quot;${var.lb_name}&quot;<br>  protocol    = var.backend_protocol<br>  timeout_sec = 30<br><br>  // &quot;A backend service cannot have a healthcheck with Serverless network endpoint group backends&quot;<br>  health_checks = var.is_sneg ? null : [google_compute_health_check.health_check.self_link]<br><br>  outlier_detection {<br>    base_ejection_time {<br>      nanos     = 0<br>      seconds   = 1<br>    }<br>    consecutive_errors = 3<br>    enforcing_consecutive_errors = 100<br>    interval {<br>      nanos     = 0<br>      seconds   = 1<br>    }<br>    max_ejection_percent = 50<br>  }<br><br>}<br><br>resource &quot;google_compute_health_check&quot; &quot;health_check&quot; {<br>  project    = var.project_id<br><br>  name   = &quot;${var.lb_name}&quot;<br>  http_health_check {<br>    port_specification = &quot;USE_SERVING_PORT&quot;<br>  }<br>}<br><br>// l7-crilb-cr.tf<br>module &quot;l7-crilb-cr&quot; {<br>  source            = &quot;./modules/l7crilb&quot;<br>  project_id        = var.project_id<br>  lb_name           = &quot;l7-crilb-cr&quot;<br><br>  network_id        = data.google_compute_network.lb_network.name<br>  subnetwork_ids    = { for k, v in data.google_compute_subnetwork.lb_subnetwork : k =&gt; v.id }<br>  certificate_id    = google_certificate_manager_certificate.ccm-cert.id<br>  backend_group_ids = [ for k, v in module.cr-service : v.sneg_id ]<br>  is_sneg           = true<br>}</pre><p>Please note that all load balancer related resources in this case are global (not regional).</p><p>In this demo case we need to <a href="https://github.com/sshcherbakov/professional-services/blob/cloud-dns-load-balancing/examples/cloud-dns-load-balancing/dns-l7-crilb-cr.tf">define</a> <a href="https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/cloud-dns-load-balancing/dns-l7-crilb-cr.tf">define</a> the Cloud DNS resources as well:</p><pre>// dns-l7-crilb-cr.tf<br>resource &quot;google_dns_record_set&quot; &quot;a_l7_crilb_cr_hello&quot; {<br>  name         = &quot;l7-crilb-cr.${google_dns_managed_zone.hello_zone.dns_name}&quot;<br>  managed_zone = google_dns_managed_zone.hello_zone.name<br>  type         = &quot;A&quot;<br>  ttl          = 1<br><br>  routing_policy {<br>    dynamic &quot;geo&quot; {<br>      for_each  = var.locations<br>      content {<br>        location  = geo.key<br>        health_checked_targets {<br>          internal_load_balancers {<br>              ip_address         = module.l7-crilb-cr.lb_ip_address[geo.key]<br>              ip_protocol        = &quot;tcp&quot;<br>              load_balancer_type = &quot;globalL7ilb&quot;<br>              network_url        = data.google_compute_network.lb_network.id<br>              port               = &quot;443&quot;<br>              project            = var.project_id<br>            }<br>        }<br>      }<br>    }<br>  }  <br>}</pre><p>For each region where the Cloud Run instance with our application is running we need to create a dedicated Cloud DNS routing policy.</p><p>Let’s now apply the Terraform to the target Google Cloud project and see how the clients can access our Cloud Run application.</p><p>Similarly to the Network Passthrough load balancer case described in the previous section, we’ll call our application endpoint exposed by the Cloud Run via the configured FQDN hostname:</p><pre>gcloud compute ssh jumpbox</pre><pre>docker run fortio/fortio load --https-insecure \<br>    -t 5m -qps 1 <a href="https://l7-crilb-cr.hello.zone">https://l7-crilb-cr.hello.zone</a></pre><p>The results after a minute of execution should look similar to the following:</p><pre>IP addresses distribution:<br>10.156.0.55:443: 4<br>Code 200 : 8 (100.0 %)<br>Response Header Sizes : count 8 avg 216 +/- 0 min 216 max 216 sum 1728<br>Response Body/Total Sizes : count 8 avg 226 +/- 0 min 226 max 226 sum 1808<br>All done 8 calls (plus 4 warmup) 17.066 ms avg, 1.4 qps</pre><p>With our Fortio tool setup of one call per second, all calls have reached their destination.</p><p>The IP address that shows up in the output is the IP of the L7 internal cross-regional load balancer in the nearest region that is receiving all of our calls at the moment.</p><p>To simulate Cloud Run backend service outage, while running Fortio tool started in the previous step, in the second console window we can delete the backend resource in the nearest region from the load balancer backend service definition, e.g.:</p><pre>gcloud compute backend-services remove-backend l7-crilb-cr \<br>   --network-endpoint-group=cloudrun-sneg \<br>   --network-endpoint-group-region=europe-west3 \<br>   --global</pre><p>We can also check, to which regions the load balancer sends the traffic using:</p><pre>gcloud compute backend-services list --filter=&quot;name:l7-crilb-cr&quot;<br><br>NAME         BACKENDS                                         PROTOCOL<br>l7-crilb-cr  us-central1/networkEndpointGroups/cloudrun-sneg  HTTPS</pre><p>There is only one backend left running in the remote region. Yet, the Fortio results in the first console session show no hiccup or interruption:</p><pre>IP addresses distribution:<br>10.156.0.55:443: 4<br>Code 200 : 300 (100.0 %)<br>Response Header Sizes : count 300 avg 216.33333 +/- 1.106 min 216 max 220 sum 64900<br>Response Body/Total Sizes : count 300 avg 226.33333 +/- 1.106 min 226 max 230 sum 67900<br>All done 300 calls (plus 4 warmup) 193.048 ms avg, 1.0 qps</pre><p>What we have seen so far was the failover at the Internal Cross-Regional Application load balancer backend side. That is, the client application (Fortio) was still accessing the load balancer IP address in the nearest europe-west3 region. That can also be verified by running host l7-crilb-cr.hello.zone which will return the internal load balancer IP address from the subnetwork in the europe-west3 region.</p><p>What would happen in case of a full local region outage?</p><p>The first use case discussed above (Network Passthrough Load Balancer with MIG backends) illustrates that case. The Cloud DNS L4 health checks for the Network Passthrough load balancer test connection all way through to the actual application process running in GCE VMs (it is not possible to configure this type of load balancer with <a href="https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts">Serverless Network Endpoint Groups</a> backends for Cloud Run instances) and flips the IP address for the application service host name to the load balancer IP address in another region automatically.</p><p>Unfortunately, the <a href="https://cloud.google.com/dns/docs/zones/manage-routing-policies#health-checks">Cloud DNS health checks</a> for application (L7) load balancers cannot detect the outage of the application backend service with that fidelity level yet. Regional and Cross-regional Application load balancers are built on Envoy proxies internally and Envoy proxy based load balancers are only health checking the state and availability of the Envoy proxy instances, not the application backends themselves.</p><p>If an application running in Cloud Run experiences malfunction (e.g. as a result of internal program error) and returns 500 response codes, the Cloud DNS won’t detect that and won’t take action switching the load balancer IPs for the application hostname. That situation would be detected by the <a href="https://cloud.google.com/load-balancing/docs/https/setting-up-global-traffic-mgmt#configure_outlier_detection">Outlier Detection</a> feature of the Internal Cross-regional Application Load Balancer and the load balancer will redirect traffic to the healthy backend by looking at the rate of successful calls towards each backend.</p><p>A missing load balancer backend is not considered as an outage by the Cloud DNS health checks though. When a Cross-regional Application Load Balancer backend resources are not properly configured, load balancer has none of them, or has malfunctioning backends, the Cloud DNS won’t take action and won’t flip load balancer IPs automatically. The Cloud DNS health checks only check the availability of the internal Google Cloud infrastructure (Envoy proxies) supporting the application (L7) load balancers.</p><p>Yet, in case a full Google Cloud region outage the load balancer infrastructure would not be available fully and the Cloud DNS health check could detect that and act as expected.</p><h3>Options Choice</h3><p>When designing a highly available application service distributed across multiple regions on Google Cloud we need to consider the currently existing constraints in the Google Cloud services and pick a combination that would support the application requirements.</p><p>Here are the constraints and trade-offs that you should consider when picking the Google Cloud load balancer type for your distributed application.</p><p><strong>1. External vs Internal Load Balancers</strong></p><p>The Global External Application Load Balancer offers the tools for building geographically distributed application service with best availability guarantees.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-l8le_X_Hupbj5M4CIqaGw.png" /><figcaption>Fig. 10: External application load balancer with backends in Cloud Run</figcaption></figure><p>Instead of relying upon the DNS load balancing trick, it provides application endpoint availability to the clients via global anycast Virtual IP addresses and smart global network infrastructure that cleverly routes client traffic to the infrastructure and services in the healthy regions.</p><p>The Google Cloud <em>internal</em> load balancers infrastructure is available via regional IP addresses instead and hence require additional mechanism, Cloud DNS load balancing suggested in this article, to address the full regional outage scenario.</p><p><strong>2. Managed Instance Groups vs Cloud Run Backends</strong></p><p>The Network Passthrough (L4) Load Balancers cannot be configured with Cloud Run backends. They don’t support Serverless Network Endpoint Groups (NEGs) required for Cloud Run and Cloud Function based backends at the moment.</p><p>Hence, if you are building a multi-regional application service that should only be available in the <em>internal</em> company VPC network and would like to address majority of possible outage scenarios (full region outage, individual regional Google Cloud service outage, application service malfunction), then Network Passthrough (L4) Load Balancer with GCE Managed Instance Groups is the only option for the application backends. Please remember, that the GCE MIGs is the mechanism supporting GKE node pools as well. Hence, the GCE MIG backends option is also applicable to the Kubernetes workloads running in GKE.</p><p>An important consideration for the Cloud Run backends in multiple regions is authentication.</p><p>In order to seamlessly continue service for the authenticated Cloud Run clients the Cloud Run instances in different regions must be configured with <a href="https://cloud.google.com/run/docs/configuring/custom-audiences">Custom Audiences</a>. In that way, the access token that client passes along with an authenticated call can be validated and accepted by the Cloud Run backends in all regions. Please note that the Custom Audiences is a feature available in Cloud Run instances of the 2nd generation. Cloud Run instances of the 1st generation can be used in the suggested multi-regional setup only when the application service does not need to authenticate clients.</p><p><strong>3. Network Passthrough (L4) vs Application (L7) Load Balancers</strong></p><p>Selecting the load balancer type also depends on the functionality that a load balancer can provide. In case of a Passthrough (L4) load balancer the application would need to implement the following tasks by itself (to name a few):</p><ul><li>terminate TLS connections</li><li>authenticate incoming calls</li><li>implement request routing</li></ul><p>The Application (L7) load balancers can help with that but their internal versions address less failure scenarios, in comparison to the Network Passthrough (L4) load balancer based solution, because of the current feature level of the Cloud DNS health check mechanism. For example, Cloud DNS would not flip an application service IP address in case if the application is experiencing internal malfunction (e.g. returning 50x error codes) or the load balancer backend is unavailable or missing altogether.</p><p>This is not a problem with External Application (L7) load balancers, since there is no need in Cloud DNS load balancing solution for exposing application in a high available way in multiple regions.</p><p>These mentioned “partial” or individual regional service infrastructure outage scenarios are handled by the Internal Cross-Regional Application Load Balancers on their backend side though. In addition, optional <a href="https://cloud.google.com/load-balancing/docs/https/setting-up-global-traffic-mgmt#configure_outlier_detection">Outlier Detection</a> load balancer configuration can help detecting application level malfunctions at the cost of wasting certain percentage of actual client requests in the case of outage.</p><h3>Conclusion</h3><p>Google Cloud goes beyond usual redundant deployments and offers architects and developers tools for building highly available application services across multiple geographic locations also for internal security restricted corporate use cases.</p><p>The choice of the particular combination of Google Cloud resources for improving multi-regional application availability depends on the individual applications requirements and features currently supported in the Google Cloud services such as network load balancers and Cloud DNS.</p><p>Enterprise security features in Google Cloud services get special attention and differentiate Google Cloud from other cloud hyperscalers. Please check one of my previous articles on <a href="https://medium.com/google-cloud/application-secrets-encryption-in-kubernetes-and-anthos-products-ae5de5905224">Application Secrets Encryption in Google Cloud Kubernetes products</a> for an example of possible with Google Cloud products.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=823b9f706578" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/multi-region-ha-in-google-cloud-823b9f706578">Multi-region HA in Google Cloud</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automating data extraction from SEC 10-K forms using Document AI and Generative AI]]></title>
            <link>https://medium.com/google-cloud/automating-data-extraction-from-sec-10-k-forms-using-document-ai-and-generative-ai-6b2a086d6167?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6b2a086d6167</guid>
            <category><![CDATA[genai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[automation]]></category>
            <category><![CDATA[document-ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Harish Verma]]></dc:creator>
            <pubDate>Thu, 18 Apr 2024 06:15:18 GMT</pubDate>
            <atom:updated>2024-04-18T06:15:17.975Z</atom:updated>
            <content:encoded><![CDATA[<p>SEC10K forms are comprehensive financial reports that public companies file with the U.S. Securities and Exchange Commission (SEC) to disclose their financial performance. However, SEC 10-K forms can be very large, ranging from 50 to over 200 pages. Extracting data from these forms can be time-consuming and challenging due to their large size and complex format.</p><p>In this blog post, we will show you how to use Google Cloud’s Document AI and Generative AI to parse SEC 10-K forms and extract key information. This solution can save you time and effort, and it can help you to make more informed investment decisions quickly.</p><h3>Solution Architecture</h3><p>The solution architecture for Sec10k Form Parser using Document AI and Generative AI is shown below. The solution consumes a pdf document and extracts predefined fields.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zoXK4BtnBH0mRdUC" /><figcaption><em>Solution Architecture</em></figcaption></figure><p>The solution consists of the following components:</p><ul><li>Document AI <a href="https://cloud.google.com/document-ai/docs/workbench/build-custom-splitter-processor">Custom Document Splitter (CDS)</a>: Given a Sec 10-K document it splits the SEC 10-K form into individual sections.</li><li>Document AI <a href="https://cloud.google.com/document-ai/docs/workbench/build-custom-processor">Custom Document Extractor (CDE)</a>: Extracts key information present in tabular form from different sections of the SEC 10-K form.</li><li>Generative AI: Extracts text-based information from the SEC 10-K form.</li><li>BigQuery: Stores the extracted data</li></ul><h3>Data and Model Training</h3><p>The solution was trained on a dataset of SEC10K forms. You can find Kaggle Dataset <a href="https://www.kaggle.com/datasets/pranjalverma08/sec-edgar-annual-financial-filings-2021?resource=download">SEC Edgar Annual Financial Filings — 2021</a> for Sec10K form dataset.</p><p>For Generative AI, fields like company names, addresses, year end date are extracted by providing relevant content to the text-bison model.</p><p>For Custom Document Splitter, we divided the document into sections like Introduction and Signature along with identifying important tables like Consolidated Balance Sheet and Statement of Operations. We labeled and trained on 50+ numbers of training documents.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*A7xu-j3WS4zCOgxZ" /><figcaption><em>Snapshot of Custom Document Splitter developed</em></figcaption></figure><p>For Custom Document Extractor, the documents were labeled to identify the relevant fields. Examples of labels from tables of Consolidated Balance Sheet and Statement of Operations are total current liabilities and assets, total net sales and operating expenses with year wise mapping. We labeled and trained on 50+ numbers of training documents.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IiDpojwfGtVvG5nP" /><figcaption><em>Snapshot of fields for Custom Document Extractor developed</em></figcaption></figure><p>Below is a sample page having a Consolidated Balance Sheet table in a Sec10k form.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rBJZwtTJjYax9op4" /><figcaption><em>Consolidated Balance Sheet table in Sec10K form (</em><a href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000045/goog-20230331.htm#i0c9da2ad630a471ab2611da204d142c8_19"><em>Source</em></a><em>)</em></figcaption></figure><h3>Results</h3><p>The solution was evaluated on a test set of 20 documents and has demonstrated impressive results.</p><ul><li><strong>95%+</strong> accuracy on Document Splitter to identify different sections of the forms</li><li><strong>90%+</strong> accuracy on field extraction of tabular data using Custom Document Extractor</li><li><strong>99%+</strong> accuracy on field extraction of textual data using Generative AI</li></ul><p>We tried our solution developed on the latest filing of Sec10k form by Alphabet Inc. which is publicly available <a href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000045/goog-20230331.htm">here</a>. Below is the snapshot of the 50 pager document.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YhZdo6JY_JR1u9CG" /><figcaption><a href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000045/goog-20230331.htm#i0c9da2ad630a471ab2611da204d142c8_19">Source</a></figcaption></figure><p>Here is the output produced from the solution developed by directly ingesting the pdf shared by Alphabet.</p><pre>{&#39;company_address&#39;: &#39;1600 Amphitheatre Parkway Mountain View, CA 94043&#39;,<br>&#39;company_name&#39;: &#39;Alphabet Inc.&#39;,<br>&#39;company_phone&#39;: &#39;(650) 253-0000&#39;,<br>&#39;fiscal_year&#39;: &#39;March 31, 2023&#39;,<br>&#39;form_type&#39;: &#39;10-Q&#39;,<br>&quot;chief_financial_officer&quot;: &quot;Ruth M. Porat&quot;,<br>&#39;current_assets&#39;: {&#39;previous&#39;: &#39;164,795&#39;, &#39;current&#39;: &#39;161,985&#39; ,&#39;description&#39;: &#39;Total current assets&#39;}<br>&#39;current_liabilities&#39;: {&#39;previous&#39;: &#39;69,300&#39;, &#39;current&#39;: &#39;68,854&#39; ,&#39;description&#39;: &#39;Total current liabilities&#39;}<br>&#39;net_income&#39;: {&#39;previous&#39;: &#39;16,436&#39;, &#39;current&#39;: &#39;15,051&#39;, &#39;description&#39;: &#39;Net income&#39;}<br>&#39;total_net_sales&#39;: {&#39;previous&#39;: &#39;68,011&#39;,   &#39;current&#39;: &#39;69,787&#39;,  &#39;description&#39;: &#39;Revenues&#39;}</pre><h3>Conclusion</h3><p>The integration of Document AI and Generative AI offers a powerful solution for automating and enhancing SEC Form 10-K parsing. By leveraging machine learning and natural language processing capabilities, investors, analysts, and stakeholders can extract structured data with high accuracy, gain contextual understanding, and unlock data insights that are crucial for making informed decisions.</p><p>Learn more about the products used in the solution from links below:</p><ul><li><a href="https://cloud.google.com/document-ai?hl=en">Document AI</a></li><li><a href="https://cloud.google.com/vertex-ai?hl=en">Vertex AI</a></li><li><a href="https://cloud.google.com/storage?hl=en">Cloud Storage</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6b2a086d6167" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/automating-data-extraction-from-sec-10-k-forms-using-document-ai-and-generative-ai-6b2a086d6167">Automating data extraction from SEC 10-K forms using Document AI and Generative AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google APIs from Windows DNS Server]]></title>
            <link>https://medium.com/google-cloud/google-apis-from-windows-dns-server-874aa2cc1a35?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/874aa2cc1a35</guid>
            <category><![CDATA[dns]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[windows]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Julio Diez]]></dc:creator>
            <pubDate>Thu, 18 Apr 2024 06:14:53 GMT</pubDate>
            <atom:updated>2024-04-18T14:19:11.399Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*j8DUdJ5W5vY70kgVGcU5GA.png" /></figure><h3>Background</h3><p>Google Cloud customers often need to route particular Google-bound traffic like <em>storage.googleapis.com</em> from their on-premises network to their private Google Cloud Platform (GCP) connectivity — VPN or interconnect — rather than over the Internet, for efficiency, security, or compliance reasons. For example, this is necessary for cases like when a <a href="https://cloud.google.com/vpc-service-controls/docs/overview">VPC SC perimeter</a> surrounds Cloud Storage buckets since external requests are blocked, allowing only those from within the perimeter. If the on-premises network is part of the VPC SC perimeter, client devices there will need to forward their requests through the private connectivity to GCP to access the bucket.</p><p>In certain situations, redirecting all *.googleapis.com traffic from the on-premises network to GCP may not be feasible, and a more granular approach is required. This is particularly relevant when only a specific subset of clients should be directed to GCP projects within the company, while other clients accessing general Google APIs such as Search, Maps, or even Cloud Storage should retain access to public internet endpoints, with all clients sharing a single DNS environment. This article outlines how to configure GCP and Windows DNS Server to achieve this selective redirection.</p><h3>Solution</h3><p>The solution presented will handle a more complex scenario where various on-premises clients are linked to different GCP environments, <em>production</em> and <em>development</em>, in addition to regular clients accessing Google APIs through public endpoints. The solution assumes that clients can be identified based on their IP subnets, primarily those connecting to GCP.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/771/1*-sRE3m2lByAAz4DR5E9KvQ.png" /></figure><p>Due to the need to consider both DNS records and client-specific fields when responding to DNS queries, modifications to DNS results are necessary. This can be achieved using DNS Response Policies (<a href="https://www.ietf.org/archive/id/draft-vixie-dnsop-dns-rpz-00.txt">RPZ</a>), or in our case <a href="https://learn.microsoft.com/en-us/windows-server/networking/dns/deploy/dns-policies-overview">DNS Policies</a> that Windows Server 2016 introduced to provide similar capabilities to RPZ. DNS Policies provide enhanced flexibility and capabilities, such as Geo-location Traffic Management (ensuring clients receive the IP address of the geographically closest resource) and Split Brain DNS (tailoring responses based on the client’s network location).</p><h4>GCP-only clients</h4><p>To ensure that all traffic to Google APIs from the on-premises network is forwarded to GCP, one solution is for the on-premises network to become authoritative for the googleapis.com zone. With Windows DNS Policies, you can create your own records for your authoritative zones and implement the capabilities mentioned earlier through new objects:</p><ul><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverzonescope?view=windowsserver2022-ps">Zone Scope</a>: a zone comprises one default zone and can have various zone scopes. Each scope may contain an identical set of DNS records, but with different IP addresses assigned to each scope.</li><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverqueryresolutionpolicy?view=windowsserver2022-ps">Query Resolution Policy</a>: a policy applied to zone scopes that establishes procedures for resolving DNS queries using predefined criteria. These criteria could include the client’s subnet or the type of record being queried.</li><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverclientsubnet?view=windowsserver2022-ps">Client Subnet</a>: an object representing subnets corresponding to clients. You can utilize client subnet objects in query resolution policies to match incoming queries to specific clients.</li></ul><p>To access Google APIs supported by VPC Service Controls, you must set up either <a href="https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid">Private Google Access (PGA) for on-premises</a> or <a href="https://cloud.google.com/vpc/docs/about-accessing-google-apis-endpoints">Private Service Connect (PSC)</a>. With PGA, you redirect traffic to the special domain names <em>private.googleapis.com</em> or <em>restricted.googleapis.com</em>. The <em>restricted range</em>, 199.36.153.4/30, allows access to APIs and services that are supported by VPC Service Controls and blocks access to those services that are not supported. We will use this range in our configuration.</p><p>Within the googleapis.com zone scope, the on-premises DNS server should set up a CNAME record from *.googleapis.com to restricted.googleapis.com. Additionally, an A record should be created, pointing to the IP addresses within the restricted range. Further steps, such as configuring routing from the on-premises network to the restricted range in GCP, are necessary. Refer to the official documentation for more details.</p><p>In our final approach, we plan to use the newly introduced DNS Policy objects. However, this configuration, in its current form, will not work for regular clients that do not require to be forwarded to GCP. Any client not recognized as being in a particular subnet that requires redirection won’t match any resolution policy and will continue with the usual resolution procedure. As a result, they’ll get a reply with the records from the regular authoritative googleapis.com zone. Of course companies can’t be authoritative for Google APIs and their public endpoints.</p><h4>GCP and non-GCP clients</h4><p>In situations where both GCP and non-GCP or regular clients reside within the on-premises network, it is not feasible for the on-premises infrastructure to serve as the authoritative source for the googleapis.com zone. As a result, any queries made by these clients need to be forwarded to external resolvers. However, it’s important to note that using a resolver for Google APIs will only provide access to public endpoints.</p><p>Fortunately, Windows DNS Policies provide an object that we can use to resolve this issue:</p><ul><li><a href="https://learn.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverrecursionscope?view=windowsserver2022-ps">Recursion Scope</a>: similar to zone scopes, DNS recursion can have a default behavior as well as <em>scopes</em> where recursion or resolution behaves differently by directing to a unique set of forwarders.</li></ul><p>By configuring query resolution policies to selectively choose recursion scopes based on criteria such as the client subnet, it becomes possible to resolve the same domains to different IP addresses. This approach provides a flexible solution for managing DNS resolution in complex network environments.</p><h4>High-level diagram</h4><p>The DNS process we’ll implement is outlined in the high-level diagram below. Three types of on-premises clients are used:</p><ul><li>Non-GCP or regular clients, which should continue to access Google APIs and the Internet as usual. They should not be affected by any modifications made to the DNS server.</li><li>GCP clients in a production environment, where Google APIs resolution should be forwarded to Cloud DNS in a production VPC typically through an Interconnect connection.</li><li>GCP clients in a development environment, where Google APIs resolution should be forwarded to Cloud DNS in a development VPC, usually through an Interconnect or VPN connection.</li></ul><p>Each client sends a DNS query for a given domain, such as storage.googleapis.com. Depending on the type of client, the response to the query may vary because different clients use different resolvers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/798/0*-MVssu9jEQuBtR3N" /></figure><p>Cloud DNS will act as the resolver for GCP clients. We will configure private zones to CNAME *googleapis.com to restricted.googleapis.com with A records from the 199.36.153.4/30 range. To accommodate both the production and development environments, the restricted range will be split into two, with each environment receiving two IPs. This setup allows traffic routing from the on-premises network to the appropriate physical connectivity. Additionally, inbound server policies will be created in the production and development VPC networks, and the inbound forwarder IPs will be identified. Refer to the official <a href="https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid">Google Cloud documentation</a> for instructions on configuring the GCP side.</p><p>As shown in the picture, queries are matched based on the client subnet <strong>and</strong> the specified domain. If we didn’t include the domain of the request as a criterion, all GCP client queries for non-authoritative zones would be forwarded to Cloud DNS. While this setup could work, customers typically prefer to redirect only DNS GCP traffic to GCP.</p><h4>Configuration of Windows DNS Server</h4><p>Following is the configuration of the on-premises DNS server. It uses the DNS Policy objects described previously. To create these objects, run PowerShell as an administrator on the server.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*N6cPF_ZFXhFoif-GuHbseQ.png" /></figure><ul><li>Create ClientSubnets objects in the DNS server for production and development clients, to identify DNS queries from each other and from the rest of clients. Let’s assume the range 10.100.0.0/24 is used for production and 10.200.0.0/24 for development. Multiple IP ranges can be specified if necessary. Notably, ClientSubnets objects do not automatically replicate across DNS servers. Therefore, if there are multiple servers, they must be explicitly added to each server.</li></ul><pre>&gt; $dcs = (Get-ADDomainController -Filter *).Name<br>&gt; foreach ($dc in $dcs) {<br> Add-DnsServerClientSubnet -Name &quot;Prod&quot; -IPv4Subnet &quot;10.100.0.0/24&quot; -ComputerName $dc<br> Add-DnsServerClientSubnet -Name &quot;Dev&quot; -IPv4Subnet &quot;10.200.0.0/24&quot; -ComputerName $dc<br>}</pre><ul><li>Create recursion scopes for both production and development to send traffic to Cloud DNS. Utilize the inbound forwarder IP addresses (inbound server policies) specific to each environment. Let’s assume those forwarder IPs are 172.20.0.2 (prod) and 172.30.0.2 (dev).</li></ul><pre>&gt; Add-DnsServerRecursionScope -Name &quot;CloudDNSProd&quot; -Forwarder &quot;172.20.0.2&quot; -EnableRecursion $true<br>&gt; Add-DnsServerRecursionScope -Name &quot;CloudDNSDev&quot; -Forwarder &quot;172.30.0.2&quot; -EnableRecursion $true</pre><ul><li>Now comes a not well-documented aspect. The initial query for googleapis.com from a client populates the global cache of the Windows DNS server, <em>corrupting</em> the cache for subsequent clients. To prevent this issue, it’s necessary to <em>shard</em> or <em>partition</em> the DNS server’s global cache before implementing any query resolution policy for the recursion scopes. The partitioning process is based on creating zone scopes, as the cache is treated as an additional zone.<br>Create zone scopes for the DNS server cache, which will serve as cache shards dedicated to clients meeting the query resolution policies that we will create later.</li></ul><pre>&gt; Add-DnsServerZoneScope -ZoneName &quot;..cache&quot; -Name &quot;CloudDNSProdCache&quot;<br>&gt; Add-DnsServerZoneScope -ZoneName &quot;..cache&quot; -Name &quot;CloudDNSDevCache&quot;</pre><ul><li>Bind the cache zone scopes to query resolution policies for them to take effect. The policies will use FQDN and client subnet criteria to map to the zone scopes. As client subnets, query resolution policy objects do not replicate across DNS servers.</li></ul><pre>&gt; $dcs = (Get-ADDomainController -Filter *).Name<br>&gt; foreach ($dc in $dcs) {<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSProdCache&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Prod&quot; -Action ALLOW -ZoneScope &quot;CloudDNSProdCache&quot; -ZoneName &quot;..cache&quot; -ComputerName $dc<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSDevCache&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Dev&quot; -Action ALLOW -ZoneScope &quot;CloudDNSDevCache&quot; -ZoneName &quot;..cache&quot; -ComputerName $dc<br> }</pre><ul><li>Create query resolution policies with the same criteria to map to the recursion scopes created previously.</li></ul><pre>&gt; $dcs = (Get-ADDomainController -Filter *).Name<br>&gt; foreach ($dc in $dcs) {<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSProd&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Prod&quot; -Action ALLOW -ApplyOnRecursion -RecursionScope &quot;CloudDNSProd&quot; -ComputerName $dc<br> Add-DnsServerQueryResolutionPolicy -Name &quot;CloudDNSDev&quot; -Fqdn &quot;EQ,*.googleapis.com&quot; -ClientSubnet &quot;EQ,Dev&quot; -Action ALLOW -ApplyOnRecursion -RecursionScope &quot;CloudDNSDev&quot; -ComputerName $dc<br> }</pre><ul><li>To troubleshoot DNS issues, consider taking these additional steps:<br>- Clear the DNS caches. This may temporarily affect DNS performance, but it can resolve some issues.<br>- Inspect the caches to identify any irregularities or errors.</li></ul><pre>&gt; Clear-DnsServerCache<br>&gt; Clear-DnsServerCache -CacheScope &quot;CloudDNSProdCache&quot;<br>&gt; Clear-DnsServerCache -CacheScope &quot;CloudDNSDevCache&quot;<br>&gt; Show-DnsServerCache<br>&gt; Show-DnsServerCache -CacheScope &quot;CloudDNSProdCache&quot;<br>&gt; Show-DnsServerCache -CacheScope &quot;CloudDNSDevCache&quot;</pre><p>After applying these changes, different types of on-premises clients should now resolve *.googleapis.com to different IP addresses.</p><h3>Final notes</h3><ul><li>The shell commands show how to configure redirection for *.googleapis.com. However, this is usually not the only domain required for GCP services to work from on-premises networks. Domains such as accounts.google.com or gcr.io are also typically needed. You would need to add corresponding Add-DnsServerQueryResolutionPolicy commands with those FQDNs. A list of possible domains can be found in the <a href="https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid#config-choose-domain">documentation</a>.</li><li>Although it is possible to redirect only some services like Google Cloud Storage or BigQuery specifying domains like storage.googleapis.com, using *.googleapis.com makes the DNS configuration simpler and you can still control access to GCP services based on IAM roles (as you should).</li><li>There is no GUI to configure DNS Policies, only PowerShell. However, if you already automate your IT processes with PowerShell this approach can become an advantage, as it allows for easier integration and management of DNS settings.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=874aa2cc1a35" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/google-apis-from-windows-dns-server-874aa2cc1a35">Google APIs from Windows DNS Server</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Platform Technology Nuggets — April 1–15, 2024 Edition]]></title>
            <link>https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-april-1-15-2024-edition-6a6f751a0dbf?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6a6f751a0dbf</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-tech-nuggets]]></category>
            <category><![CDATA[tech-nuggets]]></category>
            <category><![CDATA[gcp-weekly]]></category>
            <dc:creator><![CDATA[Romin Irani]]></dc:creator>
            <pubDate>Wed, 17 Apr 2024 12:48:57 GMT</pubDate>
            <atom:updated>2024-04-17T12:48:56.927Z</atom:updated>
            <content:encoded><![CDATA[<h3>Google Cloud Platform Technology Nuggets — April 1–15, 2024 Edition</h3><p>Welcome to the April 1–15, 2024 edition of Google Cloud Platform Technology Nuggets.</p><p>Please feel free to give <a href="https://forms.gle/UAsAS7YLxYSBTNBy9">feedback</a> on this issue and share the <a href="https://gcptechnuggets.substack.com/">subscription form</a> with your peers.</p><h3>Google Cloud Next 2024</h3><p>Google Cloud Next 2024 was held from April 9–11, 2024 in Las Vegas and Generative AI was centrestage across the keynotes, sessions and more. It is difficult to put down all the blog posts that go into details of what was announced/demonstrated at the conference, but here are few links worth your time to catch up on the announcements:</p><ol><li><a href="https://www.youtube.com/watch?v=M-CzbTUVykg">Cloud Next ’24 Opening Keynote in under 14 minutes</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next24">Welcome to Google Cloud Next 24 </a>: A text form of key announcements made in the Keynote.</li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2024-wrap-up">All 218 things announced at Cloud NEXT ‘24</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/next24-day-1-recap">Day 1 Recap</a>: AI Agents for everyone</li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/next24-day-2-recap">Day 2 Recap</a>: building AI Agents</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nbrSlUJ-lKVOd-oY.jpg" /></figure><p>The sessions are all available on demand at the Cloud NEXT ’24 site. The only requirement is that you will need to register to view the sessions. Check out the <a href="https://cloud.withgoogle.com/next">entire session library</a> from the conference.</p><p>The other sections in this newsletter will highlight key announcements pertaining to those areas.</p><h3>Infrastructure</h3><p>Google Cloud’s Compute portfolio saw some key announcements vis-a-vis workload optimized infrastructure. This new introductions included:</p><ul><li>C4 and N4, new general-purpose VMs powered by 5th Generation Intel Xeon processors</li><li>Upcoming preview of native bare-metal C3 machine types</li><li>X4, Compute Engine’s new memory-optimized instances, now in preview.</li><li>Z3, Google Cloud’s first storage-optimized VM family</li><li>Google Axion Processor, a new Arm-based CPU</li></ul><p>Check out the <a href="https://cloud.google.com/blog/products/compute/compute-and-infrastructure-enhancements-at-next24">blog post</a> for more details.</p><p>Google Axion Processors, our first custom Arm®-based CPUs designed for the data center has been announced. Built using the Arm Neoverse™ V2 CPU, the Axion processors are expected to deliver significant performance improvements for general-purpose workloads like web and app servers, containerized microservices, open-source databases, in-memory caches, data analytics engines, media processing, CPU-based AI training and inferencing, and more. This compute option is likely to be available to customers later in the year. Check out the <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu">blog post</a> for more details.</p><p>Two new Open Source offerings in the area of AI Inferencing have been announced: JetStream and MaxDiffusion. Check out the <a href="https://cloud.google.com/blog/products/compute/accelerating-ai-inference-with-google-cloud-tpus-and-gpus">blog post</a>.</p><p>A suggested read in this space is the key enhancements that Google Cloud has been doing in its AI Hypercomputer architecture space. <a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">Check it out.</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FKeezDA92z9dNg0e_tuDzg.jpeg" /></figure><h3>Containers and Kubernetes</h3><p>Here is a fantastic post that highlights how Container Services on Google Cloud are set to efficiently serve the needs of Gen AI applications in the future. The post nicely highlights to get started with Cloud Run for an easy AI starting point, GKE for training and inference and more. <a href="https://cloud.google.com/blog/products/containers-kubernetes/google-clouds-container-platform-for-the-next-decade-of-ai">Check out</a> how Google Cloud is positioning these services to continue serving key workloads via containers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SJnc-mRDIYmm8qo8.png" /></figure><p>Looking to serve and deploy Gemma on GKE Standard as well as Autopilot clusters, check out a <a href="https://cloud.google.com/blog/products/containers-kubernetes/serving-gemma-on-google-kubernetes-engine-deep-dive">blog post</a> that highlights the key enhancements that have been made to GKE to help you do that. The post includes Integration with Hugging Face, Kaggle and Vertex AI Model Garden, GKE notebook experience using Colab Enterprise and A cost-efficient, reliable and low-latency AI inference stack.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CrH2GwwGyw-4Op48.png" /></figure><p>GKE Autopilot announced support for burstable workload support. As the blog post states, “Bursting allows your Pod to temporarily utilize resources outside of those resources that it requests and is billed for.” Another feature announced was Pods as small as 1/20th of a vCPU can be used now. Additionally, you can create any size of Pod you like between the minimum to the maximum size, for example, 59m CPU, 302m, 808m, 7682m etc. These features when combined together is a great way to run high-density workloads. Check out the <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot-burstable-workloads">blog post</a> for more details.</p><h3>Networking</h3><p>A big announcement was Cloud Service Mesh, a fully managed service mesh across all Google Cloud platform types. This is a single offering that combines Traffic Director’s control plane and Google’s open-source Istio-based service mesh, Anthos Service Mesh. Check out the <a href="https://cloud.google.com/blog/products/networking/introducing-cloud-service-mesh">blog post</a> that highlights what customers get from this new offering and the benefits.</p><p>Google Cloud’s next-generation cloud firewall offering is now available in GA. The product Cloud Firewall Plus, now called Cloud NGFW (Next Gen Firewall) is available in 3 tiers: Essentials, Standard and Enterprise. Check out the <a href="https://cloud.google.com/blog/products/identity-security/announcing-next-gen-firewall-enterprise-now-in-ga-next24">blog post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x_WPDHw3EDCADEAE.png" /></figure><p>If you’d like to get all the details on Whats New in Networking announced at Cloud NEXT 24, check out this <a href="https://cloud.google.com/blog/products/networking/whats-new-for-networking-at-next24">blog post</a>.</p><h3>Identity and Security</h3><p>Security saw some key updates at Google Cloud NEXT ’24. Gemini in Security Operations has a new assisted investigation feature, generally available at the end of this month, which will guide analysts through their workflow wherever they are in Chronicle Enterprise and Chronicle Enterprise Plus. Gemini recommends actions based on the context of an investigation, and can run searches and create detection rules to improve response times. You can also ask Gemini for the latest threat intelligence from Mandiant directly in-line — including any indicators of compromise found in their environment — and Gemini will navigate users to the most relevant pages in the integrated platform for deeper investigation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eedRxvVFALw-ex3gsOljSg.gif" /></figure><p>This is just one of several updates in the Security space. Check out the <a href="https://cloud.google.com/blog/products/identity-security/make-google-part-of-your-security-team-supercharged-by-ai-next24">blog post </a>for more security related announcements.</p><h3>Machine Learning</h3><p>Gemini is now right across Google Cloud. Whether you are a developer, security analyst, data analyst, operator, etc — you are bound to use Gemini now across various services. The names can get a bit confusing and hence it is important that you read this article first to get the product names right and what Gemini does to help work with specific areas. Check out the <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-for-google-cloud-is-here">post</a> that talks more about the diagram that you see below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KPJX_qfeG1CgBpgr.jpg" /></figure><p>Once you are done with the above post, do check out a <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">post</a> that highlights updates to Gemini, Imagen, Gemma and MLOps on Vertex AI. The updates include Gemini Pro 1.5 now available in Public Preview, Imagen’s new text-to-live image capabilities and more.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hcZHb8R7KfOVV4evOklVEg.png" /></figure><p>When it comes to using Generative AI in Enterprise applications, grounding these systems to the truth is essential and an absolute requirement. Google Cloud defined “enterprise truth” as the approach to grounding a foundation model in web information; enterprise data like databases and data warehouses; enterprise applications like ERP, CRM, and HR systems; and other sources of relevant information. And how does it propose to do that? Via a preview of <strong>Ground with Google Search</strong> in Vertex AI. Check out the <a href="https://cloud.google.com/blog/products/ai-machine-learning/grounding-gen-ai-in-enterprise-truth">post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WAKrEtso3C_yAYWwyP98Pg.gif" /></figure><p>Vertex AI Search and Conversation products, along with other developer tools is all coming together under one umbrella: Vertex AI Agent Builder. The key objective is to help build our AI Agents that are grounded in factuality and uses key features like Vertex AI Extensions, function calling and data connectors. Check out the <a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder">blog post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X61k2gIx7kirQjZC7GHuvQ.gif" /></figure><p>If you have been tracking Vertex AI Text Embeddings, at Cloud NEXT ’24, two new text embeddings were <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-announces-new-text-embedding-models">announced</a>:</p><ul><li>English only: text-embedding-preview-0409</li><li>Multilingual: text-multilingual-embedding-preview-0409</li></ul><h3>Storage and Data Transfer</h3><p>Before Cloud NEXT ’24 where several storage innovations were announced, there were a couple of Storage updates:</p><ul><li>You can now leverage the power of Google Cloud tags, including inheritance, to easily configure backup policies for Compute Engine VMs, ensuring consistent protection of your dynamic cloud environments. Check out the <a href="https://cloud.google.com/blog/products/storage-data-transfer/tags-support-in-backup-and-dr-service-simplifies-vm-protection">post</a>.</li><li>You can now look at meeting data retention compliance via the new Object retention lock. This feature helps you set and lock retention configurations on Cloud Storage objects, with a “retain until time.” This means that an object with an object retention lock can not be deleted or replaced until the retain until time has passed. Check out the <a href="https://cloud.google.com/blog/products/storage-data-transfer/introducing-cloud-storage-object-retention-lock">post</a>.</li></ul><p>At Cloud NEXT 24, with GenAI clearly dominating the mindspace, optimized storage solutions and features were announced to address the challenge of decreasing model load, training, and inference times while maximizing accelerator utilization. These included Cloud Storage FUSE with file caching, ParallelStore, Hyperdisk ML and more. Check out the <a href="https://cloud.google.com/blog/products/storage-data-transfer/storage-announcements-at-next24">blog post</a> for more announcements made.</p><h3>Databases</h3><p>Gemini in Databases was a key announcement in the area of Databases at NEXT. This meant AI-powered assistance to simplify all aspects of the database journey including developing, monitoring, optimizing, securing and migrating database-driven applications, vector support across more of the databases and more. Check out the <a href="https://cloud.google.com/blog/products/databases/whats-new-with-databases-at-next24">blog post</a> for more details.</p><p>Check out this additional <a href="https://cloud.google.com/blog/products/databases/a-deep-dive-into-gemini-in-databases">blog post</a> that dives into Gemini for Databases and gives a preview of the AI assisted features in Database Studio.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xZKzdHbRxBz6mxadnGzU-Q.gif" /></figure><p>Expanding on the above, <a href="https://cloud.google.com/blog/products/databases/understanding-natural-language-support-in-alloydb">AlloyDB for PostgreSQL got natural language support</a>. This feature is key in helping developers integrate real-time operational data into generative AI applications. AlloyDB also introduced parameterized secure views, a new kind of database view that locks down access to end-user data at the database level to help you protect against prompt injection attacks.</p><p>Also announced is the new ScaNN index for AlloyDB, bringing 12 years of Google research and innovation in approximate nearest neighbor algorithms to AlloyDB. This index is said to “deliver up to <strong>4x</strong> faster vector queries, up to <strong>8x</strong> faster index build times and typically a <strong>3–4x</strong> smaller memory footprint than the HNSW index in standard PostgreSQL”. At the moment, it is available in technology preview in AlloyDB Omni, and will become available in the AlloyDB for PostgreSQL managed service in Google Cloud later. Check out the <a href="https://cloud.google.com/blog/products/databases/understanding-the-scann-index-in-alloydb">blog post</a> for more details.</p><p>I love the statement “How do you store the entire Internet?” in the following <a href="https://cloud.google.com/blog/products/databases/bigtable-enhancements-at-next24">blog post</a>, that highlights the goal that the team set out with, designed BigTable for that and now 20 years later, continue to boost BigTable with features that are expected to bring it to a more widestream set of users.</p><p>Looking to do migrations of SQL Server to the Cloud SQL for SQL Server managed service? A preview of support for SQL Server migrations to Cloud SQL for SQL Server in Database Migration Service is now available. DMS is a fully managed serverless cloud service that performs database migrations with minimal downtime. Check out a blog post how Database Migration Service works.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QaK7NlTLnPGyFIfK.jpg" /></figure><p>Speaking of migrations, Gemini in Databases and specifically when it comes to migrations can help out in multiple ways. How about explainability when it comes to understanding existing queries that you need to migrate, schema conversions and more. Check out an interesting <a href="https://cloud.google.com/blog/products/databases/gemini-helps-migrate-oracle-to-postgresql-on-google-cloud">blog post</a> that highlights how you can use Gemini to help migrate Oracle to Cloud SQL for PostgreSQL on Google Cloud.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dzgjz_qXDoAhEj7I.jpg" /></figure><p>Private Service Connect is a capability of Google Cloud networking that allows consumers to access managed services privately from inside their VPC network. Private Service Connect is now fully integrated with <a href="https://cloud.google.com/sql">Cloud SQL</a>, Google Cloud’s fully managed database service for PostgreSQL, MySQL, and SQL Server. Check out the <a href="https://cloud.google.com/blog/products/databases/private-service-connect-for-cloud-sql-databases">blog post</a> for details on how to get started, configuring Private Service Connect and deployment architectures.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/520/0*xMM66CL5v2XhWshS.png" /></figure><p>Memorystore for Redis Cluster saw some major announcements at Cloud NEXT:</p><ul><li>Public preview of data persistence for both RDB (Redis Database) and AOF (Append Only File)</li><li>General availability of new nodes types of 1.4 GB, 6.5 GB and 58 GB</li><li>General availability of ultra-fast vector search on Memorystore for Redis</li><li>Public preview of new configuration options</li></ul><p>Check out the <a href="https://cloud.google.com/blog/products/databases/memorystore-for-redis-cluster-updates-at-next24">blog post</a> for more details.</p><h3>Data Analytics</h3><p>If you are looking at scanning just the key announcements, check out this <a href="https://cloud.google.com/blog/products/data-analytics/data-analytics-at-next24">post</a>. Now let’s dive into some of those announcements.</p><p>BigQuery is now the single, AI-ready data analytics platform. A single product that helps you manage structured data in BigQuery tables, unstructured data like images, audience and documents, and streaming workloads, all with the best price-performance. Dive into this <a href="https://cloud.google.com/blog/products/data-analytics/bigquery-is-a-unified-ai-ready-data-analytics-platform">post</a> to understand how.</p><p>Duet AI in BigQuery is now Gemini in BigQuery. Key assistance is now available in AI augmented data preparation that helps users to cleanse and wrangle their data. Another interesting feature is the new semantic search capabilities to help you pinpoint the most relevant tables for your tasks. Leveraging the metadata and profiling information of these tables from <a href="https://cloud.google.com/dataplex?e=48754805&amp;hl=en">Dataplex</a>, Gemini in BigQuery surfaces relevant, executable queries that you can run with just one click. Check out the <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24">post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FWNlIa3m4bAkWkJfyID_wg.gif" /></figure><p>The new <strong>BigQuery data canvas </strong>provides<strong> </strong>a reimagined natural language-based experience for data exploration, curation, wrangling, analysis, and visualization, allowing you to explore and scaffold your data journeys in a graphical workflow that mirrors your mental model. Check out this <a href="https://cloud.google.com/blog/products/data-analytics/get-to-know-bigquery-data-canvas">post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yA1LiRSthQ49CcKBuNvWvw.gif" /></figure><p>The deep integration now of Gemini models into Looker is also going to open up multiple possibilities in the realm of Looker as a BI Platform. Check out this <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-looker-at-next24">post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rbu-dumyV74OC-w6Sg43Mg.gif" /></figure><p>A couple of other posts that are interesting and which give a glimpse into how BigQuery and GenAI capabilities have merged are:</p><ul><li><a href="https://cloud.google.com/blog/products/data-analytics/how-to-use-gemini-pro-vision-in-bigquery">Analyze images and videos in BigQuery using Gemini 1.0 Pro Vision</a></li><li><a href="https://cloud.google.com/blog/products/data-analytics/bigquery-multimodal-embeddings-generation">Introducing multimodal and structured data embedding support in BigQuery</a></li></ul><h3><strong>De</strong>velopers and Practitioners</h3><p>Duet AI for Developers got rebranded as Gemini Code Assist, which is set to supercharge your development workflow. There were key demos provided at Cloud NEXT Developer Keynote that showed extended capabilities that include full codebase awareness, increase in local context, code transformation support (refactoring, etc), connecting to existing source repositories and various partner integrations. Check out the <a href="https://cloud.google.com/blog/products/application-development/gen-ai-and-app-development-tools-and-partnerships">blog post</a> that highlights each of these areas.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HQ5Rd6TSsZwda3tlvHe6Vw.gif" /></figure><p><a href="https://cloud.google.com/blog/products/databases/firestore-launches-at-next24">Firestore saw some key announcements at NEXT 24</a>. These include:</p><ul><li>Use Gemini Code Assist in your favorite Integrated Development Environment (IDE) to use natural language to define your Firestore data models and write queries.</li><li>Firestore now has built-in support for vector search using exact nearest neighbors, the ability to automatically generate vector embeddings using popular embedding models via a turn-key extension, and integrations with popular generative AI libraries such as LangChain and LlamaIndex. Check out a detailed article on using <a href="https://cloud.google.com/blog/products/databases/get-started-with-firestore-vector-similarity-search">Firestore vector similarity search</a>.</li><li>Firestore now supports Customer Managed Encryption Keys (CMEK) in preview, which allows you to encrypt data stored at-rest using your own specified encryption key.</li><li>Retain daily backups using Firestore’s Scheduled Backup feature for up to 98 days, up from seven days.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0rbVbd1WHXzgpdoyJzT0sA.gif" /></figure><p>An interesting new service has been announced at Cloud NEXT 24 : App Hub. Think of one or more applications that you have deployed on Google Cloud. It is a challenge to understand visualizing that application in terms of not just the cloud resources that it uses but dependencies on other services, across project services and more. App Hub is targetted to address that by introducing abstractions in the form of Applications, Workloads and Services. It is able to injest automatically key Google Cloud services that Applications would use and then build out a dependency graph that is kept updated all the time. The current resources that it supports are various Load Balancing services (Services) and Compute Engine MIGs (Workloads). As the service grows to support GKE and Cloud Run, it could get very useful. Check out the <a href="https://cloud.google.com/blog/products/application-development/introducing-app-hub">blog post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/837/0*5XM3_D4twH2WlPqq.png" /></figure><p>If you are using Apigee, Gemini Code Assist is making its way into the product suite to help ease the task of developing API specifications, API integrations and more. For example, when it comes to building out APIs, you can build out an API Specification using the Gemini Code Assist integration inside of the Cloud Code VS Code extension. These specifications can then be published to the API Hub. Not just that but Gemini offers step-by-step guidance for adding new policy configurations while creating an API proxy. Lastly, Gemini also provides explanations for your existing configurations, reducing the learning curve during updates and maintenance. Check out the <a href="https://cloud.google.com/blog/products/api-management/gemini-code-assist-for-apigee-api-management">post</a> for more details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*F3_-Swqf_VlNZ1NguXo5xw.gif" /></figure><h3>Learn Google Cloud</h3><p>When it comes to service discovery and DNS resolution in your GKE clusters, you have a choice with kube-dns, Cloud DNS, etc. Deep dive into this options via this informative <a href="https://cloud.google.com/blog/products/networking/understanding-dns-options-for-gke">blog post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4jraBWptrpXidDTE.png" /></figure><h3>Stay in Touch</h3><p>Have questions, comments, or other feedback on this newsletter? Please send <a href="https://forms.gle/UAsAS7YLxYSBTNBy9">Feedback</a>.</p><p>If any of your peers are interested in receiving this newsletter, send them the <a href="https://gcptechnuggets.substack.com/">Subscribe</a> link.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6a6f751a0dbf" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-april-1-15-2024-edition-6a6f751a0dbf">Google Cloud Platform Technology Nuggets — April 1–15, 2024 Edition</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Master Data Management Simplified: Match & Merge with Generative AI!]]></title>
            <link>https://medium.com/google-cloud/master-data-management-simplified-match-merge-with-generative-ai-35fd35d306c2?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/35fd35d306c2</guid>
            <category><![CDATA[mdm]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai]]></category>
            <dc:creator><![CDATA[Abirami Sukumaran]]></dc:creator>
            <pubDate>Wed, 17 Apr 2024 01:27:38 GMT</pubDate>
            <atom:updated>2024-04-17T01:27:38.029Z</atom:updated>
            <content:encoded><![CDATA[<p>It is my dream to accelerate some of the tedious MDM processes with Generative AI, Embeddings, Vector Search and more… Read on.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/740/1*zccJwrLqwmYQ38V2yeCtRw.png" /><figcaption>Image showing a bike station with docked bikes to represent the use case</figcaption></figure><h3>Introduction</h3><p>At its core, Master Data Management (MDM) is about creating a single, reliable source of truth for your organization’s most critical data. Think of MDM as a meticulously curated library where every book (data point) is correctly labeled, up-to-date, and easy to find. Implementing robust MDM has always been a strategic necessity, but it often comes with complexities and resource demands. This is where the transformative power of Generative AI, particularly models like Gemini 1.0 Pro, Gemini 1.0 Pro Vision, Gemini 1.5 Pro, comes into play.</p><blockquote><strong>In this article</strong>, we will demonstrate how Gemini 1.0 Pro simplifies master data management applications like enrichment and deduplication, for the citibike_stations data available in the BigQuery public dataset. For this, we will use:<br>1. BigQuery public dataset bigquery-public-data.new_york_citibike.<br>2. Gemini Function Calling (a Java Cloud Function that gets the address information using the reverse Geocoding API for the coordinates available with the citibike_stations data) that we have already created in <a href="https://medium.com/google-cloud/using-gemini-function-calling-in-java-for-deterministic-generative-ai-responses-4c86a5ab80a9">one</a> of our previous articles.<br>3. Vertex AI Embeddings API and Vector Search in BigQuery to identify duplicates.</blockquote><h3>Master Data Management (MDM): The Foundation for Data-Driven Decisions</h3><p>The key elements of master data are business entities (like customers, products, suppliers, locations, etc. which are the nouns that your business revolves around), identifiers (Unique identifiers ensure each entity is distinct and traceable across systems), attributes (These are the characteristics that describe each entity e.g. a customer’s address, a product’s price etc.). I am going to take a little space here with it because understanding the importance of MDM is paramount in the generative AI era more than ever. Why MDM Matters:</p><ul><li><strong>Adds clarity to data:</strong></li></ul><p>Without MDM, businesses struggle with fragmented data scattered across various systems. This leads to inconsistencies, duplicates, and sometimes ambiguous understanding of relationships between data points (e.g. is “Abirami Sukumaran” in one system the same customer as “Abirami S.” in another?).</p><ul><li><strong>Helps in decision making:</strong></li></ul><p>Accurate, unified master data is the foundation of informed business decisions. It answers questions like — Who are our most valuable customers? Which products are underperforming? What suppliers provide the best value?</p><ul><li><strong>Serves data as a Product:</strong></li></ul><p>MDM is essential for treating data as a valuable asset. Clean, integrated, and up-to-date master data forms the basis for building insightful data products that drive business outcomes.</p><p><strong>Let me paint a picture to help you understand master data better by comparing it with transactional data: </strong>Transactional data captures individual events (a purchase, a shipment etc.). Master data provides the context for those events by defining the entities involved. For example, a sales transaction links to master data for the customer, product, and salesperson. Without MDM, data tends to remain in silos hindering a holistic view, prone to quality issues and relatively heavy resource-spend on data reconciliation and consolidation on demand.</p><h3>The powerful synergy of MDM and Generative AI</h3><p>Below are some of the industry grade applications and advantages of empowering and simplifying MDM with Generative AI:</p><ul><li><strong>Automated Data Cleansing:</strong> Gemini 1.0 Pro can intelligently identify and rectify inconsistencies, duplicates, and errors in master data, significantly reducing manual effort.</li><li><strong>Intelligent Matching:</strong> Leveraging its language understanding capabilities, Generative AI capabilities, embeddings and Vector Search, come together to accurately match and merge records from disparate sources, even with variations in formatting or naming conventions.</li><li><strong>Contextual Enrichment:</strong> Generative AI models can generate missing attributes or provide additional context to master data, enhancing its value for analysis and decision-making.</li><li><strong>Adaptive Learning</strong>: You can leverage fine tuning and reinforcement learning to enable generative AI to continuously improve its understanding of your business entities and relationships, leading to more accurate and efficient MDM over time.</li></ul><h3>High Level Flow Diagram</h3><p>This diagram represents the flow of data and steps involved in the implementation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1001/1*pMW7YoxbqYbJjYXgucff9g.png" /><figcaption>High level flow of the use case</figcaption></figure><h3>Demo</h3><p>The steps loosely are as follows:</p><ol><li>Create a BigQuery dataset for the use case. Create a landing table with data from the public dataset table bigquery-public-data.new_york_citibike.citibike_stations.</li><li>Make sure the Cloud Function that includes Gemini Function Calling for address standardization is deployed.</li><li>Store the enriched address data in the landing tables (from 2 sources for demo purpose).</li><li>Invoke Vertex AI Embeddings API from BigQuery on the address data.</li><li>Use BigQuery Vector Search to identify duplicate records.</li></ol><h3>Setup</h3><ol><li>In the <a href="https://console.cloud.google.com/">Google Cloud Console</a>, on the project selector page, select or create a Google Cloud <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects">project</a>.</li><li>Make sure that billing is enabled for your Cloud project. Learn how to <a href="https://cloud.google.com/billing/docs/how-to/verify-billing-enabled">check if billing is enabled on a project</a>.</li><li>You will use Cloud Shell, a command-line environment running in Google Cloud that comes preloaded with bq. From the Cloud Console, click Activate Cloud Shell on the top right corner.</li><li>Enable necessary APIs for this implementation if you haven’t already. Alternative to the gcloud command is through the console using this <a href="https://pantheon.corp.google.com/apis/enableflow?apiid=cloudfunctions.googleapis.com,run.googleapis.com,bigquery.googleapis.com,bigqueryconnection.googleapis.com,aiplatform.googleapis.com">link</a>.</li></ol><h3>Step 1: Create BigQuery Dataset and External Connection</h3><p>BigQuery <a href="https://cloud.google.com/bigquery/docs/datasets-intro">dataset</a> is a container for all the tables and objects for your application. BigQuery <a href="https://cloud.google.com/bigquery/docs/remote-functions#create_a_connection">connection</a> is used to interact with your Cloud Function. In order to create a remote function, you must create a BigQuery connection. Let’s begin with creating the dataset and the connection.</p><ol><li>From the Google Cloud Console, go to the <a href="https://console.cloud.google.com/bigquery">BigQuery page</a> and click the 3 vertical dots icon next to your project id. From the list of options, select “Create data set”.</li><li>In Create data set pop up, enter the data set ID “mdm_gemini” with the region set to the default value “US (multiple regions…)”</li><li><a href="https://cloud.google.com/bigquery/docs/remote-functions#create_a_connection">BigLake Connection</a> allows us to connect the external data source while retaining fine-grained BigQuery access control and security, which in our case is the Vertex AI Gemini Pro API. We will use this connection to access the model from BigQuery via the Cloud Function. Follow steps below to create the BigLake Connection:</li></ol><p>a. Click ADD on the Explorer pane of the BigQuery page:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/0*mqX5rzEHWjkzEQIE" /><figcaption>ADD button to create connection</figcaption></figure><p>b. Click Connections to external data sources in the sources page.</p><p>c. Enter external data source details as below in the pop up that shows up and click CREATE CONNECTION:</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*b31hiO4ynbDLRrXWEFF4aQ.png" /><figcaption>Create Connection</figcaption></figure><p>d. Once the connection is created, go to the connection configuration page and copy the Service account ID for access provisioning:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xh8uToOATp-TmC5O" /><figcaption>Connection Info</figcaption></figure><p>e. Open <a href="https://console.cloud.google.com/iam-admin/iam">IAM and admin</a> page, click GRANT ACCESS, enter the service account id in the new principals tab and roles as shown below and click SAVE.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fCExrL0trAtqirew" /><figcaption>Grant Access</figcaption></figure><h3>Step 2: Deploy Function Calling (Java Cloud Function)</h3><ol><li>Clone the github <a href="https://github.com/AbiramiSukumaran/GeminiFunctionCalling">repo</a> from your Cloud Shell Terminal and change the YOUR_API_KEY and YOUR_PROJECT_ID with your values</li></ol><pre>git clone https://github.com/AbiramiSukumaran/GeminiFunctionCalling</pre><p>2. Go to Cloud Shell terminal, navigate into the newly cloned project directory “GeminiFunctionCalling” and execute the below statement build and deploy the Cloud Function:</p><pre>gcloud functions deploy gemini-fn-calling --gen2 --region=us-central1 --runtime=java11 --source=. --entry-point=cloudcode.helloworld.HelloWorld --trigger-http</pre><p>The result for this deploy command would be a REST URL in the format as below :</p><p><a href="https://us-central1-abis-345004.cloudfunctions.net/gemini-bq-fn"><strong>https://us-central1-</strong>YOUR_PROJECT_ID<strong>.cloudfunctions.net/gemini-fn-calling</strong></a></p><p>3. Test this Cloud Function by running the following command from the terminal:</p><pre>gcloud functions call gemini-fn-calling - region=us-central1 - gen2 - data &#39;{&quot;calls&quot;:[[&quot;40.714224,-73.961452&quot;]]}&#39;</pre><p>Response for a random sample prompt:</p><pre>  &#39;{&quot;replies&quot;:[&quot;{ \&quot;DOOR_NUMBER\&quot;: \&quot;277\&quot;, \&quot;STREET_ADDRESS\&quot;: \&quot;Bedford Ave\&quot;, \&quot;AREA\&quot;:<br>  null, \&quot;CITY\&quot;: \&quot;Brooklyn\&quot;, \&quot;TOWN\&quot;: null, \&quot;COUNTY\&quot;: \&quot;Kings County\&quot;, \&quot;STATE\&quot;:<br>  \&quot;NY\&quot;, \&quot;COUNTRY\&quot;: \&quot;USA\&quot;, \&quot;ZIPCODE\&quot;: \&quot;11211\&quot;, \&quot;LANDMARK\&quot;: null}}```&quot;]}&#39;</pre><p>The request and response parameters of this Cloud Function are implemented in a way that is compatible with BigQuery’s remote function invocation. It can be directly consumed from BigQuery data in-place. It means that if your data input (lat and long data) lives in BigQuery then you can call the remote function on the data and get the function response which can be stored or processed within BigQuery directly.</p><p>4. Run the following DDL from BigQuery to create a remote function that invokes this deployed Cloud Function:</p><pre> CREATE OR REPLACE FUNCTION <br>  `mdm_gemini.MDM_GEMINI` (latlng STRING) RETURNS STRING<br>  REMOTE WITH CONNECTION `us.gemini-bq-conn`<br>  OPTIONS (<br>    endpoint = &#39;https://us-central1-YOUR_PROJECT_ID.cloudfunctions.net/gemini-fn-calling&#39;, max_batching_rows = 1<br>  );</pre><p><strong>WORKAROUND</strong></p><p>If you do not have the necessary key or Cloud Function deployed, feel free to jump to the landing table directly by exporting the data from the csv into your new BigQuery dataset mdm_gemini using the following command in the Cloud Shell Terminal. <strong>Remember</strong> to download the file <a href="https://github.com/AbiramiSukumaran/MDMwithGemini/blob/main/CITIBIKE_STATIONS.csv">CITIBIKE_STATIONS.csv</a> from the <a href="https://github.com/AbiramiSukumaran/MDMwithGemini">repo</a> into your Cloud Shell project folder and navigate into that folder before executing the following command:</p><pre>bq load --source_format=CSV --skip_leading_rows=1 mdm_gemini.CITIBIKE_STATIONS ./CITIBIKE_STATIONS.csv \ name:string,latlng:string,capacity:numeric,num_bikes_available:numeric,num_docks_available:numeric,last_reported:timestamp,full_address_string:string</pre><h3>Step 3: Create Table and Enrich Address data</h3><p>a. If you have used the WORKAROUND approach from the last step, you can skip this step, since you have already created the table there. If NOT, proceed to the running the following DDL in BigQuery SQL Editor:</p><pre>CREATE TABLE mdm_gemini.CITIBIKE_STATIONS as (<br>select  name, latitude || &#39;,&#39; || longitude as latlong, capacity, num_bikes_available, num_docks_available,last_reported,<br>&#39;&#39; as full_address_string <br> from bigquery-public-data.new_york_citibike.citibike_stations) ;</pre><p>Let’s enrich the address data by invoking the remote function on the latitude and longitude coordinates available in the table. Please note that we will update this only for data reported for the year 2024 and where number of bikes available &gt; 0 and capacity &gt; 100:</p><pre>update `mdm_gemini.CITIBIKE_STATIONS` <br>set full_address_string = `mdm_gemini.MDM_GEMINI`(latlong) <br>where EXTRACT(YEAR FROM last_reported) = 2024 and num_bikes_available &gt; 0 and capacity &gt; 100;</pre><p>b. Do not skip this step even if you used the WORKAROUND approach in the last step. This is where we will create a second source of bike station location data for the purpose of this use case. Afterall, MDM is bringing data from multiple sources together and identifying the golden truth.</p><p>Run the following DDLs in BigQuery SQL Editor for creating the second source of location data with 2 records in it. Let’s name this table mdm_gemini.CITIBIKE_STATIONS_SOURCE2 and insert 2 records into it:</p><pre>CREATE TABLE mdm_gemini.CITIBIKE_STATIONS_SOURCE2 (name STRING(55), address STRING(1000), embeddings_src ARRAY&lt;FLOAT64&gt;);<br><br>insert into mdm_gemini.CITIBIKE_STATIONS_SOURCE2 VALUES (&#39;Location broadway and 29&#39;,&#39;{ &quot;DOOR_NUMBER&quot;: &quot;1593&quot;, &quot;STREET_ADDRESS&quot;: &quot;Broadway&quot;, &quot;AREA&quot;: null, &quot;CITY&quot;: &quot;New York&quot;, &quot;TOWN&quot;: null, &quot;COUNTY&quot;: &quot;New York County&quot;, &quot;STATE&quot;: &quot;NY&quot;, &quot;COUNTRY&quot;: &quot;USA&quot;, &quot;ZIPCODE&quot;: &quot;10019&quot;, &quot;LANDMARK&quot;: null}&#39;, null);<br><br>insert into mdm_gemini.CITIBIKE_STATIONS_SOURCE2 VALUES (&#39;Allen St &amp; Hester&#39;,&#39;{ &quot;DOOR_NUMBER&quot;: &quot;36&quot;, &quot;STREET_ADDRESS&quot;: &quot;Allen St&quot;, &quot;AREA&quot;: null, &quot;CITY&quot;: &quot;New York&quot;, &quot;TOWN&quot;: null, &quot;COUNTY&quot;: &quot;New York County&quot;, &quot;STATE&quot;: &quot;NY&quot;, &quot;COUNTRY&quot;: &quot;USA&quot;, &quot;ZIPCODE&quot;: &quot;10002&quot;, &quot;LANDMARK&quot;: null}&#39;, null);</pre><h3>Step 4: Generate Embeddings for Address Data</h3><p>Embeddings are high-dimensional numerical vectors that represent a given entity, like a piece of text or an audio file. Machine learning (ML) models use embeddings to encode semantics about such entities to make it easier to reason about and compare them. For example, a common operation in clustering, classification, and recommendation models is to measure the distance between vectors in an embedding space to find items that are most semantically similar. The Vertex AI text-embeddings API lets you create a text embedding using Generative AI on Vertex AI. Text embeddings are numerical representations of text that capture relationships between words and phrases. Read more about Vertex AI Text Embeddings <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings">here</a>.</p><p>Run the below DDL to create a remote model for Vertex AI text embeddings API:</p><pre>CREATE OR REPLACE MODEL `mdm_gemini.CITIBIKE_STATIONS_ADDRESS_EMB`<br>REMOTE WITH CONNECTION `us.gemini-bq-conn`<br>OPTIONS (ENDPOINT = &#39;textembedding-gecko@latest&#39;);</pre><p>Now that the remote embeddings model is ready, let’s generate embeddings for the first source and store it in a table. You can store the embeddings result field in the same mdm_gemini.CITIBIKE_STATIONS table as before, but I am choosing to create a new one for clarity:</p><pre>CREATE TABLE `mdm_gemini.CITIBIKE_STATIONS_SOURCE1` AS (<br>SELECT *<br>FROM ML.GENERATE_EMBEDDING(<br>  MODEL `mdm_gemini.CITIBIKE_STATIONS_ADDRESS_EMB`,<br>  ( select name, full_address_string as content from `mdm_gemini.CITIBIKE_STATIONS` <br>  where full_address_string is not null )<br>   )<br>);</pre><p>Let’s generate embeddings for address data in table CITIBIKE_STATIONS_SOURCE2:</p><pre>update `mdm_gemini.CITIBIKE_STATIONS_SOURCE2` a set embeddings_src =<br>(<br>SELECT  ml_generate_embedding_result<br>FROM ML.GENERATE_EMBEDDING(<br>  MODEL `mdm_gemini.CITIBIKE_STATIONS_ADDRESS_EMB`,<br>  ( select name, address as content from `mdm_gemini.CITIBIKE_STATIONS_SOURCE2` ))<br>where name = a.name) where name is not null;<br>This should create embeddings for the second source, note that we have created the embeddings field in the same table CITIBIKE_STATIONS_SOURCE2.</pre><p>To visualize the embeddings are generated for the source data tables 1 and 2, run the below queries:</p><pre>select name,address,embeddings_src from `mdm_gemini.CITIBIKE_STATIONS_SOURCE2`;<br>select name,content,ml_generate_embedding_result from `mdm_gemini.CITIBIKE_STATIONS_SOURCE1`;</pre><p>Let’s go ahead and perform vector search to identify duplicates.</p><h3>Step 5: Vector Search for Flagging Duplicate Addresses</h3><p>In this step, we will search the address embeddings ml_generate_embedding_result column of the `mdm_gemini.CITIBIKE_STATIONS_SOURCE1` table for the top 2 embeddings that match each row of data in the embeddings_src column of the `mdm_gemini.CITIBIKE_STATIONS_SOURCE2` table:</p><pre>select query.name name1,base.name name2, <br>/* (select address from mdm_gemini.CITIBIKE_STATIONS_SOURCE2 where name = query.name) content1, base.content content2, */<br> distance<br> from VECTOR_SEARCH(<br>  TABLE mdm_gemini.CITIBIKE_STATIONS_SOURCE1,<br>  &#39;ml_generate_embedding_result&#39;,<br>  (SELECT * FROM mdm_gemini.CITIBIKE_STATIONS_SOURCE2),<br>  &#39;embeddings_src&#39;,<br>  top_k =&gt; 2 <br>) where query.name &lt;&gt; base.name<br>order by distance desc;</pre><p><strong>Table that we are querying:</strong> mdm_gemini.CITIBIKE_STATIONS_EMBEDDINGS on the field ‘ml_generate_embedding_result’</p><p><strong>Table that we use as base:</strong> mdm_gemini.CITIBIKE_STATIONS_SOURCE2 on the field ‘embeddings_src’</p><p><strong>top_k:</strong> specifies the number of nearest neighbors to return. The default is 10. A negative value is treated as infinity, meaning that all values are counted as neighbors and returned.</p><p><strong>distance_type:</strong> specifies the type of metric to use to compute the distance between two vectors. Supported distance types are EUCLIDEAN and COSINE. The default is EUCLIDEAN.</p><p>The result of the query is as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EQJgLI91pzmTsXnW" /><figcaption>Result Set</figcaption></figure><p>As you can see, we have listed 2 nearest neighbors (in other words, closest duplicates) for the 2 rows in CITIBIKE_STATIONS_SOURCE2 from CITIBIKE_STATIONS_SOURCE1. Since the distance_type is unspecified, it assumes that it is EUCLIDEAN and the distance is read as the distances in address TEXT values between the two sources, lowest being the most similar address texts.</p><p>Let’s set distance_type to COSINE:</p><pre>select query.name name1,base.name name2, <br>/* (select address from mdm_gemini.CITIBIKE_STATIONS_SOURCE2 where name = query.name) content1, base.content content2, */<br> distance<br> from VECTOR_SEARCH(<br>  TABLE mdm_gemini.CITIBIKE_STATIONS_SOURCE1,<br>  &#39;ml_generate_embedding_result&#39;,<br>  (SELECT * FROM mdm_gemini.CITIBIKE_STATIONS_SOURCE2),<br>  &#39;embeddings_src&#39;,<br>  top_k =&gt; 2,distance_type =&gt; &#39;COSINE&#39;<br>) where query.name &lt;&gt; base.name<br>order by distance desc;</pre><p>Result:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/905/0*lkeIwXC98QwSoqoq" /><figcaption>Result set</figcaption></figure><p>Both queries (of both distance types) are ordered by distance DESCENDING which means we want to list the results in the order of decreasing distance. But you will notice that the second query’s distance order is reversed. Guess why?</p><p>Yes!! You got it right! When we say COSINE, it returns the similarity. So bigger the number, closer the similarity and hence lesser the distance. In EUCLIDEAN, bigger the number, farther the distance of values (in this case text).</p><blockquote><strong>Tips to understand the difference and applications of EUCLIDEAN and COSINE:</strong></blockquote><blockquote><strong>Euclidean Distance</strong> measures the straight-line distance between two points in a multi-dimensional space and<strong> Cosine Similarity</strong> measures the cosine of the angle between two vectors.</blockquote><blockquote>Scale Invariance: Cosine similarity is not affected by the magnitude (length) of vectors, only their direction. This is useful when comparing documents of different lengths or user preferences with varying intensities.</blockquote><blockquote>Curse of Dimensionality: Euclidean distance can become less informative in very high-dimensional spaces, while cosine similarity tends to hold up better.</blockquote><blockquote>When to Use Which:<br>Similar Scale, Absolute Differences Matter: Euclidean distanceDifferent Scales, Direction Matters More: Cosine similarity</blockquote><blockquote>5. Example:<br>Imagine two users’ movie ratings:<br>User 1: [5, 4, 3] (action, comedy, drama)<br>User 2: [10, 8, 6] (same genres, but rated higher)</blockquote><blockquote>Euclidean distance would be large due to the difference in rating scale.<br>Cosine similarity would be high, indicating similar taste despite the rating difference.</blockquote><blockquote>Please note in our example, we have used location text similarity — it doesn’t mean we are tracing the absolute geocoding distance between the 2 locations by value. We are only finding their similarity by the address text values.</blockquote><p>I built a UI around the Vector Search data from BigQuery for visualizing nearest neighbors based on their similarity in address string, NOT THE PHYSICAL DISTANCE, for a slightly larger dataset in a simple Java Spring Boot application and deployed it in Cloud Run. I created a <a href="https://github.com/AbiramiSukumaran/mdm_gemini_web">sample app</a>, edit the SQL in the controller class to match your distance results table:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X5vJaFouSO6L2e97C-fnoA.gif" /><figcaption>A demo of Bike Station Location UI that lists stations with similar addresses (not actual distance)</figcaption></figure><h3>Conclusion</h3><p>This project has demonstrated the power of using Gemini 1.0 Pro and Function Calling in transforming a few MDM activities into simplified yet powerful, deterministic and reliable generative AI capabilities. Now that you know, feel free to identify other ways of implementing the same use case or other MDM functionalities. Are there datasets you could validate, information gaps you could fill, or tasks that could be automated with structured calls embedded within your generative AI responses? Here is the link to the <a href="https://github.com/AbiramiSukumaran/MDMwithGemini">repo</a> and for further reading, refer to the documentation for <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>, <a href="https://cloud.google.com/bigquery/docs/remote-functions">BigQuery Remote Functions</a>, and <a href="https://cloud.google.com/functions">Cloud Functions</a>, <a href="https://cloud.google.com/bigquery/docs/vector-index-text-search-tutorial#create_the_remote_model_for_text_embedding_generation">Embeddings</a>, <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/search_functions#vector_search">Vector Search</a> for more in-depth guidance.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=35fd35d306c2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/master-data-management-simplified-match-merge-with-generative-ai-35fd35d306c2">Master Data Management Simplified: Match &amp; Merge with Generative AI!</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps…]]></title>
            <link>https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/c9ce870dcb99</guid>
            <category><![CDATA[google-apps-script]]></category>
            <category><![CDATA[google-workspace]]></category>
            <category><![CDATA[google-spreadsheets]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-sheets]]></category>
            <dc:creator><![CDATA[Kanshi Tanaike]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 09:29:21 GMT</pubDate>
            <atom:updated>2024-04-16T09:29:21.906Z</atom:updated>
            <content:encoded><![CDATA[<h3>Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps Script</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*rqD6sT_95bXtlahR.jpg" /></figure><h3>Abstract</h3><p>Consolidate scattered cell references (A1Notation) in Google Sheets for efficiency. This script helps select cells by background color or update values/formats, overcoming limitations of large range lists.</p><h3>Introduction</h3><p>When working with Google Spreadsheets, there might be a scenario where you need to process scattered A1Notations (cell addresses in the format “A1”). This could involve selecting cells with specific background colors, updating cell values, or modifying cell formats.</p><p>One approach to handle scattered A1Notations is to create a range list containing the individual cell coordinates and activate it. However, this method becomes inefficient when dealing with a large number of cells due to the high processing cost associated with activating each cell individually.</p><p>To address this limitation, consolidating scattered A1Notations into continuous ranges offers a significant performance improvement. While a previous report discussed expanding consolidated A1Notations back into individual cells Ref: <a href="https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/">https://tanaikech.github.io/2020/04/04/updated-expanding-a1notations-using-google-apps-script/</a>, consolidating them for processing efficiency had not been covered.</p><p>During the development of a script to achieve consolidation, it became apparent that existing solutions were not straightforward. To ensure clarity and facilitate debugging in the initial stages, the script was created by splitting each step into smaller, testable functions. While this approach might appear less elegant, it prioritizes understandability during the development process.</p><p>The provided script offers a solution for consolidating scattered A1Notations, as illustrated in the demonstration image. By consolidating the notations, the script can efficiently select cells with a specific background color, reducing the overall processing cost.</p><p>Furthermore, the script’s functionality can be extended to other use cases. For instance, it can be used to update the values or formats of scattered cells across the spreadsheet.</p><h3>Principle</h3><p>In this script, the process of consolidating A1Notations into rectangles is achieved by calculating the maximum rectangle size for all given A1Notations. This essentially combines scattered A1Notations into a single, most efficient rectangle.</p><p>The sample situation is as follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*53lMnCPL-j25cyO-.png" /></figure><p>The cells with a red background color are used in this example. When the A1Notations are retrieved from those cells, it is as follows.</p><pre>[<br>  &quot;C2&quot;,<br>  &quot;D2&quot;,<br>  &quot;E2&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;C3&quot;,<br>  &quot;D3&quot;,<br>  &quot;E3&quot;,<br>  &quot;C4&quot;,<br>  &quot;D4&quot;,<br>  &quot;C6&quot;,<br>  &quot;D6&quot;,<br>  &quot;C7&quot;,<br>  &quot;D7&quot;,<br>  &quot;B8&quot;,<br>  &quot;C8&quot;,<br>  &quot;D8&quot;,<br>  &quot;E8&quot;,<br>  &quot;F8&quot;<br>]</pre><p>When these A1Notations are consolidated, it becomes as follows.</p><pre>[<br>  &quot;C2:E3&quot;,<br>  &quot;C6:D8&quot;,<br>  &quot;C4:D4&quot;,<br>  &quot;E8:F8&quot;,<br>  &quot;F2&quot;,<br>  &quot;B3&quot;,<br>  &quot;B8&quot;<br>]</pre><p>Here, the maximum size of the rectangle is calculated starting from the top-left cell (C2 in this example). This approach determines the result values in the above output order.</p><p>It’s important to note that if the situation is changed, the maximum size of the rectangle might not always be obtainable using this method. While it’s possible to modify the starting cell for calculation to ensure the maximum rectangle size is always found, this can significantly increase the processing cost. Therefore, this script adopts the top-left cell as the starting point for calculation to strike a balance between efficiency and accuracy.</p><p>The image provides a visual representation of the consolidation process applied to a sample set of A1Notations.</p><p>The script can be seen at <a href="https://github.com/tanaikech/UtlApp/blob/master/forStringProcessing.js#L466">my repository</a>.</p><h3>Usage</h3><h3>1. Create a Google Spreadsheet</h3><p>Please create a new Google Spreadsheet. And, please set the background color as the above image. In this sample, the background colors are set in “B2:F8”.</p><p>And, please open the script editor of this Spreadsheet.</p><h3>2. Install library</h3><p>In this case, the script is a bit complicated. So, I added this script to my existing library <a href="https://github.com/tanaikech/UtlApp">UtlApp</a>. By this, this library can expand and consolidate the A1Notations.</p><p>You can see how to install this library at <a href="https://github.com/tanaikech/UtlApp?tab=readme-ov-file#1-install-library">here</a>.</p><h3>3. Sample script 1</h3><p>In this sample, the situation of the above image is used. The script is as follows.</p><pre>function sampl1() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  Browser.msgBox(JSON.stringify(res));<br>}</pre><p>When this script is run, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/909/0*HGSpkIuucZ75_3Xh.gif" /></figure><h3>4. Sample script 2</h3><p>In this sample, the following result is obtained.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/0*xTvZlCIhIhj0z6mH.gif" /></figure><p>The cells of the red background color are selected by this script. And, the background color of the selected cells is manually changed.</p><p>The script is as follows.</p><pre>function sampl2() {<br>  const defColor = &quot;#ffffff&quot;;<br>  const sheet = SpreadsheetApp.getActiveSheet();<br>  const backgrounds = sheet<br>    .getRange(1, 1, sheet.getMaxRows(), sheet.getMaxColumns())<br>    .getBackgrounds();<br>  const array = backgrounds.reduce((ar, r, i) =&gt; {<br>    r.forEach((c, j) =&gt; {<br>      if (c != defColor) {<br>        ar.push(`${UtlApp.columnIndexToLetter(j)}${i + 1}`);<br>      }<br>    });<br>    return ar;<br>  }, []);<br>  const res = UtlApp.consolidateA1Notations(array);<br>  sheet.getRangeList(res).activate();<br>}</pre><h3>IMPORTANT</h3><ul><li>I’m worried that this method might not be able to be used on a Google Spreadsheet with a large size because of the process cost.</li></ul><h3>Note</h3><ul><li>The top abstract image was created by <a href="https://gemini.google.com/">Gemini</a> from the section of “Introduction”.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9ce870dcb99" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/consolidate-scattered-a1notations-into-continuous-ranges-on-google-spreadsheet-using-google-apps-c9ce870dcb99">Consolidate Scattered A1Notations into Continuous Ranges on Google Spreadsheet using Google Apps…</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FHIR Whistle Data Mappings Validation]]></title>
            <link>https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/cd62c8613a92</guid>
            <category><![CDATA[healthcare-data-engine]]></category>
            <category><![CDATA[fhir-mapping]]></category>
            <category><![CDATA[whistle-data-mapping]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ashwinshetty]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:06:38 GMT</pubDate>
            <atom:updated>2024-04-19T09:53:18.356Z</atom:updated>
            <content:encoded><![CDATA[<h3>Business Scenario</h3><p>Healthcare Data Engine(HDE) is a popular GCP based solution to help Healthcare stakeholders transition to FHIR (Fast Healthcare Interoperability Resources). HDE provides pipelines which helps convert non FHIR data to FHIR and reconciles them to form a single Longitudinal Patient Record, which then makes deriving insights from patient data easy and quick.</p><p>One of the core components of HDE is the Data Mapping Language known as Whistle. This Open Source Data Mapping language is used for converting complex, nested data from one schema to another. For Example, from HL7 to FHIR.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QdkyZMg9q9d3AMfi3A4l8A.png" /></figure><p>This article talks about how to use Whistle to write sample mappings for HL7 data. Run the mapping code locally and then test the resultant converted FHIR format data against a FHIR store.</p><h3><strong>What do we need</strong></h3><p>We will test the Whistle Mappings to convert sample HL7 data to FHIR. We will be leveraging APIs provided by GCP Healthcare API to ingest some sample HL7 messages to a HL7 store provided by GCP Healthcare API. We will use the schematized variant of this HL7 message from HL7 store and run Whistle mapping code to convert it to FHIR on our local machines. We will then test this converted FHIR data by ingesting it into GCP Healthcare API FHIR store</p><h3>Steps</h3><p><strong>Step 1</strong> — Enable GCP Cloud Healthcare API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/953/1*4QU0vG0lwQMn5RSS-yC2Fw.png" /></figure><p><strong>Step 2</strong> — Follow documentation in <a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">git repo — Healthcare Data</a> Harmonization to set up Whistle Engine on our local machines. This would need us to install below softwares on our local machines, as we will be using ‘<strong>gradle</strong>’ to run our Whistle engine application.</p><ul><li><a href="https://git-scm.com/">Git</a></li><li><a href="https://www.azul.com/downloads/?version=java-11-lts&amp;package=jdk#zulu">JDK 11.x</a></li><li><a href="https://gradle.org/next-steps/?version=7.6&amp;format=bin">Gradle 7.x</a></li></ul><p><strong>Step 3 </strong>— Create a <a href="https://cloud.google.com/healthcare-api/docs/datasets#create-dataset">Healthcare API Dataset</a>. In this dataset create <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2#creating_an_hl7v2_store">HL7 store</a> and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/fhir#creating_a_fhir_store">Fhir store</a>. We will use these resources for our testing.</p><p>Once created we should see an output like below, where ‘<strong>datastore</strong>’ is our Healthcare API Dataset, ‘<strong>hl7v2store</strong>’ is the HL7 store and ‘<strong>fhirstore</strong>’ is the FHIR store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/793/1*FfLztPEw6stASDDU3c6C3w.png" /></figure><p><strong>Step 4</strong> — Let us <a href="https://cloud.google.com/healthcare-api/docs/how-tos/hl7v2-messages#ingesting_hl7v2_messages">ingest a sample HL7 message</a> into our HL7 store. Save the below sample message in a file named ‘<strong>sample-hl7-msg.hl7</strong>’.</p><pre>MSH|^~\&amp;|FROM_APP|FROM_FACILITY|TO_APP|TO_FACILITY|20170703223000||ADT^A01|20170703223000|P|2.5|<br>EVN|A01|20210713083617|<br>PID|1||21004033^^^^MRN||SULIE^BRAN||19941208|M|||444 MAIN ST^^MOUNTAIN SPRINGS^CO^80444||1111111144|2222222244|<br>PV1||I|H44 RM4^1^^HIGHWAY 44 CLINIC||||5144^MARRIE QUINIE|||||||||Y||||||||||||||||||||||||||||20170703223000|</pre><p>The default segment separator in HL7v2 is a carriage return (\r). Most text editors use newline (\n) characters as segment separators. So we will use the below command to replace any \n with \r.</p><pre>sed -z &#39;s/\n/\r/g&#39; sample-hl7-msg.hl7 &gt; sample-hl7-msg-fixed.hl7</pre><p>HL7 store expects input messages to be in base64 encoded string format. So let us use the below command to encode the sample HL7 message.</p><pre>openssl base64 -A -in ./sample-hl7-msg-fixed.hl7 -out ./sample-hl7-msg-base64.txt</pre><p>Copy the encoded string from ‘<strong>sample-hl7-msg-base64.txt</strong>’ in the below format and save it in a file named ‘<strong>hl7v2-sample.json’</strong>.</p><pre>{<br>  &quot;message&quot;: {<br>    &quot;data&quot;: &quot;&lt;base64-encoded-string&gt;&quot;<br>  }<br>}</pre><p>We will run the below CURL command in a terminal to ingest this message to an HL7 store.</p><pre>curl -X POST      \<br>    -H &quot;Authorization: Bearer $(gcloud auth application-default print-access-token)&quot;      \<br>    -H &quot;Content-Type: application/json; charset=utf-8&quot;      \<br>    --data-binary @hl7v2-sample.json      \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages:ingest&quot;</pre><p>Once the command is successful, we will get a ‘<strong>message.name</strong>’ field in the response as shown below.</p><pre>{<br>  &quot;hl7Ack&quot;: &quot;&lt;base64-encoded-string&gt;&quot;,<br>  &quot;message&quot;: {<br>    &quot;name&quot;: &quot;&lt;gcp-project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;MESSAGE_ID&gt;&quot;,<br>    }<br>}</pre><p>Using the ‘<strong>message.name</strong>’ field we will next fetch the schematized message into an output json file. This file will act as an input for our whistle mappings.</p><pre>curl -X GET \<br>     -H &quot;Authorization: Bearer &quot;$(gcloud auth print-access-token) \<br>     -H &quot;Content-Type: application/json; charset=utf-8&quot; \<br>     &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/hl7V2Stores/&lt;hl7store-name&gt;/messages/&lt;message-name&gt;&quot; \<br>     | jq &#39;.schematizedData.data | fromjson&#39; &gt; &lt;output-filename.json&gt;</pre><p><strong>Step 5</strong> — Let us open any IDE or terminal. We will run below gradle command to trigger mapping, in the directory where github repo was cloned.</p><pre>gradle :runtime:run -q --args=&quot;-m $HOME/wstl_codelab/codelab.wstl -i $HOME/wstl_cod<br>elab/&lt;output-filename.json&gt;&quot; &gt; converted-fhir.json</pre><blockquote>Explanation of the above <strong>gradle</strong> command:</blockquote><blockquote><strong>gradle</strong>: This invokes the Gradle build automation tool.<br><strong>:runtime:run</strong>: This tells Gradle to execute the run task to start the Whistle application.<br><strong>-q</strong>: This flag tells Gradle to run in “quiet” mode, suppressing most of the output except for errors.<br> <strong>— args</strong>: This introduces arguments that will be passed to the run task (and ultimately to the application it starts).<br><strong>-m $HOME/wstl_codelab/codelab.wstl</strong>: This argument specifies the path to a whistle file that the application will use for data mapping.<br><strong>-i $HOME/wstl_codelab/&lt;output-filename.json&gt;</strong>: This argument points to a JSON file containing input data for the Whistle mapping.</blockquote><p><strong>Sample Patient Whistle Mapping:</strong></p><blockquote>This code is just for demo purposes and does not represent the actual FHIR structure. It maps Patient fields like ‘<strong>identifier</strong>’, ‘<strong>name</strong>’ and ‘<strong>address</strong>’ from the PID segment in our input file. These mappings are structured into functions like ‘<strong>Build_Identifier</strong>’, ‘<strong>Build_Name</strong>’ and ‘<strong>Build_Address</strong>’ for better readability.</blockquote><pre>PID_Patient($root.ADT_A01.PID)<br><br>def PID_Patient(PID){<br>  identifier[]: Build_Identifier(PID.3[])<br>  name[]: Build_Name(PID.5[])<br>  address[]: Build_Address(PID.11[])<br>  active: true<br>  resourceType: &quot;Patient&quot;<br>}<br><br>def Build_Identifier(CX) {<br>  value: CX.1<br>}<br><br>def Build_Name(XPN) {<br>  family: XPN.1.1<br>  given[]: XPN.2<br>  given[]: XPN.3<br>}<br><br>def Build_Address(XAD) {<br>  line[]: XAD.2<br>  city: XAD.3<br>  state: XAD.4<br>  postalCode: XAD.5<br>}</pre><p><strong>Step 6</strong> — Once we have the mapped output, we can check if all the fields were converted as per our requirements. Once confirmed, we can try and <a href="https://cloud.google.com/healthcare-api/docs/how-tos/fhir-resources#creating_a_fhir_resource">load this to a FHIR store</a> using the below command.</p><pre>curl -X POST \<br>    -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>    -H &quot;Content-Type: application/fhir+json&quot; \<br>    -d @converted-fhir.json \<br>    &quot;https://healthcare.googleapis.com/v1/projects/&lt;project-name&gt;/locations/&lt;location&gt;/datasets/&lt;dataset-name&gt;/fhirStores/&lt;fhirstore-name&gt;/fhir/Patient&quot;</pre><p>Post successful completion of the above command, we should be able to see the record in our FHIR store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGeHlfu01LH9IC1oCEeN7Q.png" /></figure><h3>Conclusion</h3><p>By following the steps outlined above, we explored a method to validate the HL7 to FHIR conversion workflow utilizing the Open Source Whistle Data Mapping repository. This approach can be readily adapted to validate data conversion workflows involving any other data format to FHIR. This technique proves useful for conducting quick tests, proofs of concept (POCs), or pilot projects for healthcare data conversion to FHIR. Engaging with this process offers a deeper understanding of the capabilities of the powerful Whistle Data Mapping Language.</p><h3>Reference Links</h3><p><a href="https://github.com/GoogleCloudPlatform/healthcare-data-harmonization">Whistle github repo</a></p><p><a href="https://cloud.google.com/healthcare-api/docs">Cloud Healthcare API documentation</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd62c8613a92" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fhir-whistle-data-mappings-validation-cd62c8613a92">FHIR Whistle Data Mappings Validation</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dazbo’s Google Cloud Next ’24 Recap: Keynote]]></title>
            <link>https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f5518238c9d</guid>
            <category><![CDATA[ai-agent]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[wrap-up]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud-next]]></category>
            <dc:creator><![CDATA[Dazbo (Darren Lester)]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:05:10 GMT</pubDate>
            <atom:updated>2024-04-16T07:13:06.235Z</atom:updated>
            <content:encoded><![CDATA[<h3>Shall I? Shan’t I?</h3><p>It’s been a couple of days since Google Cloud Next ’24 wrapped up, and I’ve seen recaps appear on Medium already. So I ask myself: <em>“Should I bother this year?”</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*jvPtfzoKolvQBc2o.jpg" /><figcaption>To recap or not to recap…</figcaption></figure><p>I’ve decided “Yes” for two reasons…</p><ul><li>I’ve been doing recaps of these events for a few years, so I’d hate to break my streak! (Check out <a href="https://medium.com/google-cloud/google-next-2023-experience-and-favourite-sessions-fb00add5f59e">here</a>, and <a href="https://docs.google.com/presentation/d/1gfgijQjlQlvn6CEd29j5uVRXgynNFoBwMz2bUIXvydA/edit#slide=id.g27956d63397_0_230">here</a>.)</li><li>I find writing stuff down helps me learn and remember. So even if no one else finds this useful, I will!</li></ul><p>This year’s Google Cloud Next was in Las Vegas. Alas, this is the Next in the last few that I haven’t been able to attend in person. 😭 So, this wrap-up is based purely on watching the virtual content. And in case you weren’t aware, you can view all the recorded sessions at <a href="https://cloud.withgoogle.com/next">cloud.withgoogle.com/next</a>.</p><h3>Summary of Announcements</h3><p>I’ll update this list as a view more sessions.</p><ul><li>New investments in sub-sea cabling and data centres.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: A3 Mega VMs, powered by NVIDIA H100 Tensorcore GPUs. Twice as powerful has the previous iteration.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">AI Hypercomputer</a>: GA of TPU v5p. Google’s most powerful TPU yet. These have 4x the compute capacity of the previous generation of TPUs.</li><li>Preview: Hyperdisk ML — next generation block storage optimised for AI workloads.</li><li>Vertex AI on <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>GKE Enterprise support for <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>AI Model support (including Gemma and Llama) on <a href="https://cloud.google.com/distributed-cloud?hl=en">GDC</a>.</li><li>Preview: <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805">Google Axion</a>. A custom ARM-based CPU. Claims 50% better performance and 60% more energy efficient than comparable current-gen x86 VMs! Google are migrating many services to Axiom.</li><li>Intel 5th Gen Xeon processors.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Gemini AI 1.5 Pro in Vertex AI</a>. Google’s multimodal foundational model. It can parse 1m tokens of information!</li><li>Gemini AI 1.5 Pro now integrated with Gemini Code Assist.</li><li>Supervised tuning for Gemini models.</li><li>Preview: Gemini Cloud Assist, which helps with the entire development lifecycle, including design and optimisation.</li><li>Public preview: <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates">Grounding of Gemini models with Google Search</a>! This significantly reduces hallucination.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder">Vertex AI Agent Builder</a>: rapidly speed up the creation of multi-modal AI agents.</li><li><a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations">Google Vids</a> will be released to Workspace labs in June. This is an AI-powered collaborative video creation app, as part of Workspace.</li><li>Imagen 2.0 is now GA in Vertex AI. Google’s most advanced text-to-image model.</li><li>Public preview: Text-to-Live Image. This creates animated video-like images from a text prompt.</li><li>Public preview: Gemini in Looker.</li><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiant’s frontline threat intelligence using using natural language prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths and risks.</li></ul><h3>The Irony Isn’t Wasted On Me</h3><p>I’ve watched the keynote, and I’m summarising it here. Manually. Without AI.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/181/0*z3Pfn-gqrDOZ2lSc.gif" /></figure><h3>Opening Keynote: The New Way to Cloud</h3><p>You can see the full keynote <a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">here</a>.</p><h4>Keynote Quick Thoughts</h4><ul><li>It’s all about AI. Shocking.</li><li>The biggest announcements are around Gen AI capabilities.</li><li>I think the keynote mentioned AI agents 1,806,402 times. (Okay, I’m exaggerating slightly.)</li></ul><h4>Introduction</h4><blockquote>Google are at the forefront of the AI platform shift. More than 60% of funded Gen AI startups, and nearly 90% of Gen AI unicorns are Google Cloud customers.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5JXpAkiOWGrILB4z.jpg" /></figure><p>The keynote opens with an introductory video talking the power of AI today. (<em>“AI you say? I’m shocked. Shocked, I tell you!”</em>) The video talks about things we can do with AI now, like:</p><ul><li>Using satellites to reduce methane emissions.</li><li>Turning DNA into code to make… Crop-resistant corn!</li><li>Spoting and filling potholes.</li><li>Spoting diseases earlier.</li><li>Scanning 100K lines of code in 2 minutes, in order to spot and fix bugs.</li></ul><p>So this is <em>“The new way to Cloud.”</em></p><p>So far, so cool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1PHwomsMjmMnmhFTPJ62dw.png" /><figcaption>Google has announced partnerships with 100s of leading AI partners</figcaption></figure><p>The early keynote includes a brief introduction to some of the topics of this year’s Next:</p><ul><li>Over 300 customers and partners will be sharing their <strong>Gen AI success stories</strong> at this event.</li><li>Some discussion around the launch of <strong>Gemini </strong>and the advancements since its launch.</li><li><strong>Google Distributed Cloud and Edge</strong>, to support highly confidential and edge workloads.</li><li><strong>Cross-cloud networking</strong> now provides secure, low-latency connectivity of Google’s AI services to any application on any cloud.</li><li><strong>Chrome Enterprise Premium Browser</strong>.</li><li><strong>Multimodal Gen AI Agents</strong> will transform how we interact with the applications and the web. Agents are intelligent entities to do things like: customer agents, to help a shopper find the perfect dress; or helping an employee pick the right health benefits.</li></ul><h4>The AI Stack</h4><p>The keynote talks about <strong>Google’s AI stack</strong>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FxbiOgARL1a4-Q__qFFqfA.png" /><figcaption>Google’s AI stack</figcaption></figure><ul><li>Note the rebranding of Duet AI to “<a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-for-google-cloud-is-here"><strong>Gemini for Google Cloud</strong></a>”.</li><li><a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture"><strong>AI Hypercomputer</strong></a>: an integrated AI infrastructure platform for offering AI at scale. There are a number of announcements related to GPUs, TPUs, and AI-optimised storage.</li></ul><p>The keynote includes announcements around:</p><ul><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization/unlock-ai-anywhere-with-google-distributed-cloud?e=48754805"><strong>Google Distributed Cloud</strong></a>, which has a number of capability enhancements around GKE, Vertex AI, and AI model support. GDC now has both “secret” and “top secret” accreditations. Mobile operator “Orange” referenced as an organisation running across 26 countries and using GDC to keep data localised to each country.</li><li><a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu?e=48754805"><strong>Google Axion</strong></a>. A new custom ARM-based CPU that offers considerably higher performance and lowe energy consumption than caparable current gen x86.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-gemini-image-2-and-mlops-updates"><strong>Gemini 1.5 Pro in public preview</strong></a>. It has the world’s largest context window. In a single shot, it can process: 1M tokens, 1 hour of video, 11 hours of audio, and over 30K lines of code.</li><li><strong>Grounding of Gemini models with Google Search!</strong> This significantly reduces hallucination. Or you can ground with data from your own databases.</li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/build-generative-ai-experiences-with-vertex-ai-agent-builder"><strong>Vertex AI Agent Builder</strong></a> — to rapidly speed up creating AI Agents. Gemino Pro can create free-flowing conversations with text, voice, images and video as inputs. But also, it can even provide real time interactions in voice! Natural language can be used to train the AI agents, e.g. to describe topics that are verboten. You can configure transcription and summarisation. And response quality can be improved using vector search. Also, modular extensions can be integrated to complete standard customer workflows, e.g. booking a flight.</li></ul><h4>Shopping AI Agents</h4><p>The keynote then demonstrates a <strong>shopping AI Agent</strong>, and the ability to upload a video and ask it:</p><blockquote>Find me a checkered shirt like the keyboard player is wearing. I’d like to see prices, where to buy it, and how soon can I be wearing it?</blockquote><p>The response is near instantaneous on the website. And then we see a demo of interacting with a <em>voice </em>AI agent which continues the interaction and completes the transaction. That’s pretty cool!</p><h4>A Few Google Workspace Updates</h4><p>Then the keynote moves onto <strong>Gemini for Google Workspace</strong>. Use it to:</p><ul><li>Answer questions.</li><li>Create notes in meetings.</li><li>Extract insights from reports.</li><li>Create images to insert in presentations.</li><li>Real-time translation.</li></ul><p>Announcements related to <strong>Google Workspace</strong>:</p><ul><li>A recent benchmarking study shows <strong>Google Meet now outperforms Zoom and MS Teams</strong> for overall video performance.</li><li>Chat summarisation and real time translation now available for Google Meet.</li><li><strong>AI Security add-on</strong> can automatically classify and protect company data.</li><li><strong>Gemini in Google Chat</strong> can provide summaries of long conversations.</li></ul><p>We see a demo of reviewing proposals, comparing them, and asking questions, e.g.</p><blockquote>Does this offer comply with our compliance rule book?</blockquote><h4>Employee Agents</h4><p>Next, we talk about how to <strong>create a multi-modal AI employee agents using Vertex AI</strong>:</p><ul><li>Create a custom model with Vertex AI.</li><li>Connect the custom model to your company data and web data.</li><li>Ground in enterprise truth, e.g. with BigQuery and AlloyDB.</li></ul><p>Then we see a demo of how you can use a Vertex AI employee agent to summarise an employee benefits enrollment email, as well as a one hour benefits video. The agent is able to reason across text, video and the prompt, and provide a summary. Furthermore, the agent is able to compare the proposed plan to a previous plan, and make inferences.</p><h4>Creative AI Agents</h4><p>Now we move on to <strong>Creative AI Agents</strong>. Carrefore are using Creative AI Agents for marketing; they built a new marketing studio using Vertex AI, in just five weeks. Now they can build personalised campaigns in just a few clicks.</p><p>Creative agents uses Gemino Pro to look at existing material, documents and brand images, to infer a brand identity. We can generate multi-modal content; we can create live images, and even podcasts!</p><p>Then there was the announcement of <a href="https://workspace.google.com/blog/product-announcements/new-generative-ai-and-security-innovations"><strong>Google Vids</strong></a>, the AI-powered collaborative video creation app, as part of Google Workspace. Aparna then demos creating a recap video of the Next event, using Google Vids:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kZo1hMT6Ne-896WcxB3sGQ.png" /><figcaption>Creating a video recap in seconds, using Google Vids</figcaption></figure><p>Then we have announcements of <strong>Imagen 2.0 Text-to-Image</strong>, including new editing modes to edit a generated image. And there’s the new <strong>Text-to-Live Image</strong>, which is now in preview:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*K2OL48TfgPnzrPn6EXgqhw.gif" /><figcaption>Generating a live image from a prompt</figcaption></figure><h4>Data Agents</h4><p>So many agents!!</p><p>AI Data Agents us to ask natural language questions of our data. <a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24"><strong>Gemini in BigQuery</strong></a> is now in Preview, and allows AI-powered data preparation, analysis and querying. BigQuery can be integrated directly with Vertex AI. So now we can perform multi-modal analysis across all of documents, images, videos, audio, and structured data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LlwikgKhm7Q-kjaSvSKqig.png" /><figcaption>Querying a data agent</figcaption></figure><p>One extremely cool thing about this demo was that the agent built a forecast dynamically, using BigQuery ML. And then uses vector embeddings to find products that look like a supplied image.</p><h4>Code Agents</h4><p>Surprise… More agents.</p><p>Google’s AI code assistant is now called <strong>Gemini Code Assist</strong>. (No more Duet AI.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/1*pizqI2uBk7FGn_0Y5TBAhg.png" /><figcaption>Benefits of using Code Assist</figcaption></figure><p>The keynote talks about how Gemini Code Assist can be used with a code base anywhere… On-prem, GitLab, GitHub, BitBucket, etc. Furthermore, Gemini Code Assist supports data residency requirements in multiple regions. It is now integrated with Gemini 1.5 Pro, and can leverage the new 1-million token context window.</p><p>The demo was cool… Show the visual mockup of a new UI to Gemini Code Assist, and it generates the code, leveraging our entire (huge) code base, and aligned to our code standards.</p><h4>Security Agents</h4><p>Please… No more agents!</p><p>These AI agents assist security operations teams, radically increasing the speed of security investigation and response.</p><p>There were a number of announcements relating to integration of Gemini into security products:</p><ul><li>Public preview: Gemini in Threat Intelligence. Tap into Mandiant’s frontline threat intelligence using using natural language prompts.</li><li>Public preview: Gemini in Security Operations. Summarise and explain findings, recommend next steps, and even write and execute remediation playbooks.</li><li>Public preview: Gemini in Security Command Centre. Evaluate security posture, and summarise potential attack paths and risks.</li></ul><h4>Wrap-Up</h4><p>Thomas Kurian wraps-up by saying:</p><blockquote>Our open platform offers choice at every layer.</blockquote><ul><li>Chips (CPUs, TPUs, GPUs) for training and serving.</li><li>Your choice of models.</li><li>Your choice of development environments.</li><li>Databases, including vector.</li><li>Your choice of business applications.</li></ul><blockquote>We’re creating a new era of generative AI agents, built on a new, truly open platform for AI. And we’re reinventing infrastructure to support it.</blockquote><h3>What’s Next?</h3><p>(See what I did there?)</p><p>I’ll watch a bunch of sessions I’m interested in, and provides some useful nuggets and summaries soon. I’ll put these in some separate articles, rather than just adding to this one.</p><h3>Links</h3><ul><li><a href="https://cloud.withgoogle.com/next">Google Cloud Next ‘24</a></li><li><a href="https://www.youtube.com/watch?v=V6DJYGn2SFk&amp;t=1s">Keynote</a></li><li><a href="https://cloud.google.com/blog/topics/google-cloud-next/google-cloud-next-2024-wrap-up">All 218 things we announced at Google Cloud Next ‘24</a></li></ul><h3>Before You Go</h3><ul><li><strong>Please share</strong> this with anyone that you think will be interested. It might help them, and it really helps me!</li><li>Please give me claps! You know you clap more than once, right?</li><li>Feel free to <strong>leave a comment</strong> 💬.</li><li><strong>Follow</strong> and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href="https://medium.com/@derailed.dash">Profile Page</a>, and click on these icons:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/163/0*73hF99AvDUGryMuV.png" /><figcaption>Follow and Subscribe</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f5518238c9d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/dazbos-google-cloud-next-24-recap-keynote-6f5518238c9d">Dazbo’s Google Cloud Next ’24 Recap: Keynote</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fine tuning Gemma with LoRA on GCP]]></title>
            <link>https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/5d25dbab9e0e</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[finetune-llm]]></category>
            <category><![CDATA[lora]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gemma]]></category>
            <dc:creator><![CDATA[pritam sahoo]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:52 GMT</pubDate>
            <atom:updated>2024-04-16T00:04:52.701Z</atom:updated>
            <content:encoded><![CDATA[<p>My obsession with Gemma continues. Folks new to the Gemma model can revisit my previous blog <a href="https://medium.com/google-cloud/gemma-open-models-from-google-0045263e53d2">link</a>.</p><p>In brief Gemma is the family of lightweight, state of the art (SOTA) open models powered by the same technology powering one of the most popular Google Cloud Gemini models.</p><p>In this blog we will get started with fine tuning with Gemma with LoRA.</p><p>Lets understand first a bit on fine tuning. One of the reasons finetuning is picking up is the reason Large language Models(LLMs) are not trained on specific tasks or domain related data. Primarily LLMs often called as foundational models are trained on internet scale massive corpus of data, texts etc. Doing a full training of pre-trained LLM models becomes technically challenging due to expensive computational resources as one of the major concerns.</p><p>Let’s understand the benefits of Fine tuning.</p><ol><li>Fine Tuning pre-trained model is much faster and cost effective leading to less computational resources required.</li><li>Better Performances for domain specific tasks especially on industry use cases related to Financial services, Insurance , Healthcare etc.</li><li>Lets not forget about democratization of GenAI models for individual users i.e. developers and others who have less computational power.</li></ol><p>Lets understand Parameter efficient fine tuning <a href="http://a.ka">a.k.a</a>. PEFT. It’s a subset of fine tuning which effectively utilizes parameters/weights with efficient output. Instead of altering all the parameters of the model PEFT selects a subset of them thereby reducing computational and memory requirements. PEFT plays a major role in the fine tuning process thereby improving the performance of base/foundational LLMs on specific tasks. This is super useful when training LLM models like Gemini and its different variants, PALM,even open source Gemma models etc from Google.</p><p>We will explore fine tuning Gemma Models with <strong>LoRA</strong>. <strong>LoRA</strong> stands for Low Rank Adaptation of Large Language Models. It’s a technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights/parameters of the base model and introducing a small number of new weights into the model.</p><p><strong>Crucial Point to consider</strong> In LoRA, the starting point hypothesis is super important . It assumes that the pre-trained model’s weights are already close to the optimal solution for the downstream tasks.</p><p>Advantages of using LoRA as fine tuning technique</p><ol><li>Reduces Parameter and memory footprint. LoRA significantly reduces the number of trainable parameters, making it much more memory-efficient and computationally cheaper.</li><li>Fine tuning and so does inference is faster ~ as it uses less parameters/weights.</li><li>Maintains performance: LoRA has been proved to maintain performance close to traditional fine-tuning methods in several tasks.</li></ol><p><strong>So let’s get started with Fine tuning with LoRA on the Gemma Model.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UkJdQkyIoYu3I-OH3bJYoA.jpeg" /></figure><p>For this demo I will be using Google Collab Notebook to get some horsepower with T4 GPUs.</p><p><strong>Step 1: Get access to Gemma</strong></p><p>To complete this collab, you will first need to complete the setup instructions at <a href="https://ai.google.dev/gemma/docs/setup">Gemma setup</a>. The Gemma setup instructions show you how to do the following:</p><ul><li>Get access to Gemma on <a href="https://kaggle.com/">kaggle.com</a>.</li><li>Select a Colab runtime with sufficient resources to run the Gemma 2B model.</li><li>Generate and configure a Kaggle username and API key.</li></ul><p>After you’ve completed the Gemma setup, move on to the next section, where you’ll set environment variables for your Colab environment.</p><p><strong>Step 2 : Select the Runtime</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/468/0*6QanOb6XI087IwsU" /></figure><h4><strong>Step 3 : Configure your secrets i.e. username and key in Account tab</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/834/0*m_Kg4FsICCsvEH73" /></figure><p><strong>Step 4 : Select the Data for fine tuning from hugging face. </strong><a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k"><strong>Databricks Dolly 15k dataset</strong></a><strong>. </strong>This dataset contains 15,000 high-quality human-generated prompt / response pairs specifically designed for fine-tuning LLMs. Brief screenshot of the datasets</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/997/0*wmgP3pZMciNG5Ix5" /></figure><p><strong>Step 5 : Set the environment variables and run the below commands in Collab</strong></p><p>import os</p><p>from google.colab import userdata</p><p>os.environ[“KAGGLE_USERNAME”] = userdata.get(‘username’)</p><p>os.environ[“KAGGLE_KEY”] = userdata.get(‘key’)</p><p><strong>Step 6 : Install the dependencies</strong></p><p>!pip install -q -U keras-nlp</p><p>!pip install -q -U keras&gt;=3</p><p><strong>Step 7 : Select the backend. You may choose from PyTorch or Tensorflow or Jax</strong></p><p>os.environ[“KERAS_BACKEND”] = “jax”.</p><p># Avoid memory fragmentation on JAX backend.</p><p>os.environ[“XLA_PYTHON_CLIENT_MEM_FRACTION”]=”1.00&quot;</p><p><strong>Step 8 : Import Packages i.e. Keras and KerasNLP.</strong></p><p>import keras</p><p>import keras_nlp</p><p><strong>Step 9 : Load the dataset from hugging face.</strong></p><p>!wget -O databricks-dolly-15k.jsonl <a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl">https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl</a></p><p><strong>Step 10 : For this demo purpose I will be using a subset of 1000 examples instead of 15K examples. For better fine tuning you may use more examples.</strong></p><p>import json</p><p>data = []</p><p>with open(“databricks-dolly-15k.jsonl”) as file:</p><p>for line in file:</p><p>features = json.loads(line)</p><p># Filter out examples with context, to keep it simple.</p><p>if features[“context”]:</p><p>continue</p><p># Format the entire example as a single string.</p><p>template = “Instruction:\n{instruction}\n\nResponse:\n{response}”</p><p>data.append(template.format(**features))</p><p># Only use 1000 training examples, to keep it fast.</p><p>data = data[:1000]</p><p><strong>Step 11 : Now its time to Load the Gemma 2B base Model. You may try using the Gemma 7B base model.</strong></p><p>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(“gemma_2b_en”)</p><p>gemma_lm.summary()</p><p>You will see below summary output if everything is working fine.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/0*df55cDERngDI3ig_" /></figure><p><strong>Step 11: Lets Inference the Model before fine tuning.</strong></p><p>Pass the below prompt i.e. “ What should I do on a trip to Europe?”</p><p>prompt = template.format(</p><p>instruction=”What should I do on a trip to Europe?”,</p><p>response=””,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>You will see very generic blant and not so great output from the base model as mentioned below</strong></p><p>— — — — — — — — — — — — — — — — — — — — —</p><p><strong>Instruction:</strong></p><p><strong>What should I do on a trip to Europe?</strong></p><p><strong>Response:</strong></p><p><strong>It’s easy, you just need to follow these steps:</strong></p><p><strong>First you must book your trip with a travel agency.</strong></p><p><strong>Then you must choose a country and a city.</strong></p><p><strong>Next you must choose your hotel, your flight, and your travel insurance</strong></p><p><strong>And last you must pack for your trip.</strong></p><p><strong>— — — — — — — — — — — — — — —</strong></p><p><strong>Step 12: Lets fine tuning using LoRA using Databricks Dolly 15K dataset.</strong></p><p>LoRA rank. It controls the expressiveness and precision of the fine-tuning adjustments.Lower rank means which requirement of computational power and also less precision adaptation. You may start with 4,8 etc for demo/experimentation purposes.</p><p>&gt;&gt; gemma_lm.backbone.enable_lora(rank=4)</p><p>&gt;&gt; gemma_lm.summary()</p><p><strong>Total params: 2,507,536,384 (9.34 GB)</strong></p><p><strong>Trainable params: 1,363,968 (5.20 MB)</strong></p><p><strong>Non-trainable params: 2,506,172,416 (9.34 GB)</strong></p><p><strong>While you run the below section in the collab notebook be patient as it will take some time and you will see reduction in losses.This step will reduce the number of trainable parameters significantly.Epoch = 1 means it will run for 1 time for 1000 datasets.</strong></p><p>gemma_lm.preprocessor.sequence_length = 512</p><p>optimizer = keras.optimizers.AdamW( // AdamW ~ optimizer for transformer models</p><p>learning_rate=5e-5,</p><p>weight_decay=0.01,</p><p>)</p><p>optimizer.exclude_from_weight_decay(var_names=[“bias”, “scale”])</p><p>gemma_lm.compile(</p><p>loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),</p><p>optimizer=optimizer,</p><p>weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],</p><p>)</p><p>gemma_lm.fit(data, epochs=1, batch_size=1)</p><p><strong>The output from the above step will show significant reduction in loss with just 1000 datasets.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qf-k1hlFro2K4DUh" /></figure><p><strong>Step 13: Let’s get started with Inferencing post fine tuning.</strong></p><p>Pass the below prompt again i.e. “ What should I do on a trip to Europe?”</p><p>prompt = template.format(</p><p>instruction=”What should I do on a trip to Europe?”,</p><p>response=””,</p><p>)</p><p>sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)</p><p>gemma_lm.compile(sampler=sampler)</p><p>print(gemma_lm.generate(prompt, max_length=256))</p><p><strong>**** Let me know the results. Must be better than before finetuning.</strong></p><p>Thats’ it folks on Gemma fine tuning with LoRA. Stay tuned for more updates coming your way on QLoRA……..</p><p><strong>Signing off…. Pritam</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d25dbab9e0e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/fine-tuning-gemma-with-lora-on-gcp-5d25dbab9e0e">Fine tuning Gemma with LoRA on GCP</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use Log Analytics for BigQuery Usage Analysis on Google Cloud]]></title>
            <link>https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c?source=rss----e52cf94d98af---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f5454626c6c</guid>
            <category><![CDATA[log-analytics]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[observability]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[logs]]></category>
            <dc:creator><![CDATA[Xiang Shen]]></dc:creator>
            <pubDate>Tue, 16 Apr 2024 00:04:36 GMT</pubDate>
            <atom:updated>2024-04-16T12:18:26.756Z</atom:updated>
            <cc:license>https://creativecommons.org/publicdomain/mark/1.0/</cc:license>
            <content:encoded><![CDATA[<p>On Google Cloud you can use <a href="https://cloud.google.com/logging/docs/log-analytics#analytics">Log Analytics</a> to query and analyze your log data, and then you can view or <a href="https://cloud.google.com/logging/docs/analyze/charts">chart the query results</a>.</p><p><a href="https://cloud.google.com/bigquery">BigQuery</a> is Google Cloud’s fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence.</p><p>While BigQuery offers built-in observability capabilities like the <a href="https://cloud.google.com/bigquery/docs/information-schema-intro">INFORMATION_SCHEMA</a> views, detailed logging remains crucial for in-depth usage analysis, auditing, and troubleshooting potential issues.</p><p>This article will walk you through how to analyze BigQuery logs using log analytics.</p><h3>Upgrade Log bucket</h3><p>First, if you haven’t, you need to configure Cloud Logging to upgrade all the existing log buckets with Log Analytics enabled.</p><p>To upgrade an existing bucket to use Log Analytics, do the following:</p><ol><li>In the navigation panel of the Google Cloud console, select <strong>Logging</strong>, and then select <strong>Logs Storage.</strong></li><li>Locate the bucket that you want to upgrade.</li><li>When the <strong>Log Analytics available</strong> column displays <strong>Upgrade</strong>, you can upgrade the log bucket to use Log Analytics. Click <strong>Upgrade</strong>.<br>A dialog opens. Click <strong>Confirm</strong>.</li></ol><h3>Perform BigQuery Activities</h3><p>Complete the following tasks to generate some BigQuery logs. In the tasks, the BigQuery command line tool <a href="https://cloud.google.com/bigquery/docs/reference/bq-cli-reference">bq</a> is used.</p><p><strong>Task 1. Create datasets</strong></p><p>Use the <strong>bq mk</strong> command to create new datasets named <strong>bq_logs</strong> and <strong>bq_logs_test </strong>in your project:</p><pre>bq mk bq_logs<br>bq mk bq_logs_testbq mk bq_logs_test</pre><p><strong>Task 2. List the datasets</strong></p><p>Use the <strong>bq ls</strong> command to list the datasets:</p><pre>bq ls</pre><p><strong>Task 3. Delete a dataset</strong></p><p>Use the <strong>bq rm</strong> command to delete the a dataset (select <strong>y</strong> when prompted):</p><pre>bq rm bq_logs_test</pre><p><strong>Task 4. Create a new table</strong></p><pre>bq mk \<br> --table \<br> --expiration 3600 \<br> --description &quot;This is a test table&quot; \<br> bq_logs.test_table \<br> id:STRING,name:STRING,address:STRING</pre><p>You should have a new empty table named <strong>test_table</strong> that has been created for your dataset.</p><p><strong>Task 5. Run some example queries</strong></p><p>You can run a simple query like the following to generates a log entry. Copy and paste the following query into the BigQuery Query editor:</p><pre>bq query — use_legacy_sql=false ‘SELECT current_date’</pre><p>The following query will leverage weather data from the <a href="https://cloud.google.com/blog/products/data-analytics/noaa-datasets-on-google-cloud-for-environmental-exploration">National Oceanic and Atmospheric Administration (NOAA)</a>. Copy the query into the BigQuery editor and click <strong>RUN</strong>.</p><pre>bq query --use_legacy_sql=false \<br>&#39;SELECT<br> gsod2021.date,<br> stations.usaf,<br> stations.wban,<br> stations.name,<br> stations.country,<br> stations.state,<br> stations.lat,<br> stations.lon,<br> stations.elev,<br> gsod2021.temp,<br> gsod2021.max,<br> gsod2021.min,<br> gsod2021.mxpsd,<br> gsod2021.gust,<br> gsod2021.fog,<br> gsod2021.hail<br>FROM<br> `bigquery-public-data.noaa_gsod.gsod2021` gsod2021<br>INNER JOIN<br> `bigquery-public-data.noaa_gsod.stations` stations<br>ON<br> gsod2021.stn = stations.usaf<br> AND gsod2021.wban = stations.wban<br>WHERE<br> stations.country = &quot;US&quot;<br> AND gsod2021.date = &quot;2021-12-15&quot;<br> AND stations.state IS NOT NULL<br> AND gsod2021.max != 9999.9<br>ORDER BY<br> gsod2021.min;&#39;</pre><h3>Perform log analysis</h3><p>Now there are some log entries for BigQuery. You can run some queries using Log Analytics.</p><p><strong>Task 1. Open Log Analytics</strong></p><p>On the left side, under <strong>Logging</strong> click <strong>Log Analytics</strong> to access the feature. You should see something like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6PrYdj0eZePE3wF8" /></figure><p>If your query field is empty or you forget which table you want to use, you can click the <strong>Query</strong> button to get the sample query back.</p><p>Now you can run your own queries in the query field. Remember to replace <strong>[Your Project Id]</strong> with the project id you are using.</p><p><strong>Task 2. To find the activities for BigQuery datasets</strong></p><p>You can query the activities that a dataset is created or deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;datasetservice%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QgplyJgYAw8L93Rn" /></figure><p><strong>Task 3. To find the activities for BigQuery tables</strong></p><p>You can query the activities that a dataset is created or deleted:</p><pre>SELECT<br> timestamp,<br> severity,<br> resource.type,<br> proto_payload.audit_log.authentication_info.principal_email,<br> proto_payload.audit_log.method_name,<br> proto_payload.audit_log.resource_name,<br>FROM<br> `[Your Project Id].global._Required._AllLogs`<br>WHERE<br> log_id = &#39;cloudaudit.googleapis.com/activity&#39;<br> AND proto_payload.audit_log.method_name LIKE &#39;%TableService%&#39;<br>LIMIT<br> 100</pre><p>After run the query, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*REyZPbwK41Vfotxk" /></figure><p><strong>Task 4. To view the queries completed in BigQuery</strong></p><p>Run the following query:</p><pre>SELECT<br> timestamp,<br> resource.labels.project_id,<br> proto_payload.audit_log.authentication_info.principal_email,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.query) AS query,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobConfiguration.query.statementType) AS statementType,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatus.error.message) AS message,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS startTime,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS endTime,<br> CAST(TIMESTAMP_DIFF( CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.endTime) AS TIMESTAMP), CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.startTime) AS TIMESTAMP), MILLISECOND)/1000 AS INT64) AS run_seconds,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalProcessedBytes) AS INT64) AS totalProcessedBytes,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalSlotMs) AS INT64) AS totalSlotMs,<br> JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.referencedTables) AS tables_ref,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.totalTablesProcessed) AS INT64) AS totalTablesProcessed,<br> CAST(JSON_VALUE(proto_payload.audit_log.service_data.jobCompletedEvent.job.jobStatistics.queryOutputRowCount) AS INT64) AS queryOutputRowCount,<br> severity<br>FROM<br> `[Your Project Id].global._Default._Default`<br>WHERE<br> log_id = &quot;cloudaudit.googleapis.com/data_access&quot;<br> AND proto_payload.audit_log.service_data.jobCompletedEvent IS NOT NULL<br>ORDER BY<br> startTime</pre><p>After the query completes, you should see the output like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yIhioPqh3mZwXguM" /></figure><p>Scroll through the results of the executed queries.</p><p><strong>Task 5. To chart the query result</strong></p><p>Instead of using a table to see the results, Log Analytics also supports creating charts for visualization. For example, to view a pie chart for the queries that have run, you can click the <strong>Chart</strong> button in the result view, select <strong>Pie chart</strong> as the chart type and <strong>query</strong> as the column. You should see a chart similar to the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HoiWqDgj6gqLAp58" /></figure><p>We’ve only scratched the surface of BigQuery log analysis; you can explore many other queries and charts to enhance your understanding of BigQuery. Feel free to contribute and create samples in <a href="https://github.com/GoogleCloudPlatform/observability-analytics-samples/tree/main/samples/logging">GCP’s sample GitHub repository</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f5454626c6c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/use-log-analytics-for-bigquery-usage-analysis-on-google-cloud-8f5454626c6c">Use Log Analytics for BigQuery Usage Analysis on Google Cloud</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>