<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Cloud Blog</title><link>https://cloud.google.com/blog/</link><description>Cloud Blog</description><atom:link href="https://cloudblog.withgoogle.com/blog/rss/" rel="self"></atom:link><language>en</language><lastBuildDate>Tue, 14 May 2024 17:05:37 +0000</lastBuildDate><image><url>https://cloud.google.com/blog/static/blog/images/google.a51985becaa6.png</url><title>Cloud Blog</title><link>https://cloud.google.com/blog/</link></image><item><title>The overwhelmed person’s guide to Google Cloud: week of May 9</title><link>https://cloud.google.com/blog/products/gcp/the-overwhelmed-persons-guide-to-google-cloud/</link><description>&lt;div class="block-paragraph"&gt;&lt;p data-block-key="2u57j"&gt;&lt;i&gt;The content in this blog post was originally published last week as a members-only email to the Google Cloud Innovators community. To get this content directly in your inbox (not to mention&lt;/i&gt; &lt;a href="https://cloud.google.com/innovators/?utm_source=cgc-blog&amp;amp;utm_medium=blog&amp;amp;utm_campaign=FY23-Q4-Global-OPGGCWeekly-CGCBlog-EN&amp;amp;utm_content=opggc&amp;amp;utm_term=-"&gt;&lt;i&gt;lots of other benefits&lt;/i&gt;&lt;/a&gt;&lt;i&gt;),&lt;/i&gt; &lt;a href="https://cloud.google.com/innovators/?utm_source=cgc-blog&amp;amp;utm_medium=blog&amp;amp;utm_campaign=FY23-Q4-Global-OPGGCWeekly-CGCBlog-EN&amp;amp;utm_content=opggc&amp;amp;utm_term=-"&gt;&lt;i&gt;sign up to be an Innovator today&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p data-block-key="di9mo"&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-paragraph"&gt;&lt;h3 data-block-key="a3czl"&gt;&lt;b&gt;New and shiny&lt;/b&gt;&lt;/h3&gt;&lt;p data-block-key="6s4ls"&gt;&lt;i&gt;Three new things to know this week&lt;/i&gt;&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key="cdnqa"&gt;&lt;b&gt;Good Cloud services make hard things easier&lt;/b&gt;. Looking to send traffic directly into a VPC from Cloud Run? Up until now, you needed to set up a Serverless VPC Access connector. No more. Direct VPC egress is now GA and gives better performance. See the &lt;a href="https://cloud.google.com/blog/products/serverless/direct-vpc-egress-for-cloud-run-is-now-ga"&gt;blog post&lt;/a&gt;, and &lt;a href="https://cloud.google.com/run/docs/configuring/vpc-direct-vpc"&gt;docs&lt;/a&gt;.&lt;/li&gt;&lt;li data-block-key="9m8qe"&gt;&lt;b&gt;Find problems before your users do&lt;/b&gt;. I like our &lt;a href="https://cloud.google.com/monitoring/uptime-checks/introduction"&gt;synthetic monitoring&lt;/a&gt; feature in Cloud Monitoring, and now you can &lt;a href="https://cloud.google.com/monitoring/synthetic-monitors/broken-links"&gt;create a broken-link checker that drops screenshots into Cloud Storage&lt;/a&gt;. See what your customer would see, and fix broken links before they cause a big problem.&lt;/li&gt;&lt;li data-block-key="c1cnl"&gt;&lt;b&gt;Artifact Registry adds support for downloads, immutable tags&lt;/b&gt;. With Container Registry &lt;a href="https://cloud.google.com/artifact-registry/docs/transition/prepare-gcr-shutdown"&gt;riding into the sunset&lt;/a&gt; next year, all eyes are on Artifact Registry as the center of gravity. And this service keeps getting better. Now you can &lt;a href="https://cloud.google.com/artifact-registry/docs/repositories/download-files"&gt;download files&lt;/a&gt; for standard or remote repos, &lt;a href="https://cloud.google.com/artifact-registry/docs/repositories/create-repos"&gt;set immutable tags&lt;/a&gt; for Docker repos, and scan for &lt;a href="https://cloud.google.com/artifact-analysis/docs/container-scanning-overview"&gt;more language vulnerabilities&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;hr/&gt;&lt;h3 data-block-key="742le"&gt;&lt;b&gt;Watch this&lt;/b&gt;&lt;/h3&gt;&lt;/div&gt;
&lt;div class="block-video"&gt;



&lt;div class="article-module article-video "&gt;
  &lt;figure&gt;
    &lt;a class="h-c-video h-c-video--marquee"
      href="https://youtube.com/watch?v=JqejFF0OX1I"
      data-glue-modal-trigger="uni-modal-JqejFF0OX1I-"
      data-glue-modal-disabled-on-mobile="true"&gt;

      
        

        &lt;div class="article-video__aspect-image"
          style="background-image: url(https://storage.googleapis.com/gweb-cloudblog-publish/images/Watch_This_-_higher_resolution_but_missing.max-1000x1000.png);"&gt;
          &lt;span class="h-u-visually-hidden"&gt;A captured graphic of a Google Cloud webinar about observability and Cloud Run&lt;/span&gt;
        &lt;/div&gt;
      
      &lt;svg role="img" class="h-c-video__play h-c-icon h-c-icon--color-white"&gt;
        &lt;use xlink:href="#mi-youtube-icon"&gt;&lt;/use&gt;
      &lt;/svg&gt;
    &lt;/a&gt;

    
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class="h-c-modal--video"
     data-glue-modal="uni-modal-JqejFF0OX1I-"
     data-glue-modal-close-label="Close Dialog"&gt;
   &lt;a class="glue-yt-video"
      data-glue-yt-video-autoplay="true"
      data-glue-yt-video-height="99%"
      data-glue-yt-video-vid="JqejFF0OX1I"
      data-glue-yt-video-width="100%"
      href="https://youtube.com/watch?v=JqejFF0OX1I"
      ng-cloak&gt;
   &lt;/a&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="block-paragraph"&gt;&lt;p data-block-key="h9w35"&gt;&lt;b&gt;Instrument your Cloud Run app with Prometheus or OpenTelemetry metrics&lt;/b&gt;. Great session here from Lee and Kyle where you’ll learn about observability, Prometheus, OTel, and Cloud Run’s new sidecar feature.&lt;/p&gt;&lt;hr/&gt;&lt;h3 data-block-key="bi0t5"&gt;&lt;b&gt;Community cuts&lt;/b&gt;&lt;/h3&gt;&lt;p data-block-key="fiv9c"&gt;&lt;i&gt;Every week I round up some of my favorite links from builders around the Google Cloud-iverse. Want to see your blog or video in the next issue? Drop Richard a line!&lt;/i&gt;&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key="85rl7"&gt;&lt;b&gt;Send SMS messages with Cloud Functions for Firebase&lt;/b&gt;. I enjoy demonstrations that mash up different platforms, and &lt;a href="https://developer.vonage.com/en/blog/send-sms-messages-with-cloud-functions-for-firebase-gen-2" target="_blank"&gt;Amanda does a terrific job&lt;/a&gt; showing us how to build a Cloud Function that uses the Vonage API to send SMS messages.&lt;/li&gt;&lt;li data-block-key="c84r1"&gt;&lt;b&gt;Build an agent, ???, profit&lt;/b&gt;. Even if it’s just for fun, most of us will probably try and build an AI agent. &lt;a href="https://medium.com/google-cloud/google-cloud-vertexai-agent-the-chatbot-easier-first-overview-bf3249d711d1" target="_blank"&gt;Guillaume offers up an excellent walkthrough&lt;/a&gt; of the new Vertex AI Agent Builder. It actually inspired me to try it out too.&lt;/li&gt;&lt;li data-block-key="foslm"&gt;&lt;b&gt;Go behind the scenes on Dataflow&lt;/b&gt;. It’s not always required knowledge, but many of us like knowing the behind-the-scenes story about how something works. &lt;a href="https://medium.com/@vutrinh274/the-stream-processing-model-behind-google-cloud-dataflow-0d927c9506a0" target="_blank"&gt;Vu looks at a paper&lt;/a&gt; that digs into the design principles and implementation of Google Cloud Dataflow.&lt;/li&gt;&lt;/ul&gt;&lt;hr/&gt;&lt;h3 data-block-key="atitm"&gt;&lt;b&gt;Learn and grow&lt;/b&gt;&lt;/h3&gt;&lt;p data-block-key="93ovk"&gt;&lt;i&gt;Three ways to build your cloud muscles this week&lt;/i&gt;&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key="c4i0v"&gt;&lt;b&gt;Let’s do boring things that are very important&lt;/b&gt;. It’s 2024. We’re talking about accessing data cached in Redis from a Java app? You betcha. For every person building a generative AI app, there are likely fifty others applying this particular pattern. Check out &lt;a href="https://codelabs.developers.google.com/codelabs/cloud-spring-cache-memorystore?hl=en#0" target="_blank"&gt;this new code lab&lt;/a&gt; for accessing Memorystore data from a Spring Boot app.&lt;/li&gt;&lt;li data-block-key="f9490"&gt;&lt;b&gt;Can AI help us build better tests&lt;/b&gt;? Romin wants to build a synthetic monitor for his service. Can he use generative AI? With the new “help me code” button in this Cloud Monitoring experience, it actually works well. &lt;a href="https://medium.com/google-cloud/using-gemini-to-help-write-synthetic-monitoring-tests-in-google-cloud-359561a23230" target="_blank"&gt;Good walkthrough&lt;/a&gt;!&lt;/li&gt;&lt;li data-block-key="7p09c"&gt;&lt;b&gt;Let’s do complex AI tasks like RLHF, together&lt;/b&gt;. We need more “hello world” pieces out there, but we also need the level 400 content that helps us find the next gear. We’ve published a ton of new notebooks with advanced topics like reinforcement learning from human feedback (RLHF). &lt;a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_rlhf_tuning.ipynb" target="_blank"&gt;This notebook&lt;/a&gt; goes step by step through this process with the Llama2 model and Vertex AI services.&lt;/li&gt;&lt;li data-block-key="5tv3h"&gt;&lt;b&gt;All the Gemini for Google Cloud, all in one place&lt;/b&gt;. Romin gets a second shoutout in this newsletter for his outstanding work keeping &lt;a href="https://github.com/rominirani/awesome-gemini-for-google-cloud/blob/main/README.md" target="_blank"&gt;this curated list of resources up to date&lt;/a&gt;. Find product pages, videos, tutorials, blog posts and more that cover Gemini in Google Cloud.&lt;/li&gt;&lt;/ul&gt;&lt;hr/&gt;&lt;h3 data-block-key="ati7h"&gt;&lt;b&gt;One more thing&lt;/b&gt;&lt;/h3&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/One_More_Thing_pDPdXIs.max-1000x1000.png"
        
          alt="222 One More Thing.."&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="s5t1j"&gt;&lt;a href="https://twitter.com/nathaniel_avery/status/1783878451025616912 "&gt;https://twitter.com/nathaniel_avery/status/1783878451025616912&lt;/a&gt;&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph"&gt;&lt;p data-block-key="w6t8h"&gt;&lt;b&gt;It’s time for DORA survey. Make your voice heard&lt;/b&gt;! The &lt;a href="https://cloud.google.com/blog/products/devops-sre/2024-dora-survey-now-open?e=48754805"&gt;2024 DORA survey is open&lt;/a&gt;, and we’re expanding on the largest body of research into good software delivery practices. Contribute to his research with your experience.&lt;/p&gt;&lt;hr/&gt;&lt;p data-block-key="6lron"&gt;&lt;a href="https://cloud.google.com/innovators/?utm_source=cgc-blog&amp;amp;utm_medium=blog&amp;amp;utm_campaign=FY23-Q4-Global-OPGGCWeekly-CGCBlog-EN&amp;amp;utm_content=opggc&amp;amp;utm_term=-"&gt;&lt;i&gt;Become an Innovator&lt;/i&gt;&lt;/a&gt;&lt;i&gt; to stay up-to-date on the latest news, product updates, events, and learning opportunities with Google Cloud.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 14 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/gcp/the-overwhelmed-persons-guide-to-google-cloud/</guid><category>Application Modernization</category><category>Application Development</category><category>Google Cloud</category><media:content height="540" url="https://storage.googleapis.com/gweb-cloudblog-publish/images/General-GC_Blog_header_2436x1200-v1.max-600x600.jpg" width="540"></media:content><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>The overwhelmed person’s guide to Google Cloud: week of May 9</title><description></description><image>https://storage.googleapis.com/gweb-cloudblog-publish/images/General-GC_Blog_header_2436x1200-v1.max-600x600.jpg</image><site_name>Google</site_name><url>https://cloud.google.com/blog/products/gcp/the-overwhelmed-persons-guide-to-google-cloud/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Richard Seroter</name><title>Chief Evangelist, Google Cloud</title><department></department><company></company></author></item><item><title>Performing large-scale computation-driven drug discovery on Google Cloud</title><link>https://cloud.google.com/blog/topics/hpc/atommap-builds-elastic-supercomputer-on-google-cloud/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;Editor’s note:&lt;/strong&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt; Today we hear from Atommap, a computational drug discovery company that has built an elastic supercomputing cluster on the Google Cloud to empower large-scale, computation-driven drug discovery. Read on to learn more.&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Bringing a new medicine to patients typically happens in four stages: (1) &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;target identification&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that selects the protein target associated with the disease, (2) &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;molecular discovery&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that finds the new molecule modulating the function of the target, (3) &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;clinical trial&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that tests the candidate drug molecule’s safety and efficacy in patients, and (4) &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;commercialization&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that distributes the drug to patients in needs. The molecular discovery stage, in which novel drug molecules are invented, involves solving two problems: first, we need to establish an effective mechanism to modulate the target function that maximizes the therapeutic efficacy and minimizes the adverse effect; second, we need to design, select, and make the right drug molecule that faithfully implements the mechanism, is bioavailable, and has acceptable toxicity.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;What makes molecular discovery hard?&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;A protein is in constant thermal motion, which changes its shape (conformation) and binding partners (other biomolecules), thus affecting its functions. Structural detail of a protein’s conformational dynamics time and again suggests novel mechanisms of functional modulation. But such information often eludes experimental determination, despite tremendous progress in experimental techniques in recent years.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The chemical “universe” of all possible distinct small molecules — estimated to number 10&lt;/span&gt;&lt;sup&gt;&lt;span style="vertical-align: baseline;"&gt;&lt;span style="vertical-align: super;"&gt;60&lt;/span&gt;&lt;/span&gt;&lt;/sup&gt;&lt;span style="vertical-align: baseline;"&gt; (&lt;/span&gt;&lt;a href="https://doi.org/10.1039/C0MD00020E" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Reymond et al. 2010&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;) — is vast. Chemists have made probably ten billion so far, so we still have about 10&lt;/span&gt;&lt;sup&gt;&lt;span style="vertical-align: baseline;"&gt;&lt;span style="vertical-align: super;"&gt;60&lt;/span&gt;&lt;/span&gt;&lt;/sup&gt;&lt;span style="vertical-align: baseline;"&gt; to go.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;There lie two major challenges of molecular discovery and its endless opportunity: chances are that we have not considered all the mechanisms of action or found the best molecules, thus we can always invent a better drug. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Atommap’s computation-driven approach to molecular discovery&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Harnessing the power of high-performance computing, Atommap’s molecular engineering platform enables the discovery of novel drug molecules against previously intractable targets through new mechanisms, making the process faster, cheaper, and more likely to succeed. In past projects, Atommap’s platform has dramatically reduced both the time (by more than half) and cost (by 80%) of molecular discovery. For example, it played a pivotal role in advancing a molecule against a challenging therapeutic target to the clinical trial in 17 months (&lt;/span&gt;&lt;a href="https://clinicaltrials.gov/study/NCT04609579" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;NCT04609579&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;) and it substantially accelerated the discovery of novel molecules that degrade high-valued oncological targets (&lt;/span&gt;&lt;a href="https://doi.org/10.1021/acs.jcim.3c00603" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Mostofian et al. 2023&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Atommap achieves this by:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Advanced molecular dynamics (MD) simulations&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that unveil complex conformational dynamics of the protein target and its interactions with the drug molecules and other biomolecules. They establish the dynamics-function relationship for the target protein, which is instrumental to choosing the best mechanism of action for the drug molecules. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Generative models&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that enumerate novel molecules. Beginning with a three-dimensional blueprint of a drug molecule's interaction with its target, our models computationally generate thousands to hundreds of thousands of new virtual molecules, which are designed to form the desired interactions and to satisfy both synthetic feasibility and favorable drug-like properties. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Physics-based, ML-enhanced predictive models&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; that accurately predict molecular potencies and other properties. Every molecular design is evaluated computationally for its target-binding affinity, its effects on the target, and its drug-likeness. This allows us to explore many times more molecules than can be synthesized and tested in the wet lab, and to perform multiple rounds of designs while waiting for often-lengthy experimental evaluation, leading to compressed timelines and increased probability of success.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Computation as a Service and Molecular Discovery as a Service&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To truly, broadly impact drug discovery, Atommap needs to augment its deep expertise in molecular discovery by partnering with external expertise in the other stages — target identification, clinical trials, and commercialization. We form partnerships in two ways: Computation as a Service (CaaS) and Molecular Discovery as a Service (MDaaS, pronounced &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Midas&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;), which make it easy and economically attractive for every drug discovery organization to access our computation-driven molecular engineering platform. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Instead of selling software subscriptions, Atommap’s pay-as-you-go CaaS model lets any discovery project first try our computational tools at a small and affordable scale, without committing too much budget. Not every project is amenable to computational solutions, but most are. This approach allows every drug discovery project to introduce the appropriate computations cheaply and quickly, with demonstrable impact, and then deploy them at scale to amplify their benefits.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;For drug hunters who would like to convert their biological and clinical hypotheses into drug candidates, our MDaaS partnership allows them to quickly identify potent molecules with novel intellectual property for clinical trials. Atommap executes the molecular discovery project from the first molecule (initial hits) to the last molecule (development candidates), freeing our partners to focus on biological and clinical validation. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;The need for elastic computing&lt;/strong&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_apPqgNo.max-1000x1000.png"
        
          alt="image1"&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="eug34"&gt;Figure 1. Diverse computational tasks in Atommap’s molecular engineering platform require elastic computing resources.&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;For Atommap, the number of partnership projects and the scale of computation in each project fluctuate over time. In building structural models to enable structure-based drug design, we run hundreds of long-timescale MD simulations on high-performance GPUs to explore the conformational ensembles of proteins and complexes between proteins and small molecules, each of which can last hours to days. Our NetBFE platform for predicting the binding affinities invokes thousands, sometimes tens of thousands, of MD simulations, although each one is relatively short and completes in a few hours. Atommap’s machine learning (ML) models take days to weeks to train on high-memory GPUs, but once trained and deployed in a project, run in seconds to minutes. Balancing the different computational loads associated with different applications poses a challenge to the computing infrastructure. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To meet this elastic demand, we chose to supplement our internal computer clusters with &lt;/span&gt;&lt;a href="https://cloud.google.com/"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;How to build an elastic supercomputer on Google Cloud&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;It took us several steps to move our computing platform from our internal cluster to a hybrid environment that includes Google Cloud.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style="vertical-align: baseline;"&gt;Slurm&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Many workflows in our platform depended on &lt;/span&gt;&lt;a href="https://slurm.schedmd.com/overview.html" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Slurm&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; for managing the computing jobs. To migrate to Google Cloud, we built a cloud-based Slurm cluster using &lt;/span&gt;&lt;a href="https://cloud.google.com/hpc-toolkit/docs/overview"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud HPC Toolkit&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, an open-source utility developed by Google. Cloud HPC Toolkit is a command line tool that makes it easy to stand up connected and secure cloud HPC systems. With this Slurm cluster up and running in minutes, we quickly put it to use with our Slurm-native tooling to set up computing jobs for our discovery projects.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Cloud HPC Toolkit naturally fits our DevOps function into best practices. We defined our compute clusters as “blueprints” within YAML files that allow us to simply and transparently configure specific details of individual Google Cloud products. The Toolkit transpiles blueprints into input scripts that are executed with Hashicorp’s &lt;/span&gt;&lt;a href="https://www.terraform.io/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Terraform&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, an industry standard tool for defining “infrastructure-as-code” such that it can be committed, reviewed, and version-controlled. Within the blueprint we also defined our compute machine image through a startup script that’s compatible with Hashicorp’s &lt;/span&gt;&lt;a href="https://www.packer.io/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Packer&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. This allowed us to easily “bake in” the software our jobs typically need, such as conda, Docker, and Docker container images that provide dependencies such as &lt;/span&gt;&lt;a href="https://ambermd.org/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;AMBER&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, &lt;/span&gt;&lt;a href="https://openmm.org/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;OpenMM&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, and &lt;/span&gt;&lt;a href="https://pytorch.org/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;PyTorch&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The deployed Slurm cloud system is as accessible and user-friendly as any Slurm system we have used before. The compute nodes are not deployed until requested and are spun down when finished, thus we only pay for what we use; the only persistent nodes are the head and controller nodes that we log into and deploy from.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Batch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Compared to Slurm, the cloud-native Google &lt;/span&gt;&lt;a href="https://cloud.google.com/batch"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Batch&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; gives us even greater flexibility in accessing the computing resources. Batch is a managed cloud job-scheduling service, meaning it can be used to schedule cloud resources for long-running scientific computing jobs. Virtual machines that Batch spins up can easily mount either NFS stores or &lt;/span&gt;&lt;a href="https://cloud.google.com/storage/docs/json_api/v1/buckets"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud Storage buckets&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, the latter of which are particularly suitable for holding our multi-gigabyte MD trajectories and thus useful as output directories for our long-running simulations.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Running our workflows on Google Cloud through the Batch involves two steps: 1) copying the input files to Google Cloud storage, 2) submitting the batch job.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;$ gcloud storage cp -R ./local_input_dir gs://my-gcs-bucket/work_dir\r\n$ gcloud batch jobs submit example-job --config=./job_cfg.json --location=us-central1&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b49f9c40&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;SURF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;A common pattern has emerged in most of our computational workflows. First, each job has a set of complex input files, including the sequence and structures of the target protein, a list of small molecules and their valence and three-dimensional structures, and the simulation and model parameters. Second, most computing jobs take hours to days to finish even on the highest-performance machines. Third, the computing jobs produce output datasets of substantial volume and subject to a variety of analyses.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Accordingly, we have recently developed a new computing infrastructure, SURF (submit, upload, run, fetch), which seamlessly integrates our internal cluster and Google Cloud through one simple interface and automatically brings the data to where it is needed by computation or the computation to where the data resides. &lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;$ am-surf submit my_job/ --num-cpus 16 --num-gpus 8 --where [gcp|internal]&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b49f9cd0&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;SURF submits jobs to Google Cloud Batch using Google’s Python API.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We now have an elastic supercomputer on the cloud that gives us massive computing power when we need it. It empowers us to explore the vast chemical space at an unprecedented scale and to invent molecules that better human health and life.&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;sup&gt;&lt;em&gt;&lt;span style="vertical-align: baseline;"&gt;Andrew Sabol supported us from the very beginning of Atommap, even before we knew whether we could afford the computing bills. Without the guidance and technical support of Vincent Beltrani, Mike Sabol, and other Google colleagues, we could not have rebuilt our computing platform on Google Cloud in such a short time. Our discovery partners put their trust in our young company and our burgeoning platform; their collaborations helped us validate our platform in real discovery projects and substantially improve its throughput, robustness, and predictive accuracy. &lt;/span&gt;&lt;/em&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 13 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/topics/hpc/atommap-builds-elastic-supercomputer-on-google-cloud/</guid><category>Customers</category><category>HPC</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Performing large-scale computation-driven drug discovery on Google Cloud</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/topics/hpc/atommap-builds-elastic-supercomputer-on-google-cloud/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Huafeng Xu</name><title>CEO, Atommap</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Christopher Ryan</name><title>Director of Machine Learning and Data Sciences, Atommap</title><department></department><company></company></author></item><item><title>Data democratization with Dataplex: Implementing a data mesh architecture</title><link>https://cloud.google.com/blog/products/data-analytics/using-bigquery-dataplex-to-build-a-data-mesh/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;&lt;strong&gt;Editor’s note&lt;/strong&gt;: Today’s blog post was written in collaboration with Google Cloud Premier Partner &lt;/span&gt;&lt;a href="https://www.virtusa.com/" rel="noopener" target="_blank"&gt;&lt;span style="font-style: italic; text-decoration: underline; vertical-align: baseline;"&gt;Virtusa&lt;/span&gt;&lt;/a&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;, which specializes in building cloud-native, microservices-based, pre-built solutions and APIs on Kubernetes, as well as machine learning and data-oriented applications, as well as offering managed cloud services and cloud operations design.&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In the ever-evolving world of data engineering and analytics, traditional centralized data architectures are facing limitations in scalability, agility, and governance. To address these challenges, a new paradigm, called “data mesh,” has emerged, which allows organizations to take a decentralized approach to data architecture. This blog post explores data mesh as a concept and delineates the ways that &lt;/span&gt;&lt;a href="https://cloud.google.com/dataplex"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Dataplex&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, a data fabric capability within the BigQuery suite, can be used to realize the benefits of this decentralized data architecture.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Data mesh is an architectural framework that promotes the idea of treating data as a product and decentralizes data ownership and infrastructure. It enables teams across an organization to be responsible for their own data domains, allowing for greater autonomy, scalability, and data democratization. Instead of relying on a centralized data team, individual teams or data products take ownership of their data, including its quality, schema, and governance. This distributed responsibility model leads to improved data discovery, easier data integration, and faster insights.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The illustration in Figure 1 is an overview of data mesh’s fundamental building blocks.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Image-1.max-1000x1000.png"
        
          alt="Image-1"&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="ib6z8"&gt;Figure 1: Representation of a data mesh concept&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Let’s discuss the core principles of data mesh architecture, then understand how they change the way we manage and leverage data.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Image-2.max-1000x1000.png"
        
          alt="Image-2"&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="ib6z8"&gt;Figure 2: Core principals of data mesh architecture&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Domain-oriented ownership:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Data mesh emphasizes decentralizing data ownership and the allocation of responsibility to individual domains or business units within an organization. Each domain takes responsibility for managing its own data, including data quality, access controls, and governance. By doing so, domain experts are empowered, fostering a sense of ownership and accountability. This principle aligns data management with the specific needs and knowledge of each domain, ensuring better data quality and decision-making.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Self-serve data infrastructure:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; In a data mesh architecture, data infrastructure is treated as a product that provides self-serve capabilities to domain teams. Instead of relying on a centralized data team or platform, domain teams have the autonomy to choose and manage their own data storage, processing, and analysis tools. This approach allows teams to tailor their data infrastructure to their specific requirements, accelerating their workflows and reducing dependencies on centralized resources.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Federated computational governance: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Data governance in a data mesh is not dictated by a central authority; rather, it follows a federated model. Each domain team collaboratively defines and enforces data governance practices that align with their specific domain requirements. This approach ensures that governance decisions are made by those closest to the data, and it allows for flexibility in adapting to domain-specific needs. Federated computational governance promotes trust, accountability, and agility in managing data assets.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Data as a product:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Data in a data mesh is treated as a product, and data platforms are built and managed with a product mindset. This means focusing on providing value to the end users (domain teams) and continuously iterating and improving the data infrastructure based on feedback. When teams adopt a product thinking approach, data platforms become user-friendly, reliable, and scalable. They evolve in response to changing requirements and deliver tangible value to the organization.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Understanding Dataplex&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Dataplex is a cloud-native, intelligent data fabric platform designed to simplify and streamline the management, integration, and analysis of large and complex data sets. It offers a unified approach to data governance, data discovery, and data lineage, enabling organizations to gain more value from their data.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Image-3.max-1000x1000.png"
        
          alt="Image-3"&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="ib6z8"&gt;Figure 3: Google Cloud Dataplex capabilities&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Key features and benefits of Dataplex&lt;/span&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt; &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;include data integration from various sources into a unified data fabric; robust data governance capabilities that help ensure security and compliance; intelligent data discovery tools for enhanced data visibility and accessibility; scalability and flexibility to handle large volumes of data in real-time; multi-cloud support for leveraging data across different cloud providers; and efficient metadata management for improved data organization and accessibility.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Steps to implement data mesh using Dataplex&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 1: Create a data lake and define the data domain.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this step, we set up a data lake on Google Cloud and establish the data domain, which refers to the scope and boundaries of the data that will be stored and managed in the data lake. A data lake is a centralized repository that allows you to store structured, semi-structured, and unstructured data in its native format, making it a flexible and scalable solution for big data storage and analytics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The following diagram illustrates domains as Dataplex lakes, each owned by distinct data producers. Within their respective domains, data producers maintain control over creation, curation, and access. Conversely, data consumers have the ability to request access to these lakes (domains) or specific zones (subdomains) to conduct their analysis.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Image-4.max-1000x1000.png"
        
          alt="Image-4"&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="ib6z8"&gt;Figure 4: Decentralized data with defined ownership&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 2: Create zones in your data lake and define the data zones.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this step, we divide the data lake into zones. Each zone serves a specific purpose and has well-defined characteristics. Zones help organize data based on factors like data type, access requirements, and processing needs. Creating data zones provides better data governance, security, and efficiency within the data lake environment. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Common data zones include the following:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Raw zone&lt;/strong&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; This zone is dedicated to ingesting and storing raw, unprocessed data. It is the landing zone for new data as it enters the data lake. Data in this zone is typically kept in its native format, making it ideal for data archival and data lineage purposes.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Curated zone:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The curated zone is where data is prepared and cleansed before it moves to other zones. This zone may involve data transformation, normalization, or deduplication to ensure data quality.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Transformed zone:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The transformed zone holds high-quality, transformed, and structured data that is ready for consumption by data analysts and other users. Data in this zone is organized and optimized for analytical purposes.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Qm8Yf3U.max-1000x1000.png"
        
          alt="Image-5"&gt;
        
        &lt;/a&gt;
      
        &lt;figcaption class="article-image__caption "&gt;&lt;p data-block-key="ib6z8"&gt;Figure 5: Data zones inside a data lake&lt;/p&gt;&lt;/figcaption&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 3: Add assets to the data lake zones.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt; In this step, we focus on adding assets to the different data lake zones. Assets refer to the data files, data sets, or resources that are ingested into the data lake and stored within their respective zones. By adding assets to the zones, you populate the data lake with valuable data that can be utilized for analysis, reporting, and other data-driven processes.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 4: Secure your data lake. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this step, we implement robust security measures to safeguard your data lake and the sensitive data it holds. A secure data lake is crucial for protecting sensitive information, helping to ensure compliance with data regulations, and maintaining the trust of your users and stakeholders.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The security model in Dataplex enables you to control access for performing the following tasks:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Managing a data lake, which involves tasks such as creating and associating assets, defining zones, and setting up additional data lakes&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt; Retrieving data linked to a data lake via the mapped asset (e.g., BigQuery data sets and storage buckets)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt; Retrieving metadata associated with the data linked to a data lake&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The administrator of a data lake manages access to Dataplex resources (including the lake, zones, and assets) by assigning the necessary basic and predefined roles.  Metadata roles possess the capability to access and examine metadata, including table schemas. With data roles granted, it gives the privilege to read or write data in the underlying resources referenced by the assets within the data lake.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Advantages of building a data mesh&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Improved data ownership and accountability:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; One of the primary advantages of a data mesh is the shift in data ownership and accountability to individual domain teams. By decentralizing data governance, each team becomes responsible for the quality, integrity, and security of their data products. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Agility and flexibility&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;: Data meshes empower domain teams to be autonomous in their decision-making, allowing them to respond swiftly to evolving business needs. This agility enables faster time to market for new data products and iterative improvements to existing ones. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Scalability and reduced bottlenecks:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; A data mesh eliminates scalability bottlenecks by distributing data processing and analysis across domain teams. Each team can independently scale its data infrastructure based on its specific needs, ensuring efficient handling of increasing data volumes.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Enhanced data discoverability and accessibility:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Data meshes emphasize metadata management, enabling better data discoverability and accessibility. With comprehensive metadata, teams can easily locate and understand available data assets. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Empowerment and collaboration:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; By distributing data knowledge and decision-making authority, domain experts are empowered to make data-driven decisions aligned with their business objectives. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Scalable data infrastructure:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; With the rise of cloud technologies, data meshes can take advantage of scalable cloud-native infrastructure. Leveraging cloud services, such as serverless computing and elastic storage, enables organizations to scale their data infrastructure on-demand, ensuring optimal performance and cost-efficiency.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Comprehensive and robust&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;data governance&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;:&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;Dataplex offers an extensive solution for data governance, ensuring security, compliance, and transparency throughout the data lifecycle. With fine-grained access controls, encryption, and policy-driven data management, Dataplex enhances data security and facilitates adherence to regulatory requirements. The platform provides visibility into the entire data lifecycle through lineage tracking, promoting transparency and accountability. Organizations can enforce standardized governance policies, ensuring consistency and reliability across their data landscape. Dataplex's tools for data quality monitoring and centralized data catalog governance further contribute to effective data governance practices.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Learn more&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;By embracing the principles of decentralization, data ownership, and autonomy, businesses can unlock a range of benefits, including improved data quality, greater accountability, and enhanced agility, scalability, and decision-making. Embracing this innovative approach can position organizations at the forefront of the data revolution, driving growth, innovation, and a competitive advantage. Learn more about &lt;/span&gt;&lt;a href="https://cloud.google.com/partners/ai"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud’s open generative AI partner ecosystem&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. To get started with Google Cloud and Virtusa and to learn more about building a data mesh using Dataplex, &lt;/span&gt;&lt;a href="https://cloud.google.com/contact/"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;contact us&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; today.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 13 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/data-analytics/using-bigquery-dataplex-to-build-a-data-mesh/</guid><category>Partners</category><category>Data Analytics</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Data democratization with Dataplex: Implementing a data mesh architecture</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/data-analytics/using-bigquery-dataplex-to-build-a-data-mesh/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Suhrid Saran</name><title>Senior Solution Architect, Data and Analytics, Virtusa</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Saurabh Dubey</name><title>Partner Engineer, Google Cloud</title><department></department><company></company></author></item><item><title>Built with BigQuery: Making data activation and monetization accessible with Optable</title><link>https://cloud.google.com/blog/products/data-analytics/optable-data-clean-room-platform-integrates-with-bigquery/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;Editor’s note&lt;/strong&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;: The post is part of a series showcasing tech companies and data providers that are &lt;/span&gt;&lt;a href="https://cloud.google.com/solutions/data-cloud-isvs"&gt;&lt;span style="font-style: italic; text-decoration: underline; vertical-align: baseline;"&gt;Built with BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In recent years, changes in consumer privacy regulations have created disruption within the media and advertising ecosystem. Publishers have felt some of the biggest impact, needing to change the way they think about ad monetization so they can continue to respe&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;ct u&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;ser privacy, reduce reliance on third parties, and build sustainable revenue growth. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Key to helping publishers navigate these challenges is their data management and collaboration technologies, which enable them to utilize private identity data, extract meaningful business insights, and safely scale data activation. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.optable.co/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Optable&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is the maker of an end-to-end data clean room platform for the advertising industry that integrates with &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and enables audience activation and insights through connections with downstream systems.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Optimizing data management for monetization &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The methods and systems that support audience-focused ad planning, activation, and measurement have undergone a massive shift. As a result, publishers are needing to reevaluate their data strategies, shifting their investments to cloud computing and big data. Unfortunately, a lack of innovation in data management platforms (DMPs) for publishers has stifled revenue growth and created inefficiencies in operations. This lack of ROI in legacy DMPs comes from the following challenges:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Platforms can’t deliver insights across known users, anonymous traffic, and ad data. &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;A shift across the media landscape to subscription-based approaches and user consent has meant publishers have an influx of first-party data from their audiences. This change means they need new types of privacy-centric tools to not only manage this data, but also combine it with other datasets to derive insights for both internal business operations as well as advertising partners.&lt;/span&gt;&lt;/li&gt;
&lt;li role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Identity management can consume the limited resources of modern publishers and media companies&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;. As media consumption has continued to fragment, identifying your audience to deliver relevant media and advertising has become a struggle. Many customer data platforms (CDPs) and legacy DMPs rely on systems that are too rigid and profile-based. This makes it difficult to understand true audience reach and attributes, and leaves analytics teams with incomplete or error-prone data sets.&lt;/span&gt;&lt;/li&gt;
&lt;li role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;There’s a lack of privacy-centric data monetization tools and integrations for publishers&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;. Given the rapid onset of privacy changes, many publishers are navigating a &lt;/span&gt;&lt;a href="https://adage.com/article/opinion/digital-marketing-2024-how-privacy-hurdles-and-signal-loss-will-drive-innovation-across-industry/2542421" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;reduction in addressability, signal loss, and programmatic revenues&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. Current technologies lack interoperability with second-party datasets, advertising ID providers, and ad delivery systems to support direct sales growth.&lt;/span&gt;&lt;/li&gt;
&lt;li role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Legacy solutions are not extendable and interoperable across a growing cloud ecosystem&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt; &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Data management solutions have not kept up with innovation in the cloud computing ecosystem. This makes data extensibility into native cloud tools for things like modeling, custom queries and data visualization is difficult without cumbersome data copying. Additionally, most data management platforms lack native functionality with cloud-based clean room capabilities, such as data analysis rules and egress restrictions, across all major cloud providers. &lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Privacy-safe addressable ad products for publishers&lt;/strong&gt;&lt;/h3&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1._Optable_Overview.max-1000x1000.png"
        
          alt="1. Optable Overview"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Optable is a next-generation data management and collaboration platform that is privacy-focused by design and built to harness the power of today's big data and cloud computing ecosystem. To achieve this, Optable focuses on three key areas of innovation.&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;1. &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Composable identity&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;br/&gt;&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;The Optable platform simplifies the complexities of processing and analyzing audience identity data in the age of privacy. By creating a flexible identity system, publishers can shape their audience graph for maximum accuracy and addressability. Optable also makes it easy to connect alternative identifiers such as UID 2.0 or ID5 to your audience data so that you can understand the full spectrum of advertiser demand and maximize revenue. It’s also straightforward to enrich audience data through connecting to second-party data sources, such as &lt;/span&gt;&lt;a href="https://www.truedata.co/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;True Data&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;2. &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Interoperability focused on monetization&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Optable makes monetization easy. It’s built the tools and integrations needed to keep up with programmatic ad-tech ecosystem changes. Audience data is fully interoperable through the Google Cloud ecosystem (and beyond), which means it’s easier to activate through ad servers like &lt;/span&gt;&lt;a href="https://admanager.google.com/intl/en_uk/home/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Ad Manager&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; (GAM) or &lt;/span&gt;&lt;a href="https://marketingplatform.google.com/intl/en_uk/about/display-video-360/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google DV 360&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, or use pre-built clean room applications within BigQuery. This combination means that direct sales and ad operations teams can quickly and easily move from ad partner planning, to audience building and insights, to actual campaign activation.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2._Optable_Data_flow.max-1000x1000.png"
        
          alt="2. Optable Data flow"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;3. &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Audience insights across all known users and event traffic&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;br/&gt;&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;Audience building and analysis with Optable provides a comprehensive 360-degree view without the massive upfront engineering that’s usually required. By allowing audiences to be created, synthesized, and managed using both known user data as well as anonymous events (such as page visits or ad serving events from GAM), operational teams can glean meaningful insights, service unique requests from ad partners, and optimize campaign performance. For example, in the image below, Optable Insights shows customers key information about the size and makeup of their private identity graph as well as the makeup of Traits across their audience.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3._Optable_Insights_Screenshot.max-1000x1000.png"
        
          alt="3. Optable Insights Screenshot"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;One of Optable’s early customers, a major North American news publisher, developed a digital-first publication strategy in 2017. As part of this they built an ad-sales strategy focused on privacy-safe first-party data monetization. A key pillar to their strategy is data collaboration through Optable. Through this investment they achieved a 9% annual increase in ad revenue in 2022.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;How Optable creates growth and efficiency&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Optable on BigQuery empowers publishers to monetize their data assets, even if they lack engineering resources&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;. &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;Optable provides the right tools and integrations so teams can use their data to create ad products. For example, audience data can be exported directly into &lt;/span&gt;&lt;a href="https://support.google.com/google-ads/answer/14639041?hl=en-GB" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Ads Data Manager&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; for easy activation.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The data clean room is one of the most important technologies in the media and advertising space. The Optable platform gives publishers pre-built applications that are powered by BigQuery data clean room primitives and APIs. These can also easily be extended to power secure data collaboration across other cloud ecosystems using Optable’s ‘Flash Nodes’ and ‘Flash Connectors.’ Flash Nodes allow companies to invite partners to easily onboard their data into a limited version of Optable simply for the purpose of collaborating with that partner; this reduces the friction of setting up a whole new collaboration platform. Likewise, ‘Flash Connectors’ give companies a set of primitives that can be shared with partners who use AWS, Snowflake, and BigQuery so that Optable users can collaborate directly with partners who house their data in those environments without moving any data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Rather than just focusing on moving data in and out, Optable’s platform is built for a data-warehouse centric world, so publishers can extend their capabilities. Identity graphs can be shaped and custom modeling easily deployed directly in BigQuery. Additionally, the capabilities of the Google Data Cloud enable custom use cases such as data visualization in &lt;/span&gt;&lt;a href="https://cloud.google.com/looker?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Looker&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. To help navigate data security compliance, Optable has built ‘Bring Your Own Account’ functionality to allow customers to utilize a Google BigQuery instance that’s fully controllable.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4._Optable_Marketecture.max-1000x1000.png"
        
          alt="4. Optable Marketecture"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Optable partners with Google across many areas, including a new integration with the &lt;/span&gt;&lt;a href="https://developers.google.com/privacy-sandbox" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Privacy Sandbox&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; APIs that enable publishers to easily onboard and activate audiences as well as enabling marketers to run campaigns via Privacy Sandbox. An early access program enables these capabilities directly through the Optable platform. You can &lt;/span&gt;&lt;a href="https://www.optable.co/google-privacy-sandbox" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;learn more here&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Later this year, Optable is announcing enhancements to its audience data management and collaboration capabilities, including support for ad serving events through Google Ad Manager and the release of the Google BigQuery Connector to allow for zero-copy partner collaboration. For more information on these and other capabilities please visit the &lt;/span&gt;&lt;a href="https://www.optable.co/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Optable website&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Optable also recently became a&lt;/span&gt;&lt;a href="https://cloud.google.com/marketplace?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt; Google Cloud Marketplace&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; vendor, so Google Cloud customers can now purchase and implement Optable’s platform directly. Following the recently announced &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/data-analytics/bigquery-data-clean-rooms-now-generally-available?e=48754805#:~:text=To%20facilitate%20this%20type%20of,to%20protect%20the%20underlying%20data."&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;general availability of BigQuery data clean rooms&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, the Optable and Google teams are working on an integration to unlock more planning, activation and measurement use cases for the media and advertising ecosystem.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;The Built with BigQuery advantage&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Built with BigQuery helps companies like Optable build innovative applications with Google Data Cloud. Participating companies can: &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Accelerate product design and architecture through access to designated experts who can provide insight into key use cases, architectural patterns, and best practices. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Amplify success with joint marketing programs to drive awareness, generate demand, and increase adoption. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;BigQuery gives ISVs the advantage of a powerful, highly scalable unified AI lakehouse that’s integrated with Google Cloud’s open, secure, sustainable platform. &lt;/span&gt;&lt;a href="https://cloud.google.com/solutions/data-cloud-isvs"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Click here to learn more about Built with BigQuery&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Mon, 13 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/data-analytics/optable-data-clean-room-platform-integrates-with-bigquery/</guid><category>Data Analytics</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Built with BigQuery: Making data activation and monetization accessible with Optable</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/data-analytics/optable-data-clean-room-platform-integrates-with-bigquery/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Bennett Crumbling</name><title>Head of Marketing, Optable</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Hamza El-Ghoujdami</name><title>Cloud Tech Advisor, Google Cloud</title><department></department><company></company></author></item><item><title>Firestore integration with Eventarc reaches GA with Auth Context</title><link>https://cloud.google.com/blog/products/databases/firestore-eventarc-integration-now-ga-with-auth-context/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Creating event-driven architectures using &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Eventarc&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; together with &lt;/span&gt;&lt;a href="https://firebase.google.com/docs/firestore" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Firestore&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is an increasingly popular pattern. Recently, the &lt;/span&gt;&lt;a href="https://cloud.google.com/datastore/docs/eventarc"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Firestore integration with Eventarc&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; became generally available, adding new functionality. You can now register multiple Cloud Functions in different regions against a multi-regional Firestore database for increased reliability, and there are new event types, including the &lt;/span&gt;&lt;a href="https://github.com/cloudevents/spec/blob/main/cloudevents/extensions/authcontext.md" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Auth Context extension for CloudEvents&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Determining who or what — the user, a service account, the system, or a third-party — is making a modification to a Firestore document as a change event has long been a top-requested feature. With the new Firestore event types with Auth Context extension, events now embed metadata about the principal that triggered a document change in the open and portable &lt;/span&gt;&lt;a href="https://github.com/googleapis/google-cloudevents" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;CloudEvents&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; format.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Example walkthrough&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Let’s say that you want to have different logic to process events in the destinations for different auth contexts (i.e. &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;unauthenticated&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; or &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;system&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;). To set up your trigger, navigate to the &lt;/span&gt;&lt;a href="https://cloud.google.com/functions/docs/calling/eventarc#deployment"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Eventarc section of the Google Cloud console&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. You’ll need to create a new &lt;/span&gt;&lt;a href="https://cloud.google.com/datastore/docs/eventarc"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;trigger&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; for Firestore using the associated event types that include authentication information. These event types end with the suffix *.withAuthContext. We’ll want to capture newly written entities, so we’ll select  &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;google.cloud.firestore.document.v1.written.withAuthContext &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;events&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_01.max-1000x1000.png"
        
          alt="blog_post_01"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can specify additional filters, which ensures only desirable events from a specified database and collection are delivered. In this case, we filter for events from the &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;(default) &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;database, and for documents of the collection &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Ops&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;On the same screen, you’ll also need to specify a destination. Triggering events can be delivered to any number of supported Eventarc destinations, like &lt;/span&gt;&lt;a href="https://cloud.google.com/eventarc/docs/run/route-trigger-cloud-firestore"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Run&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, &lt;/span&gt;&lt;a href="https://cloud.google.com/functions/docs/calling/cloud-firestore"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Functions (2nd gen)&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, and &lt;/span&gt;&lt;a href="https://cloud.google.com/eventarc/docs/gke/route-trigger-cloud-firestore"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Kubernetes Engine&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. Let’s say we have a Cloud Run service named &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;demo &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;that exposes an HTTP endpoint to receive the events. You can configure your trigger as follows:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_02.max-1000x1000.png"
        
          alt="blog_post_02"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;That’s it! When any write operation is applied to your &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;(default)&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; database with the collection &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Ops&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;, a CloudEvent with the Auth Context is delivered to the configured Cloud Run service &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;demo&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; almost immediately. You can inspect the &lt;/span&gt;&lt;code style="vertical-align: baseline;"&gt;authtype&lt;/code&gt;&lt;span style="vertical-align: baseline;"&gt; attribute as defined in &lt;/span&gt;&lt;a href="https://github.com/cloudevents/spec/blob/main/cloudevents/extensions/authcontext.md" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Auth Context extension&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to identify &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;unauthenticated&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;system &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;types as shown in &lt;/span&gt;&lt;a href="https://cloud.google.com/firestore/docs/extend-with-functions-2nd-gen#event_attributes"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/firestore/docs/extend-with-functions-2nd-gen#event_attributes&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; . &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;Next steps&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;For more information on how to set up and configure Firestore triggers, check out our &lt;/span&gt;&lt;a href="https://cloud.google.com/datastore/docs/eventarc"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;documentation&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;sup&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Thanks to both Minh Nguyen, Senior Product Manager Lead for Firestore and Juan Lara, Senior Technical Writer for Firestore, for their contributions to this blog post.&lt;/span&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-related_article_tout"&gt;





&lt;div class="uni-related-article-tout h-c-page"&gt;
  &lt;section class="h-c-grid"&gt;
    &lt;a href="https://cloud.google.com/blog/products/databases/firestore-triggers-for-cloud-run-and-google-kubernetes-engine/"
       data-analytics='{
                       "event": "page interaction",
                       "category": "article lead",
                       "action": "related article - inline",
                       "label": "article: {slug}"
                     }'
       class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6
        h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker"&gt;
      &lt;div class="uni-related-article-tout__inner-wrapper"&gt;
        &lt;p class="uni-related-article-tout__eyebrow h-c-eyebrow"&gt;Related Article&lt;/p&gt;

        &lt;div class="uni-related-article-tout__content-wrapper"&gt;
          &lt;div class="uni-related-article-tout__image-wrapper"&gt;
            &lt;div class="uni-related-article-tout__image" style="background-image: url('')"&gt;&lt;/div&gt;
          &lt;/div&gt;
          &lt;div class="uni-related-article-tout__content"&gt;
            &lt;h4 class="uni-related-article-tout__header h-has-bottom-margin"&gt;Firestore adds three new trigger destinations through an integration with Eventarc&lt;/h4&gt;
            &lt;p class="uni-related-article-tout__body"&gt;Firestore adds support for 3 new trigger destinations (Cloud Run, Cloud Functions Gen 2, Google Kubernetes Engine) through an integration...&lt;/p&gt;
            &lt;div class="cta module-cta h-c-copy  uni-related-article-tout__cta muted"&gt;
              &lt;span class="nowrap"&gt;Read Article
                &lt;svg class="icon h-c-icon" role="presentation"&gt;
                  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#mi-arrow-forward"&gt;&lt;/use&gt;
                &lt;/svg&gt;
              &lt;/span&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  &lt;/section&gt;
&lt;/div&gt;

&lt;/div&gt;</description><pubDate>Mon, 13 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/databases/firestore-eventarc-integration-now-ga-with-auth-context/</guid><category>Application Development</category><category>Containers &amp; Kubernetes</category><category>Serverless</category><category>Databases</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Firestore integration with Eventarc reaches GA with Auth Context</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/databases/firestore-eventarc-integration-now-ga-with-auth-context/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Josué Urbina</name><title>Software Engineer, Firestore</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Hansi Mou</name><title>Software Engineer, Firestore</title><department></department><company></company></author></item><item><title>AlloyDB vs. self-managed PostgreSQL: a price-performance comparison</title><link>https://cloud.google.com/blog/products/databases/alloydb-vs-self-managed-postgresql-a-price-performance-comparison/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;a href="https://www.postgresql.org/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;PostgreSQL&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is an advanced open-source, relational database management system that empowers businesses with robust, reliable, and scalable database solutions. It has a rich set of features including security, availability, and performance that ensure data integrity and business continuity. With its extensive extensibility and vibrant community support, PostgreSQL offers a flexible and cost-effective alternative to proprietary database systems, making it an ideal choice for organizations seeking a versatile database foundation for their critical applications.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/alloydb/docs/overview"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;AlloyDB for PostgreSQL&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, meanwhile, is Google Cloud’s fully managed, enterprise-grade database service that adds several enhancements to the standard PostgreSQL database engine while maintaining full compatibility: AlloyDB can handle the most demanding workloads with superior performance, high availability, and scalability, outperforming standard PostgreSQL. Further, AlloyDB leverages Google's differentiated infrastructure and machine learning for automated database management, intelligent resource optimization, and accelerated analytical queries, making it an attractive choice for organizations that need a powerful and streamlined PostgreSQL-compatible solution. So, which should you choose?&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1st-image-blog-2x-perf-andi.max-1000x1000.png"
        
          alt="1st-image-blog-2x-perf-andi"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;At Google Cloud Next 24, we &lt;/span&gt;&lt;a href="https://youtu.be/jR2YY9_IzF4?feature=shared&amp;amp;t=334" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;showcased&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; AlloyDB provides&lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt; up to 2x&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; better price-performance than self-managed PostgreSQL and &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;up to 4x&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; better transactional performance on the same-sized instance. This blog dives deeper into the data behind that comparison, and explores additional value AlloyDB delivers.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span style="vertical-align: baseline;"&gt;Comparing performance&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With AlloyDB, we made several enhancements to PostgreSQL's kernel including transactional and query processing layers and added self-managing autopilot features such as automatic memory management and adaptive vacuum management. When coupled with its intelligent distributed storage system and automated storage tiering, AlloyDB performs far more efficiently than standard PostgreSQL. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We carried out a performance comparison evaluation to contrast the cost-effectiveness of AlloyDB with self-managed PostgreSQL running on Google Compute Engine (GCE), both in terms of total cost of ownership (TCO), as well as cost per transaction. For our evaluation, we compared AlloyDB for PostgreSQL configured with HA on 8 vCPU / 64 GB Memory with self-managed PostgreSQL 15 running on two VMs configured for HA, each with 16 vCPU / 128GB Memory: &lt;/span&gt;&lt;/p&gt;
&lt;div align="left"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col/&gt;&lt;col/&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt;Configuration&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Type of workload&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;TPCC-Like (Transactional)&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;AlloyDB Pricing (vCPU, memory, storage)&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/alloydb/pricing"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/alloydb/pricing&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Compute Engine Pricing &lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/compute/all-pricing#general_purpose"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/compute/all-pricing#general_purpose&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;PostgreSQL &lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;PostgreSQL 15 https://www.postgresql.org/download/&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;# of clients&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;256 connections&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Database runs&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;3,200 warehouse / 347 GB database size&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Type of benchmark&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;TPCC-like with 30% data fit in the memory&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The table and chart below shows the price/performance improvement:&lt;/span&gt;&lt;/p&gt;
&lt;div align="left"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col/&gt;&lt;col/&gt;&lt;col/&gt;&lt;col/&gt;&lt;col/&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan="2" rowspan="2" style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt; &lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;AlloyDB High Memory&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Self managed PostgreSQL&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td rowspan="2" style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;AlloyDB Improvement&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;8 vCPU / 64GB&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;16 vCPU / 128GB&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="4" style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt; &lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="2" style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Transactions per minute (TPM)&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;280,614&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;118,928&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt; 2x transactions&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan="2" style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Cost per 1 million transactions&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;$0.1593&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;$0.3419&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: bottom; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt; 2x better price/perf&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2nd-image-Blog-2x-perf-chart.max-1000x1000.png"
        
          alt="2nd-image-Blog-2x-perf-chart"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Despite being configured with only 50% of self-managed compute resources, AlloyDB exhibited &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;better&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; TPM. With databases configured with 30% of the data cached, both performance and price-performance are &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;up to 2x &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;compared to self-managed standard PostgreSQL workloads. AlloyDB’s CPU utilization was close to 90%, suggesting the AlloyDB kernel is much better utilizing the available resources.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;By deploying AlloyDB using the same GCE SKU (16 vCPU/128GB), we observed a &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;4x increase&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; in performance. AlloyDB is engineered to scale with a larger number of vCPUs, resulting in better price-performance with a higher vCPU count. Currently, AlloyDB supports up to 128 vCPUs, which equates to 864GB of memory.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Standard PostgreSQL deployed on Compute Engine encounters resource limitations such as IOPS constraints and CPU scaling issues, hindering its ability to handle large amounts of data. Also note that this analysis does not include the manual resources required to self-manage the database, any additional inter-zone networking-related charges which can add to overall costs. &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span style="vertical-align: baseline;"&gt;Additional cost savings&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;AlloyDB compute instances in a cluster leverage the same regional colossus storage. What this means is that, when you deploy HA and add replica nodes (up to 20 at the time of publishing this blog), they all use the same storage. For example, if you have a 5TB database, and you have a HA + 4 read replicas, in a self-managed environment, the storage cost will be 6x the capacity (one primary, one standby, and 4 read nodes). With AlloyDB, you will be paying for 1 x 5TB costs.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3rd-image-storage.max-1000x1000.png"
        
          alt="3rd-image-storage"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Also, you don’t have to pre-provision AlloyDB’s storage capacity and allocate more capacity for IOPS. With AlloyDB’s cloud-scale architecture, the storage usage is dynamic which expands and also shrinks based on the usage. You pay only for the capacity you use.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;There are no additional costs for AlloyDB cross-zone data replication for HA and for replica. With &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/databases/alloydb-database-provides-reduced-replication-lag-for-postgresql"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;up to 25x lower read replica lag&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, you can scale horizontally using the same storage and with near-real time data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Besides, AlloyDB can be a single datastore for your transactional, &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/databases/alloydb-for-postgresql-columnar-engine"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;analytical&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/databases/helping-developers-build-gen-ai-apps-with-google-cloud-databases"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;vector database&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; workloads. You don’t have to provision multiple databases for these purposes which reduces additional compute, storage, networking, and ETL deployment and management aspects.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4th-image-single-data-store.max-1000x1000.png"
        
          alt="4th-image-single-data-store"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Managing PostgreSQL databases on your chosen infrastructure can offer cost advantages but comes with several challenges and hidden costs. These include capacity planning for peak workload, ongoing maintenance of hardware resources, and manual management of essential database tasks like deployment, scaling, security, high availability, backups, and disaster recovery. This can put a strain on your team and lead to higher administrative overhead and costs.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/alloydb/docs/overview"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;AlloyDB&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; provides a fully managed experience that addresses these requirements and helps you run your mission-critical applications with a 99.99% uptime SLA, including maintenance.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;AlloyDB comes out ahead&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;AlloyDB surpasses self-managed databases by offering up to 2x price-performance improvement and more cost savings with shared regional storage. Additionally, AlloyDB significantly mitigates the challenges associated with self-managed databases through automation of routine administrative tasks, on-demand vertical (read/write) and horizontal (read-only) scalability, minimal maintenance downtime, and inclusion of columnar engine &amp;amp; AI capabilities. With these features and benefits, AlloyDB provides a better, faster, and cheaper alternative to self-managed PostgreSQL environments.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Next steps&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Watch Andi Gutmans’s &lt;/span&gt;&lt;a href="https://youtu.be/jR2YY9_IzF4?feature=shared" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Spotlight session&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; from Google Cloud Next ‘24 showcasing various innovations with AlloyDB&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Learn about AlloyDB at &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/databases/alloydb-for-postgresql-intelligent-scalable-storage"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;AlloyDB for PostgreSQL intelligent scalable storage&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;For a comprehensive overview of AlloyDB, refer to the &lt;/span&gt;&lt;a href="https://cloud.google.com/alloydb/docs/overview"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;overview section&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; of the documentation.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style="vertical-align: baseline;"&gt;Start building on Google Cloud with $300 in free credits and 20+ always free products at &lt;/span&gt;&lt;a href="https://cloud.google.com/free"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/free&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><pubDate>Fri, 10 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/databases/alloydb-vs-self-managed-postgresql-a-price-performance-comparison/</guid><category>Databases</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>AlloyDB vs. self-managed PostgreSQL: a price-performance comparison</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/databases/alloydb-vs-self-managed-postgresql-a-price-performance-comparison/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Sridhar Ranganathan</name><title>Product Manager, Google Cloud Databases</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Sheshadri Ranganath</name><title>Engineering Director, Google Cloud Databases</title><department></department><company></company></author></item><item><title>How Airwallex leveraged Google Cloud to scale globally</title><link>https://cloud.google.com/blog/topics/financial-services/scaling-fintech-capabilities-globally-with-google-cloud/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;On a mission to disrupt and transform the fintech industry, &lt;/span&gt;&lt;a href="https://www.airwallex.com/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Airwallex&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is constantly innovating, embracing new technologies that can help businesses overcome international money transfer challenges so they can grow without borders. We aim to provide end-to-end solutions and currently offer services related to global payments, treasury and expense management, and embedded finance services. With our proprietary infrastructure, we can remove friction from global payments and financial operations, so that businesses of all sizes can unlock new opportunities for growth.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Our developers are at the heart of everything we do at Airwallex. We follow a “develop first, and deploy fast” mindset, which we believe has benefited our customers, and we are highly focused on finding a good balance between velocity and quality.To that end, our continuous Integration and continuous deployment (CI/CD) pipelines, which are run on Google Cloud and Gitlab, always need to be at optimum performance. Over the years, we’ve built a lot of integrations and have automated testing and validation, which enables us to deploy with confidence. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Expanding into new markets globally with Google Cloud&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;During late 2018 to early 2019, we decided that it was time to grow our presence globally and began looking for a platform that would support our ambitions. We were already on a hybrid cloud with different providers, which made sense in our earlier days as it provided more support in the APAC region. However, our goals were different as we looked to expand, and we found that Google Cloud was best suited to meet our needs because of its well-established global network. Google Cloud’s highly connected cloud infrastructure and advanced inter-region capabilities meant that we could easily build out into different geographies. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Together with the scalability of &lt;/span&gt;&lt;a href="https://cloud.google.com/kubernetes-engine"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Kubernetes Engine&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; (GKE) and agility of &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, our team has been able to take on projects that have become increasingly more complicated and challenging as we continue to extend our global network. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Relying on the stability of Google Cloud for CI/CD&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Airwallex relies on powerful and flexible APIs and CI/CD pipelines for seamless international money transfers, among other services. GitLab and GKE provide a scalable, always-available environment that enables us to respond to the rapidly changing security and market conditions. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;All our GitLab service components are run natively in GKE clusters. We use &lt;/span&gt;&lt;a href="https://cloud.google.com/sql"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud SQL&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, Google Cloud’s fully managed relational database as our GitLab database, and the cache service on &lt;/span&gt;&lt;a href="https://cloud.google.com/memorystore?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Memorystore&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;a href="https://cloud.google.com/storage/docs/creating-buckets"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Storage buckets&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; act as our GitLab object storage. Some of our GitLab runners are also hosted on &lt;/span&gt;&lt;a href="https://cloud.google.com/compute"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Compute Engine&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; virtual machines. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The diagram below illustrates how Google Cloud is integrated into our GitLab CI/CD pipelines:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_2cJOPrI.max-1000x1000.png"
        
          alt="1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_XdsgxCw.max-1000x1000.png"
        
          alt="2"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;A key benefit of Google Cloud is scalability. Running GitLab on GKE has provided us with immense benefits in terms of scalability and reliability. For example, the autoscaling of GKE and the Horizontal Port Autoscaling (HPA) feature of Kubernetes means we never have to worry about the load on GitLab. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Kubernetes nodes can be autoscaled based on the node pressure, and the replicas can also be autoscaled based on the performance of containers. Our GKE clusters are deployed regionally across three zones, which improves the availability of our GitLab instance. Using Cloud SQL, we can guarantee a 99.95% SLA for our GitLab database, and we can deliver up to 99.999% availability by backing up the GitLab data on Cloud Storage buckets — building a truly global DevOps culture.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In the past, everything was manual, and it used to take us eight hours to deliver a release to production. With an automated CI/CD pipeline, we can deliver a change in less than an hour, including manual approvals from our stakeholders.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Identifying fraud efficiently with Vertex AI &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;As a fintech company, data security is paramount. We need to ensure that we are working with verified institutions, which is why fraud detection is the first line of defense. In August 2023, we launched a new project using &lt;/span&gt;&lt;a href="https://cloud.google.com/vertex-ai"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Vertex AI&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, focused on anti-money laundering and fraud analysis using generative AI technology. We implemented Vertex AI on our proprietary fraud detection system to scan any website that a customer gives us to determine its validity. After we verify that the website is legitimate, it is put through our other machine learning models to measure the risk of that website.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Prior to this, we relied on &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to train our machine learning models to detect fraud on our system. With Vertex AI, we can detect fraud in real time, improving the speed and efficiency of our day-to-day operations and enabling us to react to potential fraud much quicker. However, we continue to use BigQuery to support other data-driven decision-making due to its minimal maintenance and high security. The serverless, highly scalable infrastructure of BigQuery makes it an incredibly economical tool. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Delivering mission critical infrastructures with a trusted partner&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;For Airwallex, we collaborating with the Google Cloud Professional Services Organization (PSO) has been a game changer. The team’s willingness to share their in-depth knowledge on system designs and SRE practices inspire our engineers to continuously do their best work. Having a dedicated team to support us also gives us the confidence to experiment and develop new products quickly, knowing that our infrastructure is secure.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 10 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/topics/financial-services/scaling-fintech-capabilities-globally-with-google-cloud/</guid><category>Infrastructure Modernization</category><category>Containers &amp; Kubernetes</category><category>Customers</category><category>Financial Services</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>How Airwallex leveraged Google Cloud to scale globally</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/topics/financial-services/scaling-fintech-capabilities-globally-with-google-cloud/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Andy Chow</name><title>Chief of Staff, Technology, Airwallex</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Cathy He</name><title>Engineering Manager, Airwallex</title><department></department><company></company></author></item><item><title>Breaking barriers: How BigQuery data insights boosts the data exploration journey</title><link>https://cloud.google.com/blog/products/data-analytics/how-bigquery-data-insights-simplifies-data-analysis/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Most data analysis starts with exploration — finding the right dataset, understanding the data’s structure, identifying key patterns, and identifying the most valuable insights you want to extract. This step can be cumbersome and time-consuming, especially if you are working with a new dataset or if you are new to the team. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To address this problem, we announced the preview of &lt;/span&gt;&lt;a href="https://cloud.google.com/gemini/docs/bigquery/overview#ai-assist"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;new data insights capability&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; in BigQuery at Next ‘24 that surfaces relevant, executable queries for tables that you can run with just one click. These features are available as part of &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/data-analytics/introducing-gemini-in-bigquery-at-next24"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Gemini in BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and leverage the metadata and profiling information of tables from &lt;/span&gt;&lt;a href="https://cloud.google.com/dataplex?e=48754805&amp;amp;hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Dataplex&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this blog post, we explore how Alex, a data analyst working for a large enterprise, can use the new BigQuery data insights features to accelerate his analytics workflows. Like many data professionals, he often encounters the &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;"cold-start" problem&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; when exploring new datasets. With little or no prior knowledge about the data they're working with, it can be difficult to identify patterns, much less uncover valuable insights. We also dive deeper into the concept of grounding generated queries and the roles of different personas involved in this journey.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Addressing the cold-start problem with data insights&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Data insights harnesses Google's Gemini models to generate insightful queries about hidden patterns within a table, utilizing the table's metadata. By analyzing data types, statistical summaries, and other metadata attributes, it helps data analysts like Alex overcome the cold-start problem and unlock a world of data exploration possibilities.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image1_74IUZUA.gif"
        
          alt="image1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p style="padding-left: 40px;"&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;“The Insights feature felt like it understood the table, it filtered out not so useful columns like Created_at time, Transaction Id, while highlighting important columns like Amount, Intent Type, Bank Name, App Version, Platform.”&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; - &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Product Manager, Financial Services Industry&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Grounding generated queries for data relevance and accuracy&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;One of the key features of BigQuery data insights is its ability to ground generated queries. This means that the queries are based on the actual data distribution and patterns within the dataset, ensuring their relevance and accuracy. The grounding process involves:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Analyzing profile scan data:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Data insights examines the &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery/docs/data-insights#profile-scans"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;published profile scan&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; data of the dataset, which includes information such as data types, statistical summaries, and other metadata attributes.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Generating queries based on data distribution: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Using the profile scan data, data insights creates queries that are tailored to the specific data distribution and patterns within the dataset.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Validating queries:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The generated queries are validated to ensure their relevance and accuracy.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Two key personas: admin and data consumer&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Two primary personas who can benefit from using BigQuery data insights:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Admins - &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;responsible for generating insights using the data insights feature. Admins typically include data steward, data governors, or other technical users who have the necessary permissions and access to the underlying data. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Data consumers - &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; can view and execute the generated queries without needing direct access to the underlying data. Data consumers may include business analysts, data scientists, or other non-technical users who rely on the insights generated by BigQuery data insights to make informed decisions.In our story, Alex is a data consumer.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Getting started with BigQuery data insights&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To use data Bigquery data insights, follow these steps:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Access data insights: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;With your data in BigQuery, navigate to the BigQuery Studio in the Google Cloud console. Here, you'll find an overview of your tablesand their associated metadata.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Generate queries&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;: Select a table and click on the "Generate insights" button. Data insights analyzes the metadata and generates a list of insightful queries tailored to your dataset.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Explore and refine queries:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Review the generated queries and refine them as needed. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Run queries:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Execute the queries against your table and analyze the results to uncover valuable insights.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Alex's path to greater data insights&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Initially, Alex struggled to get up to speed when working with a new dataset. However, after discovering BigQuery data insights, he was able to streamline his data exploration process. Here's what data insights brought to Alex's work:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Efficient data exploration:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; By automatically generating insightful queries based on metadata, data insights enabled Alex to explore new tables more efficiently and independently.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Time and resource savings:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; With data insights handling low-to-moderate complexity data analysis tasks, Alex was able to focus on more challenging projects and save valuable time and resources.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Collaboration and democratization:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Data insights made data analysis more accessible to non-technical users in Alex's organization, fostering collaboration and promoting a unified approach to data interpretation.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Real-time insights:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; By automatically deriving insights from continuously flowing business data, data insights helped Alex and his team respond to changing business conditions in real-time.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p style="padding-left: 40px;"&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;“It’s fantastic that the Insights generation feature in BigQuery not only provides new insights but also simplifies the process of running derived queries. The tool surprises me with fresh perspectives, going beyond what I initially considered. Its user-friendly nature makes it accessible to everyone, enabling efficient query execution.”&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; - &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;Data Analyst, renewable energy industry&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Unlock insights from your data, fast&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;BigQuery d&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;ata insights is a powerful tool to help you unlock valuable insights from your data. By leveraging the metadata of tables, it streamlines the data exploration process and enables data professionals to focus on more challenging tasks. The grounding of generated queries ensures the relevance and accuracy of the insights, while the two primary personas – admin and data consumer – facilitate collaboration and democratization of data analysis. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Check out the &lt;/span&gt;&lt;a href="https://cloud.google.com/dataplex/docs/data-insights"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;documentation&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to learn more about data insights and reimagine the way you explore and analyze data.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 10 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/data-analytics/how-bigquery-data-insights-simplifies-data-analysis/</guid><category>Data Analytics</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Breaking barriers: How BigQuery data insights boosts the data exploration journey</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/data-analytics/how-bigquery-data-insights-simplifies-data-analysis/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Sai Charan Tej Kommuri</name><title>Product Manager, Data Analytics</title><department></department><company></company></author></item><item><title>Tencent: Building analytics culture for better game development</title><link>https://cloud.google.com/blog/products/data-analytics/the-road-to-analytics-culture-with-funcom-and-tencent/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;a href="https://www.tencent.com/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Tencent&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is a leading internet and technology company headquartered in Shenzhen, China. Our mission is “value for users, tech for good.” We are also the company behind world-famous games like Level Infinite, &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;PubG Mobile, Honor of Kings, GTFO, and Assassin’s Creed Jade. In February 2020, we acquired &lt;/span&gt;&lt;a href="https://www.funcom.com/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Funcom&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, a gaming company that recently celebrated its 30th anniversary, with a critically acclaimed portfolio that includes games like The Longest Journey, Anarchy Online, and Metal Hellsinger.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The acquisition of Funcom brought together the best of tech and the gaming industry, and with that, we decided to build an analytics culture at Funcom, supported by Google Cloud. We’ll share the challenges we faced as well as the solutions that were implemented in the process.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To demonstrate, we’ll use the online multiplayer survivor game, Conan Exiles, as an example. Developed by Funcom and released in 2017, this game constantly updates and releases new content, and was transitioned into a live service game model in 2022. As such, we needed data to support our business decisions. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;An architecture for scalability and growth&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Funcom’s architecture was developed to support the internal development team with live operations and to monitor the game servers’ health. The entire architecture was made of on-premises virtual machines and open-source frameworks, which limited our use cases and scalability. The legacy technology stack was not built with a data-driven approach in mind from a live service game model standpoint. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Based on interest from both developers and executives at Funcom, we decided to collaborate with Google Cloud to develop a new architecture. With only a few months to go before the release of the first season of Conan Exiles, the Google Cloud team provided us with a fully operational data warehouse that could be used to build dashboards and provide insights to key stakeholders, including executives, marketing, and live operations. The diagram below illustrates the architecture that we used:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_7iIgU9e.max-1000x1000.png"
        
          alt="1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We built our new technology stack according to a few key criteria: ease of integration, diverse use case coverage, and optimizing the total cost of ownership (TCO).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Building this data platform was like putting together puzzle pieces. We replaced our legacy data infrastructure using key products like Cloud Storage and BigQuery, which acted as a data lake and query engine. As a result, we were able to build a robust data pipeline and a well-established data platform foundation in less than two months, enabling access to a host of new game data that was previously unseen, such as in-game player activity playtests. This includes marketing data, such as social listening and community responses, performance data about CPU, graphic or memory usage, and even crash monitoring data. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;A new foundation to connect gaming and marketing datasets&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With the new architecture set up, we decided to explore other ways of using data to optimize cost performance and get better control of the data stack to connect marketing and sales datasets. For example, game leads need to understand how data can support in-game development, while our marketing teams should have easy access to in-game data to support marketing efforts.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To help, we automated the entire pipeline with always up-to-date KPI reports to monitor our marketing performance within Google Cloud. Additionally, the data team can provide recommendations based on in-depth insight analytics by connecting the data across player behavior, community, and marketing.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Moving forward with a revenue-generating tech stack &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With Google Cloud, we’ve been able to redesign the raw data pipeline and data lake architecture without impacting our gaming data pipeline, day-to-day decision-making systems, or requiring additional engineering overhead. As a result, we are able to process twice as much game data on a daily basis, compared to our previous architecture. In addition, we have also decreased our overall monthly costs by 70% using BigQuery and Cloud Composer.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Moving forward, we’re looking to further expand this architecture with near real-time pipelines built with Pub/Sub. We are also planning to improve data quality monitoring and alerting, and standardizing our data structure, so that we can deploy newly developed features directly to beta and accelerate our time to market.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Fri, 10 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/data-analytics/the-road-to-analytics-culture-with-funcom-and-tencent/</guid><category>Infrastructure Modernization</category><category>Gaming</category><category>Data Analytics</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Tencent: Building analytics culture for better game development</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/data-analytics/the-road-to-analytics-culture-with-funcom-and-tencent/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Mathieu Ruiz</name><title>Lead Game Data Analyst, Funcom</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Hao Liang</name><title>Manager, Data Platform, Tencent</title><department></department><company></company></author></item><item><title>How Trendyol solves cloud governance at scale with Looker</title><link>https://cloud.google.com/blog/products/data-analytics/how-trendyol-solves-cloud-governance-at-scale-with-looker/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;Editor’s note:&lt;/strong&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt; Today, we’ll hear from Trendyol, a leading ecommerce marketplace and Google Cloud partner based in Türkiye, about how it uses Looker on top of its BigQuery data lake infrastructure to allow its thousands of users to gain real-time insights while maintaining strong governance in the cloud. &lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The beauty of cloud environments is that rather than a “one size fits all” solution, you can choose the services you need and start using them immediately. Still, ensuring enterprise-wide best practices and compliance, such as &lt;/span&gt;&lt;a href="https://cloud.google.com/security/compliance/iso-27001"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;ISO27001&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, can sometimes be a challenge, creating additional authorization requirements and friction between teams. This inherent conflict can lead to competing priorities between the groups consuming cloud resources and those responsible for looking out for the greater good of the organization. In fact, &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/topics/hybrid-cloud/a-cios-guide-to-the-cloud-hybrid-and-human-solutions-to-avoid-trade-offs#:~:text=69%25%20of%20organizations%20indicate%20that%20stringent%20security%20guidelines%20and%20code%20review%20processes%20can%20slow%20developers%20significantly."&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;69% of organizations&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; indicate that stringent security guidelines and code review processes can slow developers significantly.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With this in mind, let’s take a look at how leading ecommerce platform Trendyol uses &lt;/span&gt;&lt;a href="http://cloud.google.com/looker"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Looker&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to &lt;/span&gt;&lt;a href="https://cloud.google.com/customers/trendyol"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;visualize petabytes of data ingested from hundreds of sources&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and achieve cloud governance at scale.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Balancing innovation and risk mitigation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Within an organization, it is rarely a single development team utilizing cloud services but a complex combination of various internal, production, and non-production applications, each vying for priority and with its own set of unique demands. Additionally, it is important to consider that various governance entities, including IT and organizational governance, as well as audit teams, may also be involved — and you need to solve for all parties.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Let’s explore the two different sides:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Freedom:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Developers crave the freedom to experiment and innovate. They need to quickly prototype new ideas and bring them into production. If development cycles take longer, they are hesitant to make changes.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Governance: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;There needs to be governance to ensure systems are secure, reliable, and scalable. This includes enforcing coding standards, conducting security reviews, and monitoring system performance, especially for the resources shared across disparate teams.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Within cloud environments, these dynamics can prove more challenging. At Trendyol, for instance, thousands of users leverage Looker for everyday business intelligence tasks, including processing data and delivering reports that help optimize operations, personalize experiences, and predict market trends. While the majority are data analysts, there are many other types of users tapping into Looker across the company, including business users, warehouse operations, and cargo operations.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;As one would expect, all of these users want to use data in their own way to speed up their priorities and make a difference in how they work, such as setting up &lt;/span&gt;&lt;a href="https://cloud.google.com/looker/docs/delivering-looks-explores"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;recurring schedules&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to &lt;/span&gt;&lt;a href="https://cloud.google.com/looker/docs/best-practices/how-to-use-the-google-sheets-integration#enabling-the-google-sheets-action-in-looker"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;deliver data from Looker to Google Sheets&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. While these capabilities provide powerful ways to gain insights, it’s also important for Trendyol to ensure these actions are taken fairly and efficiently to avoid impacting experience or incurring unnecessary costs.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This is a common scenario for many organizations as they embrace cloud services. Cloud capabilities enable teams to outsource the heavy lifting and focus on more valuable business priorities. At the same time, they may not always be cautious or fully aware of risks, which can adversely impact the existing business — despite their good intentions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Therefore, it’s critical to strike the right balance between innovation and risk mitigation to create systems that enable productivity while also being safe and reliable.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Creating trend-setting ecommerce experiences with Looker&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Trendyol aims to let users explore and discover new insights while keeping its platform fast, secure and affordable. This is not just a technology challenge, but one that requires collaboration among teams and big-picture thinking.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With Looker, Trendyol can centralize key metrics from its existing tools and platforms in near real time, empowering its data analysts, business users, and other teams with streamlined data analytics and insights to power their daily workflows. Moreover, Looker provides easy ways to implement data governance, including access control features, allowlists, job scheduling, cache optimizations, and more.&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;To ensure cloud governance and maintain flexibility, Trendyol’s data warehousing team created a set of internal mechanisms and principles using Looker. This included the following:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Creating an official allowlist. &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Trendyol identified its top users and content to proactively create and maintain an allowlist of scheduled jobs. This allowlist is continuously monitored for new additions and regularly sanitized in collaboration with schedule owners to determine which jobs should remain. The cleanup process is automated, allowing Trendyol to eliminate duplicate schedules and enabling schedule owners to grant permissions to allow jobs to stay on the allowlist.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Establishing forums for user collaboration.&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Trendyol set up formalized programs for users to join and learn from one another, focusing on how top performers are utilizing scheduled jobs. This includes regular meetings, office hours, and other forums for discussions to encourage better collaboration. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Spreading out scheduled job execution times.&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Updating and processing data are some of the most compute-intensive tasks in Looker. Trendyol regularly evaluates whether scheduled jobs actually require execution at peak times and explores alternatives, such as eliminating duplicate jobs, adjusting scheduled times, or removing old and unnecessary jobs.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Reviewing job frequency&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;. Assess whether certain jobs need to be scheduled for frequent recurrence and consider reducing that rate to optimize resource utilization. For example, the team might assess whether scheduling a job once a week is sufficient to meet user needs rather than scheduling it multiple times a day. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Addressing zombie schedules.&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; If a schedule owner leaves the company, Trendyol determines whether another schedule owner can take over their scheduled jobs. If no other schedule owner can be found, these “zombie” schedules are removed from Looker.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Delivering a Hub and Spoke model and &lt;/span&gt;&lt;a href="https://cloud.google.com/looker/docs/caching-and-datagroups"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;cache optimizations&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. Trendyol adopted a &lt;/span&gt;&lt;a href="https://www.googlecloudcommunity.com/gc/Technical-Tips-Tricks/Advanced-LookML-Hub-and-Spoke-modeling/ta-p/592450" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;hub and spoke architecture model&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, which allows the company to apply universal business logic while allowing individual teams to define their own logic and access control rules and policies. Trendyol also makes use of Looker’s smart caching mechanisms to avoid fetching redundant data for similar queries.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;All together, Trendyol has found that combining these efforts and continuously working to improve them has made it possible to provide true cloud governance at scale. &lt;/span&gt;&lt;a href="https://cloud.google.com/customers/trendyol"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Trendyol can now handle more than 260,000 queries a day during peak periods and empowers more than 1,500 employees to use Looker insights for their daily work&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To learn more about Trendyol’s journey on Google Cloud, see the following:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://cloud.google.com/blog/products/data-analytics/e-commerce-data-warehouse-migration"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;E-commerce data warehouse migration | Google Cloud Blog&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://www.youtube.com/watch?v=rlQ61X9J8NM&amp;amp;t=2s" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery and BigLake: Real-world data products for AI/ML at scale&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://medium.com/trendyol-tech/tagged/google-cloud-platform" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud Platform – Trendyol Tech – Medium&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://www.youtube.com/watch?v=dffmazaRZ2Q" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;2021 Trendyol Data Management Day&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://www.youtube.com/watch?v=k7Rb2nbY2Qs" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Trendyol Data Management Day 2022 &lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We also recommend checking out the following Google Cloud guides for more data governance best practices: &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://cloud.google.com/architecture/identity/best-practices-for-planning"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Best practices for planning accounts and organizations &lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://cloud.google.com/resource-manager/docs/managing-notification-contacts"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Managing contacts for notifications &lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://cloud.google.com/security-command-center/docs/optimize-security-command-center"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Security Command Center best practices&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;a href="https://cloud.google.com/billing/docs/onboarding-checklist"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Guide to Cloud Billing Resource Organization &amp;amp; Access Management&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><pubDate>Fri, 10 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/data-analytics/how-trendyol-solves-cloud-governance-at-scale-with-looker/</guid><category>Business Intelligence</category><category>Customers</category><category>Data Analytics</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>How Trendyol solves cloud governance at scale with Looker</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/data-analytics/how-trendyol-solves-cloud-governance-at-scale-with-looker/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Saurabh Bangad</name><title>Technical Account Manager, Middle East</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Semra Uludağ</name><title>Data Warehouse and Business Intelligence Team Lead, Trendyol</title><department></department><company></company></author></item><item><title>How chaos testing adds extra reliability to Spanner’s fault-tolerant design</title><link>https://cloud.google.com/blog/products/databases/chaos-testing-spanner-improves-reiliability/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;One of the secrets behind &lt;/span&gt;&lt;a href="https://cloud.google.com/spanner"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Spanner’s&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; reliability is the team’s extensive use of &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Chaos_engineering" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;chaos testing&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, the process of deliberately injecting faults into production-like instances of the database. Although engineers focus on testing the “&lt;/span&gt;&lt;a href="https://dictionary.cambridge.org/us/dictionary/english/happy-path#google_vignette" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;happy path&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;,” most software bugs occur when things go wrong. Given Spanner’s complex architecture and constantly evolving codebase, it is inevitable that bugs will be introduced. Here, we give an overview of the types of chaos testing we employ and the kinds of bugs it finds.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;A fault-tolerant design foundation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Spanner is built from “mostly reliable” components including machines, disks, and networking hardware that have a low rate of failure. Even so, bad things happen: bad memory and disks may lead to &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Data_corruption" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;data corruption&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;; file accesses may yield transient or permanent errors or corruption; or network connectivity within or between data centers may be throttled or lost altogether. Worst of all, software bugs sometimes produce correlated failures in all servers running the same version of the code.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Since both correctness and availability are critical, Spanner uses principles of &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Fault_tolerance" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;fault-tolerant design&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to mask failures of these components and achieve high reliability for the service. For example, checksums are used to detect data corruption at many levels. Spanner tablets, which store a fragment of the database, are replicated across three or (usually) more data centers and the reads and writes use &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Paxos&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to achieve consensus and consistency of the distributed state. Checksums are also used to detect corruption of a tablet replica. The data for these tablets is stored in files, and the file system keeps multiple copies of the data blocks within the data center, using checksums to detect corrupted blocks. Finally, we proceed cautiously when rolling out new software versions, alerting on any anomalies that may be caused by a new bug.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;Upping reliability with chaos testing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We run over a thousand system tests per week to validate that Spanner’s design and implementation actually mask faults and provide a highly reliable service. Each test creates a production-like instance of Spanner comprising hundreds of processes running on the same computing platform and using the same dependent systems (e.g., file system, lock service) as production Spanner. Most tests run for between one and 24 hours and execute tens or hundreds of thousands of transactions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Actual faults in production occur at a very low rate. To cover Spanner’s error-handling and fault-tolerance mechanisms, we inject faults (e.g., file and network errors) at a much higher rate in these system tests. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;If these faults uncover bugs, the test fails in one of several ways:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;A read or query on the database does not return the expected result. Being able to compute the expected result of a randomly generated read/query on a database populated with randomly generated data is a challenging problem. Spanner’s &lt;/span&gt;&lt;a href="https://cloud.google.com/spanner/docs/true-time-external-consistency#strong-consistency"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;strong consistency model&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is the key to validate read/query results efficiently: each transaction records a log summarizing its effects, and subsequent transactions can replay these logs to compute the state they should observe. We describe this in further detail in an &lt;/span&gt;&lt;a href="https://medium.com/@jcorbett_26889/randomized-testing-of-cloud-spanner-5286f1eaba75" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;earlier article&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;A Spanner API call returns an unexpected error.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;A Spanner server crashes in an unexpected way. Some of the faults we inject will cause a server to crash, but we filter these and fail the test only if some new unexpected crash occurs.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;One of Spanner’s internal consistency checkers reports a problem. Checkers verify that:&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;ol&gt;
&lt;li aria-level="2" style="list-style-type: lower-alpha; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Files are not leaked (like Unix fsck, but on the distributed file system)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="2" style="list-style-type: lower-alpha; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Secondary indexes are consistent with the tables they index&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="2" style="list-style-type: lower-alpha; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Declared checks and foreign key constraints are satisfied&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="2" style="list-style-type: lower-alpha; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;All replicas of a tablet are equal&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Let’s take a look at the kinds of faults that we inject when chaos testing Spanner.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;1. Server crashes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;One of the most basic faults we inject is to force a server to crash abruptly (e.g., via a SIGABRT Unix signal). This simple fault causes lots of complex failure recovery logic to be executed:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Servers use a disk-based log to protect against the loss of their in-memory state, thus crashing exercises the logic that recovers the state of all the tablets that were on the crashed server from their logs.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;All &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;distributed transactions&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; being coordinated by the crashed server must abort and be restarted since the locks are kept in memory.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Clients that were pulling data from the crashed server via reads and/or queries are forced to fail over to another replica. The client must resume the operation without starting again at the beginning, and without losing or duplicating any results.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The restart logic is quite complex and we even trigger restarts without server crashes to exercise it at various points in the streaming of the results.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;2. File faults&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Spanner servers store their persistent data in the &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Colossus file system&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. In system tests, we intercept all calls to this file system and randomly inject various types of faults:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Error codes:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Force file system calls (e.g. Open, Close, Read, Write) to return an error code to the server. Some codes indicate transient errors that may be retried, while others represent permanent errors.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Corrupt content:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Corrupt the content read/written by a Read/Write call. Checksums should detect these.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Blackhole the request:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Sometimes the file system in a data center is disabled for maintenance. This will cause file system calls to hang (not return) until the file system is re-enabled. Sometimes the file system is made read-only, in which case only writes will hang. Spanner must detect this and fail over to use an alternate replica in a different data center.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;For example, through chaos testing, we found a bug involving the interaction of Spanner tablet compaction and the Colossus storage layer. Spanner tablets are stored using &lt;/span&gt;&lt;a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;log-structured merge trees&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, with an in-memory table plus a set of Colossus files. To bound the number of files, tablets are periodically compacted by merging several files into one. The test randomly injected a rare Colossus error code and discovered that the compaction code treated it as meaning end-of-file, resulting in random data loss. Tests like these ensure that subtle data corruption issues that arise from complex system interactions are very unlikely to reach production.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;3. RPC faults&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Google’s Remote Procedure Call (RPC) system allows outgoing RPCs to be intercepted and manipulated in various ways. This mechanism can be used to inject a wide variety of faults:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Delays can be inserted, triggering timeouts.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Errors, either transient or permanent, can be inserted, exercising the error handling code of a variety of services/APIs.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;RPCs to specific (simulated) data centers can be blocked, simulating a network partition.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;RPCs to specific dependent systems can be blocked or made to return errors, simulating what would happen if that system went down. For example, Spanner’s data files are encrypted, and the keys for these files are fetched via RPC from a separate key service that runs in each data center. An outage in that service will effectively bring down the replicas in the data center, though it should not prevent clients from failing over to other healthy replicas.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;RPCs having a specific network priority can be dropped to simulate the effect of network throttling. When cross data center links become saturated, lower priority packets are dropped. Some RPCs are more critical than others, and this test ensures that critical RPCs have the proper network priority.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;4. Memory/quota faults&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;When servers run low on memory, they enter a state called pushback, which should cause clients to redirect load to other (less busy) replicas. We force the servers into this state to test this behavior, and we ensure the server does not get stuck in this state. We also occasionally force a server to fail by leaking enough memory to cause the container to kill it, making sure this messy situation is handled cleanly.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Spanner enforces quotas on disk space, memory, and flash storage per user. When these limits are reached, operations fail with a special “quota exceeded” error. We inject these to ensure they are handled properly throughout the stack.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;5. Cloud faults&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Access to Spanner from the Google Cloud Platform is mediated by Spanner API Front End Servers, which proxy requests coming into Google Cloud through Google front ends to a Spanner database. External clients open sessions with the Spanner database and execute transactions on these sessions. For Spanner, we crash the Spanner API frontend servers, which forces sessions to migrate to other Spanner API frontend servers. This should not be visible to the client (besides some additional latency).&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;6. Regional outages&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The largest faults we simulate in system tests are outages of an entire region, forcing Spanner to serve data from a quorum of other regions. The majority of our system tests simulate several kinds of regional outages, triggered either by file system or network outages, and we verify Spanner continues to serve. This resilience is a property of the Paxos algorithm, which guarantees progress as long as a quorum (2 of 3, or 3 of 5) of replicas remain healthy.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;Spanner earns its reputation for reliability&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Spanner is fault tolerant by design. We continuously validate Spanner’s reliability by running many large-scale randomized system tests that employ chaos testing.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can &lt;/span&gt;&lt;a href="https://goo.gle/SpannerDatabaseUnlimited" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;learn more&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; about what makes Spanner unique and how it’s being used today. Or &lt;/span&gt;&lt;a href="https://cloud.google.com/spanner/docs/free-trial-quickstart"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;try it yourself for free&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; for 90-days or for as little as $65 USD/month for a production-ready instance that grows with your business without downtime or disruptive re-architecture.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 09 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/databases/chaos-testing-spanner-improves-reiliability/</guid><category>Spanner</category><category>Databases</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>How chaos testing adds extra reliability to Spanner’s fault-tolerant design</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/databases/chaos-testing-spanner-improves-reiliability/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>James Corbett</name><title>Software Engineer</title><department></department><company></company></author></item><item><title>The surprising economics of Horizontal Pod Autoscaling tuning</title><link>https://cloud.google.com/blog/products/containers-kubernetes/tuning-the-kubernetes-hpa-in-gke/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The &lt;/span&gt;&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Kubernetes Horizontal Pod Autoscaler&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; (HPA) is a fundamental tool for managing the scalability and efficiency of your environment, working by deploying more Pods in response to increased load. However, achieving the best price-performance with HPA requires a nuanced understanding of its settings, particularly your CPU utilization targets. The common assumption that a 50% CPU target is a good starting point can actually lead to higher costs. In fact, the 50% HPA CPU target might require significantly more provisioned resources compared to a 70% target, with a marginal impact on performance. And sometimes, changing settings such as resource requests on Pods can actually deliver a better balance between cost, performance and agility.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this blog, we explore why that is, so you can learn more about fundamental HPA optimization strategies for Google Kubernetes Engine (GKE).&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;The resource efficiency conundrum&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;On the surface, setting &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;HPA at a 50% CPU utilization target seems like a safe way to ensure efficiency. However, it can lead to an imbalance between cost and performance. To illustrate this, we performed some synthetic workload tests based on common traffic patterns. We compared autoscaling with two CPU targets: 50% and 70%, as well as a second 70% HPA target scenario with modified Pod resource requests.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Scenario 1: 50% CPU HPA target and 1 CPU/pod resource requests&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;With a 50% CPU target and our applied traffic pattern, HPA scaled to 22 CPUs. This setup resulted in an average end-user response time of 293ms and a consistent P95 response time of 395ms.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_-_50cpu_1cpu_pod.max-1000x1000.png"
        
          alt="1 - 50%cpu 1cpu_pod"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Scenario 2: 70% CPU HPA target and 1 CPU/pod resource requests&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Conversely, with a CPU target of 70%, HPA only scaled to 12 CPUs under the same user load. This represents a significant 84% reduction in resources compared to a target of 50% CPU target on HPA. While there was a slight impact on end-user performance (average response time increased from 293ms to 360ms and P95 latency increased from 398ms to 750ms), this illustrates the potential trade-off between resource efficiency and performance.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_-_70cpu_1cpu_pod.max-1000x1000.png"
        
          alt="2 - 70%cpu 1cpu_pod"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Scenario 3: 70% CPU HPA target and .5 CPU/Pod resource request&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Setting the HPA CPU target isn’t the only tool you can use to balance cost and performance. You can also tune your Pod resource requests.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In a third test, we kept the 70% CPU HPA target, but instead of requesting a full 1 CPU per Pod, we set the value at 500 millicores, while doubling the idle state of pods from 5 to 10 pods to achieve the same starting point for benchmarking. This pushes the HPA algorithm to scale more aggressively from a broader baseline of pods.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This scenario did provision 14 CPUs instead of 12; however, it also delivered meaningful improvements to the end user experience, averaging 325 ms response time, compared to 398ms in the previous scenario, and 598ms for P95 responses compared to 750 ms.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_-_70_cpu_0.5cpu_pod.max-1000x1000.png"
        
          alt="3 - 70% cpu 0.5cpu_pod"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;Comparing the results&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Let’s see how different scenarios are compared in our synthetic tests.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;Note: Results are provided for conceptual understanding of HPA and resource request implications for optimization and do not represent any kind of benchmarking or guarantees.&lt;/strong&gt;&lt;/p&gt;
&lt;div align="center"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col/&gt;&lt;col/&gt;&lt;col/&gt;&lt;col/&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Scenario&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;HPA 50% with 1 CPU/pod requested&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;HPA 70% with 1 CPU/pod requested&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;HPA 70% with 0.5 CPU/pod requested&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Minimal provisioned CPUs when idle&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;5&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;5&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;5&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Total provisioned CPUs at peak&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;22&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;12&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;14&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Average response rate of service tested&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;293 ms&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;398 ms&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;325 ms&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;P95 response rate of service tested&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;395 ms&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;750 ms&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;598 ms&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Understanding the tradeoffs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;When it comes to HPA optimization, there’s a delicate balance between efficiency and performance. The key lies in finding the optimal rate of CPU utilization vs. performance degradation on scale up (P95), which can be influenced by a combination of factors that are often beyond your control, including:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Frequency of metrics scraping:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; This can vary based on the metric type and vendor.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Traffic shape:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; How steep the traffic spikes are in your specific scenario.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Reactiveness of the workload:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The linearity and speed with which a metric change (such as CPU utilization) manifests under increased load.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Gauging these factors is paramount, as there isn't a 'one-size-fits-all' approach. And it goes without saying that you should always carry out performance testing on your workload before implementing any strategy in a production environment.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;As you can see from the scenarios above, a cost-effective approach is often to experiment with CPU targets like 70% or 80%. This offers several advantages, assuming your workload can tolerate the increased utilization:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Reduced costs:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Having fewer pods directly translates to lower aggregate resource requests, minimizing your overall bill.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Acceptable burst handling:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; A higher target provides breathing room within existing Pods to accommodate traffic bursts, reducing the frequency and intensity of scaling events.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Potential for more optimization:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Inversely, smaller Pods with higher utilization targets might perform better under load, which could also impact the HPA algorithm’s decisions, resulting in more effective scale-up.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Alternative HPA optimization strategies&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The ultimate solution for optimizing your auto scaling strategy is a balanced approach that maximizes efficiency and performance. By combining different tactics, you can achieve optimal equilibrium for your specific workload. In addition to the above techniques, you should also consider:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;C&lt;/strong&gt;&lt;strong style="vertical-align: baseline;"&gt;onfiguring scale-up and scale-down behaviors: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Leverage HPA cooldown to preserve instances longer or scale them down faster. This helps maintain stability during traffic spikes and reduces unnecessary resource consumption. Learn more here: &lt;/span&gt;&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#configurable-scaling-behavior&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Scaling on external or custom metrics:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; For more granular control over HPA’s scaling decisions, you can tap into external or custom metrics alongside CPU or memory utilization metrics. This allows you to tailor autoscaling to your unique application requirements. A great example is to scale based on traffic using metrics from your load balancer. Here are some tips: &lt;/span&gt;&lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/traffic-management#traffic-based_autoscaling"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/kubernetes-engine/docs/concepts/traffic-management#traffic-based_autoscaling&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Ultimately though, the key to successfully optimizing HPA behavior lies in understanding your workload’s profile and selecting the right node size through an iterative process that involves continuous testing, monitoring, and adjustments. This balanced approach is the best way to achieve efficiency, performance, and scalability for&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; your GKE clusters.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-related_article_tout"&gt;





&lt;div class="uni-related-article-tout h-c-page"&gt;
  &lt;section class="h-c-grid"&gt;
    &lt;a href="https://cloud.google.com/blog/products/containers-kubernetes/demand-based-scale-down-for-kubernetes-cost-optimization/"
       data-analytics='{
                       "event": "page interaction",
                       "category": "article lead",
                       "action": "related article - inline",
                       "label": "article: {slug}"
                     }'
       class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6
        h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker"&gt;
      &lt;div class="uni-related-article-tout__inner-wrapper"&gt;
        &lt;p class="uni-related-article-tout__eyebrow h-c-eyebrow"&gt;Related Article&lt;/p&gt;

        &lt;div class="uni-related-article-tout__content-wrapper"&gt;
          &lt;div class="uni-related-article-tout__image-wrapper"&gt;
            &lt;div class="uni-related-article-tout__image" style="background-image: url('')"&gt;&lt;/div&gt;
          &lt;/div&gt;
          &lt;div class="uni-related-article-tout__content"&gt;
            &lt;h4 class="uni-related-article-tout__header h-has-bottom-margin"&gt;Elite performance in demand-based downscaling: The power of workload autoscaling&lt;/h4&gt;
            &lt;p class="uni-related-article-tout__body"&gt;State of Kubernetes Cost Optimization report identifies the benefits of doing demand-based scale-down. This blog shows you how.&lt;/p&gt;
            &lt;div class="cta module-cta h-c-copy  uni-related-article-tout__cta muted"&gt;
              &lt;span class="nowrap"&gt;Read Article
                &lt;svg class="icon h-c-icon" role="presentation"&gt;
                  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#mi-arrow-forward"&gt;&lt;/use&gt;
                &lt;/svg&gt;
              &lt;/span&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  &lt;/section&gt;
&lt;/div&gt;

&lt;/div&gt;</description><pubDate>Thu, 09 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/containers-kubernetes/tuning-the-kubernetes-hpa-in-gke/</guid><category>GKE</category><category>Containers &amp; Kubernetes</category><media:content height="540" url="https://storage.googleapis.com/gweb-cloudblog-publish/images/finserve_2022.max-600x600.jpg" width="540"></media:content><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>The surprising economics of Horizontal Pod Autoscaling tuning</title><description></description><image>https://storage.googleapis.com/gweb-cloudblog-publish/images/finserve_2022.max-600x600.jpg</image><site_name>Google</site_name><url>https://cloud.google.com/blog/products/containers-kubernetes/tuning-the-kubernetes-hpa-in-gke/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Roman Arcea</name><title>GKE Product Manager</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Marcin Maciejewski</name><title>GKE Product Manager</title><department></department><company></company></author></item><item><title>LLMs, AI Studio, higher quality, oh my! Our latest Translation AI advancements</title><link>https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-translation-ai/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;When it comes to AI use cases, language translation is amongst the most practical and widely adopted by companies and organizations alike. From &lt;/span&gt;&lt;a href="https://www.canva.com/translate/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Canva&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to &lt;/span&gt;&lt;a href="https://youtu.be/1J7YexK6rwY" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Bloomberg&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, companies have pursued the flywheel of automatic multi-lingual translation, seeking to make content more accessible to employees, customers, communities, and the public. Since introducing the transformer architecture in 2017, which powers today’s large language models (or LLMs),  Google has continued to produce pioneering work, including many advancements in AI Translation. In this blog post, we’re pleased to announce a new generative model for Google Cloud’s Translation API and an overview of other recent advances that are helping businesses to accelerate translation use cases with AI.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Translation API introduces a specialized generative AI model, fine-tuned for translations &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The latest addition to Google Cloud’s &lt;/span&gt;&lt;a href="https://cloud.google.com/translate?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Translation AI portfolio&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;lets Translation API customers choose between our traditional machine translation model (a.k.a. NMT) or our new Translation LLM. Finetuned on millions of translation source and target segments, the Translation LLM is well-suited to longer context, so it should be considered for use in translating paragraphs and articles. NMT may still be optimal for chat conversations and short text, low-latency experiences, or use cases in which consistency and terminology management are critical. You can &lt;/span&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/generative/language/translation"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;try&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; out our Translation LLM on &lt;/span&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/generative/language/translation"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Vertex AI Studio in the translation mode&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;More flexible translation in real time with Generative AI&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Launched to &lt;/span&gt;&lt;a href="https://cloud.google.com/translate/docs/release-notes#February_14_2024"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;GA in February 2024&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; on Translation API Advanced, &lt;/span&gt;&lt;a href="https://cloud.google.com/translate/docs/advanced/adaptive-translation"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Adaptive Translation&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is an integrated API method that works in concert with our specialized Translation LLM. When customers request an adaptive translation, they import both the text to be translated as well as a small dataset of translated examples (as few as five or as many as 30,000). The API applies an algorithm to find the best examples for each translation request, then passes this refined context to the LLM at inference time. The result provides customers with a quick and easy method to optimize a translation output to &lt;/span&gt;&lt;a href="https://slator.com/google-partners-with-welocalize-evaluate-adaptive-translation-llm-solution/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;better fit style requirements and use cases in real-time&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;At Google Cloud Next ‘24 in April, AI-enabled translation platform &lt;/span&gt;&lt;a href="https://www.smartling.com/" rel="noopener" target="_blank"&gt;&lt;span style="font-style: italic; text-decoration: underline; vertical-align: baseline;"&gt;Smartling&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; co-presented on responsive translation using generative AI. Smartling shared benchmarks of Google Adaptive Translation spanning multiple verticals and nine languages. The findings included Google Adaptive Translation outperforming Google Translate with an up to 23% increase in quality. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;“Adding Google Adaptive Translation to our portfolio of engines is a pivotal point in the Smartling&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; translation and AI&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; strategy due to its easily customizable, dynamic and dramatic improvement in quality,” noted Olga Beregovaya, VP of AI at Smartling. “Unlike other general purpose LLMs, Google Cloud Adaptive Translation is particularly well suited for translation tasks.  It is an attractive solution due to best results in terms of performance-cost tradeoff and is especially well suited for clients with sparse data scenarios who are beginning their localization journey, entering new markets or are seeking to minimize content drift.”&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_kUaPvFe.max-1000x1000.png"
        
          alt="1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Translation in AI Studio &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Want to try out your content with a few different models in one go? We now make it fast and easy to test translations with not only our Specialized Translation LLM, but also Gemini, or Google Traditional translation models, by offering translation in AI Studio.  &lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2_qNNzPXS.gif"
        
          alt="2"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Quality gains for German, Japanese, Hindi, and Chinese on traditional translation models: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Throughout 2023, we quietly refreshed models for 30 language pairs, landing  quality gains along the way. Now as of April 1, 2024, customers using translation APIs will automatically benefit from our latest model refreshes, for German, Japanese, Hindi, and Chinese. The translation model updates for Google’s pretrained general translation models (a.k.a NMT) lands strong quality gains with significant MQM error reduction  across 4 languages, bi-directional. A lot of these gains are from enabling multi-sentence context retention (aka context window) to improve fluency and accuracy within a paragraph. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Which model should you choose? &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Should you pay for specialized models for translation to get high quality, hundreds of languages, and low latency? Or should you go with general-purpose large language models like Gemini to benefit from the long context window or low cost, even at the cost of throughput?  Generative models are excellent tools for summarization, question-answer use cases, generating content, and content editing, but today they are &lt;/span&gt;&lt;a href="https://inten.to/blog/generative-ai-for-translation-in-2024/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;orders of magnitude slower throughput than traditional translation models&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, so running large translation workflows on them can significantly slow time to delivery. &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;On the other hand, traditional translation models mostly translate sentence by sentence context, and generally are not flexible for real time customization of the output based on context. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Ultimately finding the right fit will vary based on your particular needs and use case, the good news is that you can find it all on &lt;/span&gt;&lt;a href="https://cloud.google.com/vertex-ai?e=48754805&amp;amp;hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Vertex AI in Google Cloud&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. Vertex AI platform provides model choice, global availability, and scale so that customers can choose the best model to fit with their use case, language, domain, and workflow.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 09 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-translation-ai/</guid><category>AI &amp; Machine Learning</category><media:content height="540" url="https://storage.googleapis.com/gweb-cloudblog-publish/images/aiml_.max-600x600.jpg" width="540"></media:content><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>LLMs, AI Studio, higher quality, oh my! Our latest Translation AI advancements</title><description></description><image>https://storage.googleapis.com/gweb-cloudblog-publish/images/aiml_.max-600x600.jpg</image><site_name>Google</site_name><url>https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-translation-ai/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Sarah Weldon</name><title>Lead Product Manager, Translation AI, Google Cloud</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Vinit Modi</name><title>Head of Product, Translation, Google</title><department></department><company></company></author></item><item><title>Building a Cloud Data Fusion pipeline to upload audit records generated by Cloud SQL for SQL Server to BigQuery</title><link>https://cloud.google.com/blog/products/databases/a-cloud-data-fusion-pipeline-to-upload-audit-records-to-bigquery/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/sql"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud SQL for SQL Server&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; offers configurable auditing, capturing events like login attempts, DDLs, and DMLs. These events are stored locally on the instance for up to 7 days and can be preserved longer in GCS buckets.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Some customers want to access and analyze their Cloud SQL for SQL Server audit data in other systems, like BigQuery or Tableau. While a custom-built pipeline can achieve this, it introduces complexity and maintenance burdens. Constant investment is needed for support, monitoring, and ensuring no audit records are lost.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this blog, we walk through the steps to build a flexible pipeline to output audit records to internal or external sinks with minimal coding.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Prerequisites&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Cloud Data Fusion doesn't directly support private IP Cloud SQL for SQL Server instances. To connect to these, you'll need to &lt;/span&gt;&lt;a href="https://cloud.google.com/sql/docs/sqlserver/sql-proxy"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;set up a Cloud SQL Auth Proxy&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; as a bridge between Cloud Data Fusion and your Cloud SQL for SQL Server instances. However, if your Cloud SQL for SQL Server instances have public IPs, you can connect directly without any extra configuration.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You would also need a &lt;/span&gt;&lt;a href="https://cloud.google.com/data-fusion/docs/how-to/reading-from-sqlserver"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;private IP Cloud Data Fusion instance&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. To ensure connectivity between your Cloud SQL Auth Proxy and the Cloud Data Fusion instance, establish a network peering between their respective networks. The peered project ID can be found on the Cloud Data Fusion instance page:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_ODpnwia.max-1000x1000.jpg"
        
          alt="1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h2&gt;&lt;strong style="vertical-align: baseline;"&gt;Building the workflow&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To begin, let's create the destination BigQuery table (we will store only a few fields, however, it’s possible to create a BigQuery table that stores all the available fields for a more comprehensive analysis):&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_9wY8u0y.max-1000x1000.png"
        
          alt="2"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Additionally, we'll need a table to track the last processed audit records for each instance. This table can reside in Cloud SQL, BigQuery, or even as a file in a Google Cloud Storage bucket. For simplicity, we'll create this table in BigQuery. This is necessary because if audit record fetching fails (e.g., due to the Cloud SQL instance being unavailable), we need a way to prevent missed records and ensure retries on the next attempt. We'll use the string type for last_scan_start_time and last_scan_end_time fields, as the BigQuery argument captor doesn't support the datetime type.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_6oGGTly.max-1000x1000.png"
        
          alt="3"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Now let's create a Data Fusion pipeline:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_bRtogUz.max-1000x1000.jpg"
        
          alt="4"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Let's review all the added blocks in the Data Fusion pipeline.&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;1. Initialize audit status&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This block queries BigQuery for the latest timestamp when audit records for this instance were uploaded. If no entry exists, it adds a default entry to retrieve all existing audit records.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;MERGE dataset.AuditStatus AS target&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;USING (SELECT '${instance_name}' as instance_name) AS source &lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;  ON target.instance_name = source.instance_name&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;WHEN NOT MATCHED THEN&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;  INSERT (last_scan_start_time, last_scan_end_time, instance_name)&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;  VALUES ('2000-01-01 01:01:01', '2000-01-01 01:01:01', '${instance_name}')&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;2. Set last scan time&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This block sets the last_scan_start_time variable, with the value chosen conditionally based on the instance name.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Cn26YqN.max-1000x1000.png"
        
          alt="5"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;3. Read audit records&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We'll pass in the parameters ${ip} and ${port}. How these are determined will be explained in the next section.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/6_abbP8Qh.max-1000x1000.png"
        
          alt="6"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We'll use this query to fetch new audit records from the instance, starting from the last processing time. Note that the query includes three additional parameters:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;${last_scan_start_time} is set by the previous step.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;${logicalStartTime} is automatically provided by Data Fusion.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;${instance_name} will be explained in the next section.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;SELECT event_time, server_principal_name, statement,' ${instance_name}' AS instance_name FROM msdb.dbo.gcloudsql_fn_get_audit_file('/var/opt/mssql/audit/*', NULL, NULL) &lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;WHERE CONVERT(datetime2, '${logicalStartTime(yyyy-MM-dd HH:mm:ss)}', 20) &amp;gt;= event_time AND CONVERT(datetime2, '${last_scan_start_time}', 20) &amp;lt; event_time&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;4. Convert audit records&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This optional block converts field names and types between the source and sink. This step can be skipped if the names and types already match.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/7_jH8eN6C.max-1000x1000.png"
        
          alt="7"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;5. Write audit records&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This block writes the audit records to the destination BigQuery table. Since the fields were converted in the previous step, no additional configuration besides the destination table is needed.&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;6. Update last scan time&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This final block updates the last scan time to prevent reprocessing audit records from the previous run. It also records the end time, primarily for tracking performance.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;MERGE dataset.AuditStatus AS target&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;USING (SELECT '${instance_name}' as instance_name) AS source &lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;  ON target.instance_name = source.instance_name&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;WHEN MATCHED THEN&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;UPDATE SET last_scan_start_time = '${logicalStartTime(yyyy-MM-dd HH:mm:ss)}', last_scan_end_time = CAST(CURRENT_DATETIME() AS STRING)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Scheduling the workflow&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Now you can start this workflow using gcloud, for example, from a cron job.:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;WORKFLOW_NAME="AuditPipeline"&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;CDAP_ENDPOINT=`gcloud beta data-fusion instances describe --location &amp;lt;&amp;lt;DATA_FUSION_INSTANCE_LOCATION&amp;gt;&amp;gt; --format="value(apiEndpoint)" &amp;lt;&amp;lt;DATA_FUSTION_INSTANCE_NAME&amp;gt;&amp;gt;`&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;PIPELINE_URL="${CDAP_ENDPOINT}/v3/namespaces/default/apps/${WORKFLOW_NAME}/workflows/DataPipelineWorkflow/start"&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;AUTH_TOKEN=$(gcloud auth print-access-token)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;curl -X POST \&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;     -H "Authorization: Bearer ${AUTH_TOKEN}" \&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;     -H "Content-Type: application/json" \&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;     "${PIPELINE_URL}" &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;However, we will demonstrate a different approach using &lt;/span&gt;&lt;a href="https://cloud.google.com/scheduler"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Scheduler&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, a fully managed, enterprise-grade cron job scheduler.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;First, we'll create a Cloud Function to do the following:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Enumerate Cloud SQL instances within a project&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Identify SQL Server instances with auditing enabled&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: decimal; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Trigger the previously created Data Fusion pipeline for each eligible instance, passing the instance name and IP address as parameters&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Please ensure the service account used to trigger the Cloud Function has permissions to access both Cloud SQL and Cloud Data Fusion instances.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;import functions_framework&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;import requests&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;import json&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;# This function retrieves the access token needed to list Cloud SQL for SQL Server instances and trigger the created pipeline.&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;def get_access_token():&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    METADATA_URL = 'http://metadata.google.internal/computeMetadata/v1/'&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    METADATA_HEADERS = {'Metadata-Flavor':'Google'}&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    SERVICE_ACCOUNT = 'default'&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    url = '{}instance/service-accounts/{}/token'.format(METADATA_URL, SERVICE_ACCOUNT)&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    r = requests.get(url, headers=METADATA_HEADERS)&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    access_token = r.json()['access_token']&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    return access_token&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;# This function retrieves the Cloud SQL Proxy Auth address and IP. For simplicity, it assumes there's only one Proxy Auth instance. The port is calculated as 1433 plus the last octet of the target Cloud SQL for SQL Server instance's IP address.&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;def get_proxy_address_and_port(ip):&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    PROXY_ADDRESS = '10.128.0.5'&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    PROXY_BASE_PORT = 1433    &lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    port = int(ip.split('.')[-1]) + PROXY_BASE_PORT&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    return (PROXY_ADDRESS, port)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;# This is the cloud function's entry point.&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;@functions_framework.http&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;def process_audit_http(request):&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    token = get_access_token()&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    WORKFLOW_NAME = "AuditPipeline"&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    # See the beginning of the section to learn how to determine this value.&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    CDAP_ENDPOINT = "&amp;lt;&amp;lt;YOUR_CDAP_ENDPOINT&amp;gt;&amp;gt;"&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    PIPELINE_URL = "{}/v3/namespaces/default/apps/{}/workflows/DataPipelineWorkflow/start".format(CDAP_ENDPOINT, WORKFLOW_NAME)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    # List private IP Cloud SQL for SQL Server instances with auditing enabled.&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    r = requests.get("https://sqladmin.googleapis.com/v1/projects/&amp;lt;&amp;lt;PROJECT_NAME&amp;gt;&amp;gt;/instances?filter=settings.sqlServerAuditConfig.bucket:\"gs://&amp;lt;&amp;lt;BUCKET_NAME&amp;gt;&amp;gt;\"%20settings.ipConfiguration.privateNetwork:projects/&amp;lt;&amp;lt;PROJECT_NAME&amp;gt;&amp;gt;/global/networks/default", headers={"Authorization":"Bearer {}".format(token)})    &lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    json_object = json.loads(r.text)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    # Find the Cloud SQL Auth Proxy IP and address, then start the Data Fusion pipeline..&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    for item in json_object["items"]:&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;      for ip in item["ipAddresses"]:&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;          if ip["type"] == "PRIVATE":&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;              proxy_and_port = get_proxy_address_and_port(ip["ipAddress"])&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;              param = {"ip" : proxy_and_port[0], "port": proxy_and_port[1], "instance_name" : item["connectionName"]}&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;              r = requests.post(PIPELINE_URL, json=param, headers={"Authorization":"Bearer {}".format(token)})&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;    return "done"&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Second, we create a Cloud Scheduler job that targets the previously created cloud function. Set the execution interval as needed — for example, the example below schedules it to run every 15 minutes.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/8_iKi6iAQ.max-1000x1000.png"
        
          alt="8"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The pipeline can be easily modified to process multiple instances simultaneously. This can be done within the Cloud Function or the pipeline itself, potentially improving processing speed.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Running the workflow&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Now let's &lt;/span&gt;&lt;a href="https://cloud.google.com/sql/docs/sqlserver/create-instance"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;create a Cloud SQL for SQL Server instance&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, &lt;/span&gt;&lt;a href="https://cloud.google.com/sql/docs/sqlserver/db-audit"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;enable auditing&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, and set up a simple audit rule:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;CREATE SERVER AUDIT ServerAudit1 TO FILE (FILEPATH ='/var/opt/mssql/audit', MAXSIZE=2MB)&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;CREATE SERVER AUDIT SPECIFICATION ServerAuditSpec1 FOR SERVER AUDIT ServerAudit1 ADD (DATABASE_CHANGE_GROUP) WITH (STATE = ON)&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;ALTER SERVER AUDIT ServerAudit1 WITH (STATE = ON)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Then create two databases, like these:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;CREATE DATABASE db1&lt;br/&gt;&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;CREATE DATABASE db2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Now, either wait for the next scheduled Cloud Scheduler run or trigger it manually. Once it runs, check the two BigQuery tables created earlier. The AuditRecords table should contain two 'create database' event:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/9_c68Imxf.max-1000x1000.png"
        
          alt="9"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The AuditStatus table should show the last execution time as follows:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/10_fDugcnt.max-1000x1000.png"
        
          alt="10"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Now, let's drop one of the databases we created earlier. For example:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;DROP DATABASE db1 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Now, either wait for the next scheduled Cloud Scheduler run or trigger it manually. Afterward, check the AuditRecords table should now contain one additional event:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/11_GO3kR4N.max-1000x1000.png"
        
          alt="11"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The AuditStatus table should also reflect the updated last execution time.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/12_7ZALfUq.max-1000x1000.png"
        
          alt="12"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;What’s next&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Cloud Data Fusion offers robust notification features. You can configure these to send emails when pipelines fail, among other options. For detailed instructions on setting this up, refer to the guide here: &lt;/span&gt;&lt;a href="https://cloud.google.com/data-fusion/docs/how-to/create-alerts"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/data-fusion/docs/how-to/create-alerts&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can also set up alerts in BigQuery to monitor specific conditions. See the following guide for instructions: &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery/docs/scheduling-queries"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;https://cloud.google.com/bigquery/docs/scheduling-queries&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 09 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/databases/a-cloud-data-fusion-pipeline-to-upload-audit-records-to-bigquery/</guid><category>Data Analytics</category><category>Developers &amp; Practitioners</category><category>Cloud SQL</category><category>Databases</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Building a Cloud Data Fusion pipeline to upload audit records generated by Cloud SQL for SQL Server to BigQuery</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/databases/a-cloud-data-fusion-pipeline-to-upload-audit-records-to-bigquery/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Alexander Kolomeets</name><title>Software Engineer</title><department></department><company></company></author></item><item><title>Paramount+: A streaming powerhouse with limitless entertainment</title><link>https://cloud.google.com/blog/products/media-entertainment/paramount-global-built-its-streaming-platform-on-google-cloud/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;a href="https://www.paramountplus.com/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Paramount+&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is a treasure trove of streaming entertainment for a global audience. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With a click, swipe, or voice command, viewers have instant access to iconic films like "The Godfather" and "Top Gun", television classics like "Star Trek" and "Survivor," and modern hits like "Yellowstone,” "1883," and "Halo." &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In addition to its immense library of filmed entertainment, Paramount+ also brings the excitement of live sports straight to consumers. Whether watching on connected televisions, web browsers, or mobile devices — and sometimes switching between them — viewers watched UEFA soccer, March Madness, the NFL, college football, and a variety of other sports this past year. This includes the 2024 Super Bowl LVIII, the most watched event in recent history with 123.4 million viewers across all platforms and the most-streamed Super Bowl in history, led by a record-setting audience on Paramount+.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In order to provide a global audience with streaming content available 24/7 — with huge demand spikes during those major live events — Paramount+ needed a robust technology stack that could provide speed, agility, security and global reach with zero downtime. To meet these diverse technology challenges, Paramount Global chose Google Cloud as the platform on which to build its streaming future. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Zero downtime was not only a technical goal but also the commitment to the business team to ensure subscribers can get consistent and seamless experience. Serving global subscribers requires robust architecture running on a scalable platform along with a well-trained team.  &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;The tech behind the curtain: Paramount+ and Google Cloud &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The technology stack for &lt;/span&gt;&lt;a href="https://www.paramountplus.com/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Paramount+&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; had many components specially tailored to the needs of media and entertainment. The team adopted a services-based architecture powered by &lt;/span&gt;&lt;a href="https://cloud.google.com/kubernetes-engine?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1707554&amp;amp;utm_content=text-ad-none-any-DEV_c-CRE_665665924789-ADGP_Hybrid+%7C+BKWS+-+MIX+%7C+Txt-Containers-Google+Kubernetes+Engine-KWID_43700077212829823-aud-2232802565252:kwd-335784956140&amp;amp;utm_term=KW_kubernetes+google-ST_kubernetes+google&amp;amp;gad_source=1&amp;amp;gclid=Cj0KCQjw_-GxBhC1ARIsADGgDjt3ZRdXF0QtPgPXYTKKifuBcCRszC-JR_V8rX5y02d1pIqIq15793YaAlU8EALw_wcB&amp;amp;gclsrc=aw.ds&amp;amp;e=48754805"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Kubernetes Engine&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; (GKE) for flexibility, stability, scalability and quick updates. This allowed the team to improve development and operational velocity and performance. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Subscriber growth and fast changing business needs led to exploring different Google Cloud services including &lt;/span&gt;&lt;a href="https://cloud.google.com/products/compute"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud Compute&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; Engine, &lt;/span&gt;&lt;a href="https://cloud.google.com/bigtable"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Bigtable&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, &lt;/span&gt;&lt;a href="https://cloud.google.com/pubsub/docs/overview"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Pub/Sub,&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;a href="https://cloud.google.com/products/operations"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Ops Suite&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, &lt;/span&gt;&lt;a href="https://cloud.google.com/network-intelligence-center"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Network Intelligent Center&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;a href="https://cloud.google.com/security/products/armor"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Armor&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to streamline technical operations. The architecture team, in collaboration with Google Cloud engineering, evaluated different products that could support the business SLA and security needs.  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Security and service availability is critical for any customer-facing applications. In order to prevent &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/identity-security/google-cloud-mitigated-largest-ddos-attack-peaking-above-398-million-rps?e=48754805"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;DDoS attacks&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; from disrupting the streaming experiences of tens of millions of users, Paramount+ uses the &lt;/span&gt;&lt;a href="https://cloud.google.com/security/products/armor?e=48754805"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud Armor Managed Protection&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; along with other industry standard security tools. And to ensure zero downtime across its global platform, the Paramount+ technology team applied the DevSecOps process to architecture to integrate security from the start of the development process. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To help ensure smooth operation and rapid updates, they adopted &lt;/span&gt;&lt;a href="https://cloud.google.com/sre"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Site Reliability Engineering&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; (SRE) practices in collaboration with Google Cloud. This approach hinges on automation, testing, proactive monitoring, and seamless teamwork. In addition to adopting &lt;/span&gt;&lt;a href="https://sre.google/sre-in-cloud/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;SRE&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; practices, the Paramount+ technology team utilizes a multi-zonal approach for resilience. This ensures true geo-redundancy, an active-active configuration that spans multiple Google Cloud regions. Through this strong partnership, Paramount+ is able to ensure exceptional performance, especially during high-traffic events like the Super Bowl.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Paramount+ engineers partnered closely with Google Cloud to establish guiding principles for this complex migration:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Multi-regional journey: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Paramount+ and Google Cloud teams collaborated for more than a year to ensure their infrastructure can scale into multiple regions. This journey happened without taking any downtime or downgrading end-user experience. The Paramount+ team is able to ensure that adding a new region should take only a matter of days, not years. Paramount+ had already adopted stateless principles to ensure optimal scale and usage of Google Cloud resources prior to becoming multi-regional. This strategic shift helped prepare Paramount+ to deliver a seamless experience while ensuring security and zero data loss.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Scalable architecture: &lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Paramount+ has adopted a distributed database running across multiple regions to ensure data consistency. Paramount+ and Google Cloud strive to maintain elasticity in the architecture to handle spiky traffic either during live events or for serving hit shows. This ensures the infrastructure can be both easily pre-scaled and autoscales. In addition to CI/CD principles, the Paramount+ team is also adopting a &lt;/span&gt;&lt;a href="https://cloud.google.com/architecture/application-deployment-and-testing-strategies#choosing_the_right_strategy"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;blue-green deployment approach&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to provide consistent experience to the end user and reduce risk.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Regional independence:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Bringing services closer to users while mitigating any natural disaster that may interrupt services was critical. This active-active multi-regional enabled Paramount+ to support a high number of daily active users and unprecedented amount of traffic during large sporting events. There is a strict policy to ensure that no region is dependent on any other region. This goes all the way from the content delivery network (CDN) to the databases. Paramount+ team has ensured that adding or removing scale in a region does not impact the overall end-user experience.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Operational consistency:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The Paramount+ SRE team set the standard guidelines and process to keep the regions homogenous for simplified management and addressing the business needs in timeline fashion. Consistent processes around security, audit, and deployment were put in place so that end users don’t have to know anything about the regions. &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Strict objectives:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The team had a goal to meet aggressive recovery-time-objective (RTO) and recovery-point-objective (RPO) targets. Having a strict service-level agreement and delivering on it was a critical aspect for supporting 71 million subscribers and having truly 24/7 streaming services. Strict SLAs ensured zero downtime, low latency, and robust monitoring and observability framework so the team could proactively address any issues that may impact end users.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Migrating to a multi-region setup meant rethinking deployment processes, automation tools, and the entire underlying database, all while upholding the established RTO and RPO. By working with Google Cloud, Paramount+ was able to transition from a multi-zonal architecture to an active-active multi-regional architecture and build on its world-class streaming service.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_OmOvZzD.max-1000x1000.jpg"
        
          alt="1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;The future is bright&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The media landscape is dynamic, and Paramount+ has adapted with a technology platform that has scaled to their global audience. Achieving broadcast quality across platforms and devices is non-trivial, and the teams work hard to achieve this in close collaboration with the Google Cloud team. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With this foundation, Paramount+ aims to continue optimizing and innovating with new technologies, like generative AI, all the while keeping viewers entertained without interruption and delivering a world-class customer experience. &lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Thu, 09 May 2024 13:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/media-entertainment/paramount-global-built-its-streaming-platform-on-google-cloud/</guid><category>Customers</category><category>Media &amp; Entertainment</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Paramount+: A streaming powerhouse with limitless entertainment</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/media-entertainment/paramount-global-built-its-streaming-platform-on-google-cloud/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Shiva Paranandi</name><title>SVP, Cloud Advancement &amp; SRE, Paramount</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Ashutosh Tripathi</name><title>Principal Architect, Google Cloud</title><department></department><company></company></author></item><item><title>Controlling metric ingestion with Google Cloud Managed Service for Prometheus</title><link>https://cloud.google.com/blog/products/management-tools/controlling-metric-flow-in-managed-service-for-prometheus/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;By default, Google &lt;/span&gt;&lt;a href="https://cloud.google.com/monitoring"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Monitoring&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; accepts and processes all well-formed metrics sent to a metric ingestion endpoint. However, under certain circumstances, metrics generation can be prolific, leading to a series of unnecessary expenses. This is especially true for verbose metrics of no particular utility. To control costs, platform users need a way to manage the flow of metrics prior to ingestion so that only relevant and useful metrics are processed and billed for.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Managed Service for Prometheus, which uses Cloud Monitoring under the hood, charges on a per-sample basis. Therefore, controlling the number of samples ingested is crucial to managing costs. There are two main ways to do this: filtering input or adjusting the length of the sampling period.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;As a simple example, extending the sampling interval can dramatically reduce the number of samples ingested and thus the cost. &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Changing a 10-second sampling period to a 30-second sampling period can reduce your sample volume by 66%, without any significant loss of information.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Changing a 10-second sampling period to a 60-second sampling period can reduce your sample volume by 83%.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;For additional information about how samples are counted and how the sampling period affects the number of samples, see&lt;/span&gt;&lt;a href="https://cloud.google.com/stackdriver/pricing#pricing_examples_samples"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Pricing examples based on samples ingested&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In a &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/management-tools/learn-to-understand-and-reduce-cloud-monitoring-costs"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;previous blog post&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; we discussed metrics management, how to identify high cost metrics, and how to reduce cardinality. In this blog post, we  look at some of the options we have to manage metrics ingestion and go over a few practical examples of using Prometheus to save on costs.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Using Kubernetes Custom Resources to control ingestion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The process of configuring Managed Service for Prometheus ingestion is very similar for rate reduction and time-series filtering, as both are implemented via configurations applied to either the&lt;/span&gt;&lt;a href="https://github.com/GoogleCloudPlatform/prometheus-engine/blob/v0.8.2/doc/api.md#podmonitoring" rel="noopener" target="_blank"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;PodMonitoring&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; or&lt;/span&gt;&lt;a href="https://github.com/GoogleCloudPlatform/prometheus-engine/blob/v0.8.2/doc/api.md#clusterpodmonitoring" rel="noopener" target="_blank"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;ClusterPodMonitoring&lt;/span&gt;&lt;/a&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" rel="noopener" target="_blank"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;custom resources&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. The ClusterPodMonitoring resource provides the same interface as the PodMonitoring resource but does not limit discovered Pods to a given namespace. In most scenarios, it’s better to use the PodMonitoring resource, as the ClusterPodMonitoring resource is normally used for items that are not naturally namespace-scoped, such as kube-state metrics, which reports information about the cluster from the k8s API. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Extend the period of the scrape interval&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The examples that follow modify the scrape interval of a PodMonitoring resource, thus reducing the frequency of sample collection. You can use the same basic techniques to modify ClusterPodMonitoring resources and ingestion filters. Examples of ingestion filter modification will be shown later in this article. Since increasing the scrape interval is the simplest way to throttle metric collection, we demonstrate that first.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The following manifest defines a PodMonitoring resource, &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;prom-example,&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; in the &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;EXAMPLE&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; namespace. The resource uses a&lt;/span&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" rel="noopener" target="_blank"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Kubernetes label selector&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to find all Pods in the Namespace that have the label &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;app.kubernetes.io/name&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; with the value &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;prom-example&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;. The matching Pods are scraped on a port named &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;metrics&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;, every 30 seconds, on the /metrics HTTP path. To extend the scrape interval from 30 seconds to 60 seconds, simply change the &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;30s&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; below to &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;60s&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;kubectl apply the following:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;apiVersion: monitoring.googleapis.com/v1\r\nkind: PodMonitoring\r\nmetadata:\r\n  name: prom-example\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app.kubernetes.io/name: prom-example\r\n  endpoints:\r\n  - port: metrics\r\n    interval: 60s #formerly 30s&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b51d2400&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;After making changes of this sort, it can be helpful to review the status of your modifications by enabling the target status feature, then checking the&lt;/span&gt;&lt;a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#target-status"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;status of the scrape targets&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; in the PodMonitoring or ClusterPodMonitoring resources; do this by setting the &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;features.targetStatus.enabled&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; value within the &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;OperatorConfig&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; resource to true. Note: turn off target status after using it as it is quite noisy and therefore expensive to operate continuously. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To enable target status, kubectl apply the following:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;apiVersion: monitoring.googleapis.com/v1\r\nkind: OperatorConfig\r\nmetadata:\r\n  namespace: gmp-public\r\n  name: config\r\nfeatures:\r\n  targetStatus:\r\n    enabled: true&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b14d2490&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;After a few seconds, the &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Status.Endpoint Statuses&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; field appears on every valid PodMonitoring or ClusterPodMonitoring resource, when configured.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;If there is a PodMonitoring resource with the name &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;prom-example&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; in the &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;NAMESPACE_NAME &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;namespace, then status can be verified by running the following command:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;$ kubectl -n NAMESPACE_NAME describe podmonitorings/prom-example&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b14d2ac0&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Filtering input&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;An alternative to the restriction of sampling rates is to filter inbound metrics before they are ingested. The standard method to prevent time-series metrics from being processed by the collector is to use&lt;/span&gt;&lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config" rel="noopener" target="_blank"&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Prometheus relabeling rules&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; with a &lt;/span&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;keep&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; action for an allowlist or a &lt;/span&gt;&lt;strong style="font-style: italic; vertical-align: baseline;"&gt;drop&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; action for a denylist. This rule goes in the &lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt;metricRelabeling&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; section of your PodMonitoring or ClusterPodMonitoring resource.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The following metric relabeling rule will filter out any time series that begins with &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;foo_bar_&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;, &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;foo_baz_&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;, or &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;foo_qux_&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;metricRelabeling: \r\n- action: drop\r\n  regex: foo_(bar|baz|qux)_.+\r\n  sourceLabels: [__name__]&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b14d20d0&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The following metric relabeling rule uses a regular expression to specify which metrics to keep based on the name of the metric. For example, metrics whose name begins with &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;kube_daemonset_&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; are kept.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;metricRelabeling:\r\n- action: keep\r\n  regex: kube_(daemonset|deployment|pod|namespace|node|statefulset|persistentvolume|horizontalpodautoscaler)_.+\r\n  sourceLabels: [__name__]&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b14d2700&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can also set a rule to manage specific time series based on a label value. For example, the following metric relabeling rule uses a regular expression to filter out all time series where the value for the label “direction” starts with “destination”:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;metricRelabeling:\r\n- action: drop\r\n  regex: destination.*\r\n  sourceLabels: [direction]&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b14d2820&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;As the examples above demonstrate, it’s simple to create allow/deny lists of metrics to control ingestion selection.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The decision as to whether it’s best to reduce metrics ingestion by content filtering or by scrape-rate reduction can be complicated. It’s worth pointing out that sample-rate reduction has very broad implications, while filtering offers more control and a more selective approach. Filtering also requires more thought and planning. A rule of thumb is to set scrape intervals to 30 seconds before applying content filtering, but to analyze the cost/benefit before moving a scrape interval to 60 seconds. Having a clear use case for either 30s or 60s data collection is very helpful in the decision-making process.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Links in this article:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align="left"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;&lt;table style="width: 97.7807%;"&gt;&lt;colgroup&gt;&lt;col style="width: 34.2448%;"/&gt;&lt;col style="width: 37.2396%;"/&gt;&lt;col style="width: 28.3854%;"/&gt;&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Link &lt;/span&gt;&lt;/p&gt;
&lt;/th&gt;
&lt;th scope="col" style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Description&lt;/span&gt;&lt;/p&gt;
&lt;/th&gt;
&lt;th scope="col" style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Location&lt;/span&gt;&lt;/p&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/blog/products/management-tools/learn-to-understand-and-reduce-cloud-monitoring-costs"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Monitoring Costs&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Blog Post&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Google Cloud Blog&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://github.com/GoogleCloudPlatform/prometheus-engine/blob/v0.8.2/doc/api.md#podmonitoring" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;PodMonitoring&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Resource documentation&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;GitHub&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://github.com/GoogleCloudPlatform/prometheus-engine/blob/v0.8.2/doc/api.md#clusterpodmonitoring" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;ClusterPodMonitoring&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Resource documentation&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;GitHub&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/stackdriver/pricing#pricing_examples_samples"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Ingestion Pricing&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Pricing examples&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Google Cloud&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Kubernetes Labels&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Label documentation&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Kubernetes.io&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#target-status"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Scrape Target Status&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;How to enable/disable&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Google Cloud&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Prometheus Relabeling&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Relabeling documentation&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Prometheus.io&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/monitoring/docs/metrics-management"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Metrics Management&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Interface documentation&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Google Cloud&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><pubDate>Wed, 08 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/management-tools/controlling-metric-flow-in-managed-service-for-prometheus/</guid><category>Developers &amp; Practitioners</category><category>Management Tools</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Controlling metric ingestion with Google Cloud Managed Service for Prometheus</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/management-tools/controlling-metric-flow-in-managed-service-for-prometheus/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Andrew Gold</name><title>Strategic Cloud Engineer, Cloud Platforms and Infrastructure</title><department></department><company></company></author></item><item><title>Cloud SQL for PostgreSQL data cache under the hood</title><link>https://cloud.google.com/blog/products/databases/a-closer-look-at-cloud-sql-for-postgresql-data-cache/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Data is the lifeblood of the organization. Being able to make quick, accurate and actionable decisions based on authoritative data enables enterprises to offer differentiated services and improve customer satisfaction. The rise of &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/databases/announcing-vector-support-in-postgresql-services-to-power-ai-enabled-applications"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;generative AI&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; has only further amplified this.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;It’s therefore important that your database provides near-real-time performance when interacting with operational data. For PostgreSQL databases, we offer Cloud SQL for PostgreSQL &lt;/span&gt;&lt;a href="https://cloud.google.com/sql/docs/editions-intro"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Enterprise Plus edition&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, which offers improved performance out of the box, improved data protection (35 days of PITR) and improved availability (99.99% SLA and near zero downtime maintenance.) &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Cloud SQL for PostgreSQL Enterprise Plus edition also includes an innovative data cache feature, which significantly improves read performance. The data cache is a read cache that uses a server-side SSD to cache data. Because it is co-located with compute in the server, data accesses have low latencies and high throughput. Workloads that are typically limited by read throughput and latency will therefore see significant benefits when using the data cache. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this blog post, we will explore how the data cache works, its internal mechanisms, and the types of workloads that will benefit the most from it.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;How does the data cache work?&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Cloud SQL uses high-performance, low-latency local solid-state drives as a caching layer for data storage disks. Think of the data cache as an extension to the shared buffers in PostgreSQL. This means queries avoid unnecessary network hops and are not limited by the underlying storage. The data cache is bigger than the physical memory in the instance, which means that more of the working set fits inside the server. This helps queries to achieve a much better cache-hit ratio. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The table below gives the sense of size of different storage layers for a 32 vCPU instance.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_0jIJ77R.max-1000x1000.jpg"
        
          alt="1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The data cache size is fixed for an instance and is a function of the number of vCPUs configured. The table below summarizes the data cache size for different vCPU configurations:&lt;/span&gt;&lt;/p&gt;
&lt;div align="center"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;
&lt;div style="color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;"&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col/&gt;&lt;col/&gt;&lt;col/&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt;Number of vCPU&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt;Memory (GB)&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt;Data Cache Size &lt;/strong&gt;&lt;/p&gt;
&lt;p style="text-align: center;"&gt;&lt;strong style="vertical-align: baseline;"&gt;(GB)&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;2&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;16&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;375&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;4&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;32&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;375&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;8&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;64&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;375&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;16&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;128&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;750&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;32&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;256&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;1500&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;48&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;384&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;3000&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;64&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;512&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;6000&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;80&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;640&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;6000&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;96&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;768&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;6000&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;128&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;864&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style="vertical-align: top; border: 1px solid #000000; padding: 16px;"&gt;
&lt;p style="text-align: center;"&gt;&lt;span style="vertical-align: baseline;"&gt;9000&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;What workloads benefit from the data cache?&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Workloads that benefit from the data cache feature are read workloads where the total dataset does not fit entirely into memory.  This includes but is not limited to:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Workloads that are sensitive to read latencies (for example, key-value lookups)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Workloads that are sensitive to read throughput (for example, table scans)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Gen AI workloads that use vectors for similarity searches&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Enabling the data cache&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In the Cloud SQL console, go to&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt; SQL → Create Instance → Choose PostgreSQL&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; and then choose&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Enable data cache&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;The data cache can also be enabled via gcloud and Terraform. &lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_brtZh8u.max-1000x1000.png"
        
          alt="2"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Monitoring&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Cloud SQL for PostgreSQL also includes  four new metrics for data cache observability:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Data cache quota&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;: &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Maximum data cache size&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Data cache used:&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Data cache used &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;PostgreSQL data cache hit count: &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Total number of data cache hits&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;PostgreSQL data cache miss count:&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="font-style: italic; vertical-align: baseline;"&gt;Total number of data cache misses&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Turbocharge your read-heavy workloads&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this blog we discussed how the data cache improves read performance. We also discussed how to easily monitor data cache operations. To learn more about Cloud SQL for PostgreSQL data cache, or to get started, check out the &lt;/span&gt;&lt;a href="https://cloud.google.com/sql/docs/postgres/data-cache"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;documentation&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Wed, 08 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/databases/a-closer-look-at-cloud-sql-for-postgresql-data-cache/</guid><category>Developers &amp; Practitioners</category><category>Databases</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Cloud SQL for PostgreSQL data cache under the hood</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/databases/a-closer-look-at-cloud-sql-for-postgresql-data-cache/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Virender Singla</name><title>Database Engineer</title><department></department><company></company></author></item><item><title>Product analytics for generative AI model and media asset companies using BigQuery</title><link>https://cloud.google.com/blog/products/data-analytics/perform-product-analysis-with-generative-ai-and-bigquery/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Over the last year, there’s been a lot of change in the commercial image and video asset industry: New generative AI applications let users create their own still and live images based on prompts, and traditional stock-media asset providers are offering customers richer search experiences that have a deep understanding of the image/live image content and that expose it with a natural language interface. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To continually push the state of the art, these organizations must use data to evolve their products rapidly, for example to: &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Optimize still and live image generation models&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Identify inappropriate content, such as violence or nudity&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Analyze behavior to identify improvements to the user experience&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Recommend similar images or prompts based on previous activity&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Enhance static asset search capabilities&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To do this, they need unstructured images, live images, and audio data, combined with structured user-experience data and metadata about the assets they are interacting with, whether they’re static or AI-generated.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In this post, we outline a solution&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;based on our real-life engagements with leaders in the industry who operate at the scale of petabytes per day. This solution delivers several benefits:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Minimizes costs by avoiding duplicate data and storage, while facilitating AI model proximity to data for efficient inference&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Simplifies development and delivery by combining diverse data types in a unified data architecture&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Optimizes use of limited engineering resources through an integrated, scalable serverless platform that combines &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;a href="https://cloud.google.com/storage?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Cloud Storage&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Allows users to augment and transform their data according to the needs of their business &lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Enables companies to develop lightweight, powerful analyses quickly and securely, to activate customer data and quickly iterate on the output of models&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;The challenge of unstructured data&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Generated (unstructured) image data, the (semi-structured) prompts that made them, as well as user behavior data (structured, in tables) for things like session time and frequency, are all rich in potential insights. For example, knowing which types of prompts lead to successfully generating an image — and those that don’t — provides insights into product and model development opportunities. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;But combining these different data types often requires advanced analytics to interpret them meaningfully. Technologies like natural language processing and computer vision are at the forefront of extracting these kinds of valuable insights. However, integrating unstructured data within an existing analytics framework of structured data, for example user behavior data in database tables, is not without its hurdles. Common challenges include:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Data security standards:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Adhering to stringent data security standards to protect sensitive information is crucial. These standards include applying data masking to sensitive PII data and following least-privilege security principles for data access.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Data type silos:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; Unstructured data is often stored separately from structured data, preventing effective analysis across data types, for example, filtering media assets (unstructured) based on user profiles (structured), as they reside in separate systems.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;High-performance, scalable cloud computing resources:&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; The need for powerful computing resources is imperative to manage and analyze large unstructured datasets effectively due to the data's complexity, volume, and the potential need for real-time results. In addition, high performance networking allows for low-latency data transfers to enable the transfer of unstructured data between storage (Cloud Storage) and analytical layers (BigQuery, Vertex, etc.)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;Maintaining data integrity across layers&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;:  As insights are extracted from unstructured data, preserving the original source of truth and ensuring consistency across intermediate (interstitial) layers is crucial for reliable, iterative analysis.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Streamlining data integration with Cloud Storage and BigQuery&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To overcome the challenges of working with unstructured data, &lt;/span&gt;&lt;a href="https://cloud.google.com/storage?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Storage&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; can be used to centralize data, using &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery/docs/object-table-introduction"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery object tables&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; to &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;enable consistent data access to varied sources through one analytical platform&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;. Below is an example of a simple yet effective architecture that harnesses BigQuery for both metadata generation and enhancement. This approach uses BigQuery's built-in generative AI functions, coupled with remote User Defined Functions (UDFs) that interface with &lt;/span&gt;&lt;a href="https://cloud.google.com/vertex-ai?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Vertex AI&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; APIs. The integration elevates the process of data enrichment and analysis, and offers a more streamlined and efficient workflow.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;The power of BigQuery object tables&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;In the example below, we focus on a static image use case, however, this same technique could be used for images created using generative AI. The true potential of this architecture lies in its versatility. The use of object tables in BigQuery means this pattern can be adapted to any form of unstructured data, for example images, audio, documents, opening up a world of possibilities for data science and analysis. This flexibility ensures the architecture can evolve with the changing needs and types of data, helping the solution withstand the test of time in the dynamic field of image curation and generation.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This architecture shows the integration of structured and unstructured data, utilizing the strengths of both to enhance platform capabilities. BigQuery serves as a central hub, amalgamating user data information (for example: user demographics, images viewed and used, session duration, session frequency), image metadata, and queries. Concurrently, external AI APIs augment this dataset with insights about the content of the images, for example describing what is happening in a scene (e.g. “a photographic image of a dog playing with a ball on grass”) . &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This convergence of data facilitates the training of sophisticated image-generation models, tailored to meet the specific requirements of the platform's users. It also unlocks advanced search and image-curation functionalities, enabling users to navigate through an extensive collection of images. The project's ability to provide access to external systems and empower data augmentation within BigQuery helps to centralize analytic workloads. This not only streamlines data analysis but also fosters informed decision-making.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Solution overview&lt;/strong&gt;&lt;/h3&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_-_Model_Analytics.max-1000x1000.png"
        
          alt="1 - Model Analytics"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The goal of the solution is to create a way to interact with unstructured data through BigQuery. Using BigQuery object tables to analyze unstructured data in Cloud Storage, you can perform analyses using generative AI models via remote functions, cloud APIs via Vertex AI, or perform inference by using BigQuery ML, and then join the results of these operations with the rest of your structured data in BigQuery.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 1. Creating an example dataset&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Prerequisites&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;Data: Multiple image repositories on third-party sites like Kaggle and Hugging Face&lt;br/&gt;&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;Project setup: To get started we need to activating essential project APIs:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;gcloud services enable cloudfunctions.googleapis.com&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;gcloud services enable cloudbuild.googleapis.com&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;gcloud services enable bigqueryconnection.googleapis.com&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;gcloud services enable &lt;/span&gt;&lt;a href="http://vision.googleapis.com" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;vision.googleapis.com&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 2. Create the object table&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The object table provides the reference to the non-structured data (e.g., audio, live images and images).  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;To do this, we create the BigQuery BigLake remote connection, building a bridge between BigQuery and Cloud Storage:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Command for creation: &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;bq mk --connection --location=us-central1 \&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;--project_id=bq-object-tables-demo \&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;--connection_type=CLOUD_RESOURCE biglake-connection&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;To show the details of this new creation, use: &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;bq show --connection bq-object-tables-demo.us-central1.biglake-connection&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Then, give your BQ service account the correct permissions to access your Cloud Storage bucket.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Your &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;serviceAccountId&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt; typically looks like this: &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;{"serviceAccountId": "bqcx-012345678910-abcd@gcp-sa-bigquery-condel.iam.gserviceaccount.com"&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;}`. And it needs the object viewer permission. This can be achieved by:&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;gsutil iam ch \\ serviceAccount:bqcx-012345678910-abcd@gcp-sa-bigquery-condel.iam.gserviceaccount.com:objectViewer gs://bq-object-tables-demo-data&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866640&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Make your object table in BigQuery in an existing dataset, or create a dataset for your object table.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Create the dataset with: &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;bq mk -d --data_location=us-central1 bq_object_table_demo-dataset&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This is a sample query you can use to create the object table&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;CREATE OR REPLACE EXTERNAL TABLE `bq-object-tables.bq_ot_dataset.bq_object_tables_external_table` \r\nWITH CONNECTION `bq-object-tables.us-east1.biglake-connection` OPTIONS (  object_metadata=&amp;quot;DIRECTORY&amp;quot;,  uris = [\&amp;#x27;gs://bq-object-tables-demo-data/*\&amp;#x27; ],  max_staleness=INTERVAL 30 MINUTE,  metadata_cache_mode=&amp;quot;AUTOMATIC&amp;quot;);&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866040&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The  max_staleness option lets you manage the trade-off between data freshness and performance by specifying a tolerable level of staleness for the materialized view; this can help improve query response times and reduce costs. By setting an appropriate value, you can achieve consistently high performance while keeping costs under control, even when working with large, frequently changing datasets&lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Create metadata using Native BQ Functionality&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;These steps can all be automated into a Directed Acyclic Graph (DAG) for use in an orchestration tool such as &lt;/span&gt;&lt;a href="https://cloud.google.com/composer"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud Composer&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 3. Reference the model from a native generative AI BQML function&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;First create the link back to the model in your BQ dataset like this: &lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;quot;# Create Model\r\nCREATE OR REPLACE MODEL\r\n`bq-object-tables.bq_ot_dataset.myvisionmodel`\r\nREMOTE WITH CONNECTION `bq-object-tables.us-east1.biglake-connection`\r\nOPTIONS (remote_service_type =&amp;#x27;cloud_ai_vision_v1&amp;#x27;);&amp;quot;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866f70&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;strong style="vertical-align: baseline;"&gt;Annotate image&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This code parses the images, extracts their contents and outputs a JSON array of words that describe the image and the model’s confidence that the description is correct. This function will then put the description into a table.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;quot;# Annotate image\r\nSELECT *\r\nFROM ML.ANNOTATE_IMAGE(\r\n  MODEL `mydataset.myvisionmodel`,\r\n  TABLE `mydataset.mytable`,\r\n  STRUCT([&amp;#x27;label_detection&amp;#x27;] AS vision_features)\r\n);&amp;quot;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866f40&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Step 4. Create a UDF in BigQuery&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can create a Cloud function using &lt;/span&gt;&lt;a href="https://gist.github.com/hselbie/fde69b900c4c719656ab42cdfb897b88" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;this basic code&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;If you’re unsure how to create a cloud function, please see the docs for how to &lt;/span&gt;&lt;a href="https://cloud.google.com/blog/products/data-analytics/extending-bigquery-functions"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;create a cloud function UDF&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Then, to deploy the Cloud Function, follow these steps:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;4.1. &lt;/span&gt;&lt;a href="https://cloud.google.com/functions/docs/deploying"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Deploy your Cloud Function&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;You may need to enable Cloud Functions API.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;You may need to enable Cloud Build APIs.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;4.2. &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/remote-functions#grant_permission_on_function"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Grant the BigQuery connection service account access to the Cloud Function&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;One way you can find the service account is by using the BigQuery cli ‘show’ command&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;4.3. Reference the functions in BigQuery&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Create a BigQuery remote function to reference the Cloud Function UDF&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;quot;CREATE OR REPLACE FUNCTION `mydataset.vision_safe_search`(signed_url_ STRING) RETURNS JSON\r\nREMOTE WITH CONNECTION `us.gcs-connection`\r\nOPTIONS(endpoint=&amp;#x27;https://region-myproject.cloudfunctions.net/vision_safe_search&amp;#x27;,\r\nmax_batching_rows = 1);&amp;quot;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866820&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;quot;CREATE OR REPLACE FUNCTION `mydataset.vision_annotation`(signed_url_ STRING) RETURNS JSON\r\nREMOTE WITH CONNECTION `us.gcs-connection`\r\nOPTIONS(endpoint=&amp;#x27;https://region-myproject.cloudfunctions.net/vision_annotation&amp;#x27;,\r\nmax_batching_rows = 1);&amp;quot;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866220&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;span style="vertical-align: baseline;"&gt;Step 5. Use the function in a query &lt;/span&gt;&lt;/h3&gt;&lt;/div&gt;
&lt;div class="block-code"&gt;&lt;dl&gt;
    &lt;dt&gt;code_block&lt;/dt&gt;
    &lt;dd&gt;&amp;lt;ListValue: [StructValue([(&amp;#x27;code&amp;#x27;, &amp;#x27;CREATE TABLE `mydataset.mid_processing` AS\r\nSELECT uri,mydataset.vision_safe_search(signed_url) as safe_search, mydataset.vision_annotation(signed_url) as annotation\r\nFROM EXTERNAL_OBJECT_TRANSFORM(\r\nTABLE `mydataset.imageall`,\r\n[&amp;quot;SIGNED_URL&amp;quot;]);&amp;#x27;), (&amp;#x27;language&amp;#x27;, &amp;#x27;&amp;#x27;), (&amp;#x27;caption&amp;#x27;, &amp;lt;wagtail.rich_text.RichText object at 0x3ed6b2866580&amp;gt;)])]&amp;gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Tap into unstructured data with BigQuery object tables and AI&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This architecture demonstrates the power of streamlining data integration for centralized analyses through BigQuery. Although we reference image data for this example, this methodology is highly flexible; using object tables we can reference any type of unstructured data in Cloud Storage buckets that could also refer to audio files that might reference a call center AI use case, for example, or live image files relevant to training a computer vision model. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;By centralizing data in Cloud Storage and BigQuery and intelligently using object tables, you can efficiently manage both structured and unstructured data. For our image-based example, this unified approach provides a rich dataset that contains user IDs, original prompts, prompt categories, image safety ratings, and even additional ML-generated prompts. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The potential applications for these metadata sets are huge. Product teams could use them to build more robust image-generation models or create an advanced image-search system, providing highly relevant results aligned with users' search terms and image descriptions. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Take the next step&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can get started today using this framework. For additional help, ask your Google Cloud account manager to reach out to the&lt;/span&gt;&lt;strong style="vertical-align: baseline;"&gt; &lt;/strong&gt;&lt;a href="https://cloud.google.com/solutions/data-cloud-isvs?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Built with BigQuery&lt;/span&gt;&lt;strong style="text-decoration: underline; vertical-align: baseline;"&gt; &lt;/strong&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;team&lt;/span&gt;&lt;/a&gt;&lt;strong style="vertical-align: baseline;"&gt;.&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The Built with BigQuery team helps Independent Software Vendors (ISVs) and data providers build innovative applications with &lt;/span&gt;&lt;a href="https://cloud.google.com/data-cloud?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Data Cloud&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. Participating companies can: &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Accelerate product design and architecture through access to designated experts who can provide insight into key use cases, architectural patterns, and best practices&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li aria-level="1" style="list-style-type: disc; vertical-align: baseline;"&gt;
&lt;p role="presentation"&gt;&lt;span style="vertical-align: baseline;"&gt;Amplify success with joint marketing programs to drive awareness, generate demand, and increase adoption&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><pubDate>Tue, 07 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/data-analytics/perform-product-analysis-with-generative-ai-and-bigquery/</guid><category>Partners</category><category>AI &amp; Machine Learning</category><category>Developers &amp; Practitioners</category><category>Data Analytics</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>Product analytics for generative AI model and media asset companies using BigQuery</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/data-analytics/perform-product-analysis-with-generative-ai-and-bigquery/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Annie Xu</name><title>Sr. Customer Engineer, Analytics, Google Cloud</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Hugo Selbie</name><title>Customer &amp; Partner Solutions Engineer, Gen AI, Google Cloud</title><department></department><company></company></author></item><item><title>What’s new with Active Assist: New Hub UI and four new recommendations</title><link>https://cloud.google.com/blog/products/management-tools/active-assist-gets-new-hub-and-recommendations/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The &lt;/span&gt;&lt;a href="https://cloud.google.com/solutions/active-assist"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Active Assist&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; portfolio of intelligent tools can help you reduce costs, increase performance, improve security, and even help you make more sustainable decisions. Today, we’re excited to announce some new Active Assist features that address some of our customers’ largest concerns. These features unlock some key functionality that help you better understand and use recommendations, all aimed to help make managing and optimizing your cloud simpler and easier. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Revamped Recommendation Hub&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/home/recommendations"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Recommendation Hub&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is a centralized page on Google Cloud that helps you view all of your recommendations in one place across multiple categories: cost, security, performance, reliability, manageability, and even sustainability. We recently made improvements to help you better understand the recommendations you have and to help you focus on the ones that are the most impactful: &lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;1. Organization-view of recommendations&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;One of our most in-demand features: you can now view all recommendations across all of your projects in your organization in one UI! Simply change the picker at the top left of the screen to choose an organization, and Active Assist shows all the recommendations under your organization (as long as you have the correct IAM permissions).&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/hubble-1.gif"
        
          alt="hubble-1"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;2. Pre-filtered recommendations by value category&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;You can now view all of your recommendations under one category in a simple  &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;table view, so you can prioritize and focus on the recommendations that are the most &lt;/span&gt;&lt;span style="vertical-align: baseline;"&gt;relevant and important to you.&lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;3. Custom sorting and filtering&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;With our new table views, you can sort and filter by different fields, such as product category, recommendation, cost savings, priority, etc. so you can find and view recommendations more easily. &lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/hubble-2.gif"
        
          alt="hubble-2"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Four new recommendations&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We’re continually adding new recommendations to the Active Assist portfolio based on customer feedback, to help you manage risk and optimize operations. &lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;1. Cloud deprecation and breaking change recommendations&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;At Google Cloud we take pains to provide backwards compatibility for our services. However, from time to time, we need to evolve the platform in a way that could impact some users e.g., for security purposes. In addition to following a stringent process to minimize customer impact, Active Assist now includes recommendations about potential breaking changes, providing an additional mechanism for customers to learn about them.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-image_full_width"&gt;






  
    &lt;div class="article-module h-c-page"&gt;
      &lt;div class="h-c-grid"&gt;
  

    &lt;figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      "
      &gt;

      
      
        
        &lt;img
            src="https://storage.googleapis.com/gweb-cloudblog-publish/images/hub-3.max-1000x1000.png"
        
          alt="hub-3"&gt;
        
        &lt;/a&gt;
      
    &lt;/figure&gt;

  
      &lt;/div&gt;
    &lt;/div&gt;
  




&lt;/div&gt;
&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Our new &lt;/span&gt;&lt;a href="https://cloud.google.com/recommender/docs/deprecation-change-recommender"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;cloud deprecation and breaking changes recommender&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; helps identify Cloud resources that will be affected by upcoming deprecations and breaking changes while providing guidelines on how to manage them. Like our other recommendations, you can view them through our &lt;/span&gt;&lt;a href="https://console.cloud.google.com/home/recommendations"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Recommendation Hub UI&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, API, and BigQuery export. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Deprecation and breaking change recommendations are offered at no charge and are available for all users today. Making sure you get ahead of these changes is important to help prevent any disruptions to your environment and ensure you are on the most reliable and supported services. &lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;2. IAM for BigQuery recommendations&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;We’ve expanded the popular IAM Recommender to include &lt;/span&gt;&lt;a href="https://cloud.google.com/policy-intelligence/docs/review-apply-role-recommendations-datasets"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;IAM recommendations on BigQuery datasets&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;. If your principals have roles on BigQuery datasets but they are not using all of the permissions within that role, you can now receive recommendations to remove or replace any of those roles. These recommendations help you enforce the principle of least privilege by ensuring that principals have only the permissions that they actually need. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;You can view your recommendations through UI, API, or BigQuery export. The recommendations are currently free but will require Security Command Center Premium after April 29th. &lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;3. Advisory Notifications recommendations&lt;br/&gt;&lt;/strong&gt;&lt;a href="https://cloud.google.com/advisory-notifications/docs/recommendations-overview"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Advisory Notifications&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; provides IAM policy recommendations to ensure the right parties within your organization have access to view critical security and privacy notifications in the Google Cloud console, so that they can receive and quickly address security notifications. &lt;/span&gt;&lt;/p&gt;
&lt;p role="presentation"&gt;&lt;strong style="vertical-align: baseline;"&gt;4. Recent change recommendations&lt;br/&gt;&lt;/strong&gt;&lt;span style="vertical-align: baseline;"&gt;We want to help you detect and mitigate issues (e.g., service outages) caused by misconfigurations to your important cloud resources. The new &lt;/span&gt;&lt;a href="https://cloud.google.com/recommender/docs/recent-change-recommendations"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;recent change recommendations&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; automatically flags recent risky changes to cloud resources that are identified as important based on their usage and other signals. For example, if you deleted a highly used project, recent change recommendations will proactively warn you about the risks associated with the change, helping to identify — and prevent — unintended issues.. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We’re excited about these latest &lt;/span&gt;&lt;a href="https://cloud.google.com/recommender/docs/change-risk-recommendations"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;change risk recommendations&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, and hope they will help you both prevent and mitigate misconfigurations and risky changes to your infrastructure. Try out the new features on &lt;/span&gt;&lt;a href="https://console.cloud.google.com/home/recommendations"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Recommendation Hub&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; yourself. If you have any feedback, please feel free to reach out to &lt;/span&gt;&lt;a href="mailto:active-assist-feedback@google.com"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;active-assist-feedback@google.com&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class="block-related_article_tout"&gt;





&lt;div class="uni-related-article-tout h-c-page"&gt;
  &lt;section class="h-c-grid"&gt;
    &lt;a href="https://cloud.google.com/blog/products/management-tools/introducing-active-assist-change-risk-recommenders/"
       data-analytics='{
                       "event": "page interaction",
                       "category": "article lead",
                       "action": "related article - inline",
                       "label": "article: {slug}"
                     }'
       class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6
        h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker"&gt;
      &lt;div class="uni-related-article-tout__inner-wrapper"&gt;
        &lt;p class="uni-related-article-tout__eyebrow h-c-eyebrow"&gt;Related Article&lt;/p&gt;

        &lt;div class="uni-related-article-tout__content-wrapper"&gt;
          &lt;div class="uni-related-article-tout__image-wrapper"&gt;
            &lt;div class="uni-related-article-tout__image" style="background-image: url('')"&gt;&lt;/div&gt;
          &lt;/div&gt;
          &lt;div class="uni-related-article-tout__content"&gt;
            &lt;h4 class="uni-related-article-tout__header h-has-bottom-margin"&gt;Active Assist change risk recommenders: Introducing a new way to prevent misconfigurations&lt;/h4&gt;
            &lt;p class="uni-related-article-tout__body"&gt;Active Assist change risk recommendations help prevent and detect common misconfigurations to help reduce risk, and improve operational r...&lt;/p&gt;
            &lt;div class="cta module-cta h-c-copy  uni-related-article-tout__cta muted"&gt;
              &lt;span class="nowrap"&gt;Read Article
                &lt;svg class="icon h-c-icon" role="presentation"&gt;
                  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#mi-arrow-forward"&gt;&lt;/use&gt;
                &lt;/svg&gt;
              &lt;/span&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  &lt;/section&gt;
&lt;/div&gt;

&lt;/div&gt;</description><pubDate>Tue, 07 May 2024 16:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/management-tools/active-assist-gets-new-hub-and-recommendations/</guid><category>AI &amp; Machine Learning</category><category>Databases</category><category>Management Tools</category><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>What’s new with Active Assist: New Hub UI and four new recommendations</title><description></description><site_name>Google</site_name><url>https://cloud.google.com/blog/products/management-tools/active-assist-gets-new-hub-and-recommendations/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Sharon Fang</name><title>Product Manager</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Ryan Ismert</name><title>Product Manager</title><department></department><company></company></author></item><item><title>repareo adopts cloud-based microservices architecture to scale its auto repair business</title><link>https://cloud.google.com/blog/products/application-modernization/how-repareo-is-modernizing-auto-repair-with-google-cloud/</link><description>&lt;div class="block-paragraph_advanced"&gt;&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;When it comes to auto repair, most of us are at the mercy of our mechanic. We take our car to the local garage, pay for the job, and hope for the best. And with little transparency into the work that has been done, or the costs involved, we can be left feeling unsure whether we got value for money.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;At &lt;/span&gt;&lt;a href="https://www.repareo.de/" rel="noopener" target="_blank"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;repareo&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt;, we are transforming the customer experience of vehicle repair, modernizing the market to make it e-commerce ready, while giving customers full transparency over the car-repair process. Now, they can describe their vehicle problem on our site and immediately receive a list of local garages, including customer reviews, cost breakdowns, and availability, allowing them to make an informed decision about where to take their car. And because our site has direct interfaces with the garages’ booking systems, customers are able to book their preferred garage and appointment directly on our site, saving time calling garages for availability. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Outgrowing our monolithic architecture&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;This year, we will be launching our new infrastructure on Google Cloud. Previously, repareo was built on a monolithic system using a small, hosted server, which was both easy for our small development team to maintain and allowed us to grow the business in a cost-efficient way. However, as we added more features and services, our monolith grew, which had an impact on our development speed, creating a bottleneck for the rest of our application. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Reliability became an issue too. repareo is integrated with car management fleets and leasing companies, and many drivers access our services through their car leasing app, making us highly dependent on third-party APIs to function effectively. As we grew, the increase in traffic resulted in these APIs becoming sluggish. During periods of peak traffic, such as the German tire-change season, we would see a 300% surge in traffic, placing a significant strain on our server, which was unable to scale effectively, causing our services to grind to a halt. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Modernizing infrastructure to modernize the market&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;The turning point came last year, when we signed a major deal with a leading global e-commerce player to integrate with its vehicle parts marketplace, enabling customers to book an installation during the checkout process. Realizing that we would need our infrastructure to be able to handle an expected tenfold increase in demand, while conforming with our new service-level agreements (SLAs), we decided it was time to move to the cloud. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We knew that a migration would bring other benefits too, enabling us to build a microservices architecture to develop services in modules, as well as allowing us to place certain workloads close to a leading global eCommerce player in vehicle parts in California, for fast response times. As we looked at cloud providers, Google Cloud stood out for the range of technologies and services it offered, with &lt;/span&gt;&lt;a href="https://cloud.google.com/bigquery?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;BigQuery&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; and &lt;/span&gt;&lt;a href="https://cloud.google.com/apigee?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Apigee&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; particularly impressing us as uniquely advanced solutions in the market. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Beyond the technology, however, we were just as impressed with Google Cloud’s deep understanding of our industry and its business network within the automotive sector, as well as by the personal relationship we quickly built with the Google Cloud team. Migrating to a new provider is a once-in-a-lifetime decision, a marriage of sorts, and with Google Cloud the relationship immediately felt like it was built to last. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;A robust, reliable system for a smoother customer experience&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We’re currently halfway through our migration, which has proved to be a steep learning curve for us, given the scale of the undertaking for our small team of developers. However, Apigee has helped to make the migration smooth by enabling us to easily set up a staging environment to test and adjust our system before going live, with no impact on our users. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We expect to have completed the migration in less than six months in total. Once live, we will have a robust, scalable system, capable of meeting the needs of a significantly larger user base. Building and managing our APIs with Apigee means we will be able to use the caching system to cache the high number of API requests on the site, allowing us to offer high-performance buffering algorithms without having to drastically increase the scale of the underlying system. And because Apigee’s logging system is so well developed, we will easily be able to spot and remedy any integration issues, to ensure our APIs function effectively. As a result, our customers will enjoy a smooth, reliable booking system and real-time repair updates, while garages will benefit from far wider reach. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;We won’t need to worry about being able to handle fluctuations in demand either, as the autoscale feature of &lt;/span&gt;&lt;a href="https://cloud.google.com/kubernetes-engine?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Google Kubernetes Engine&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; (GKE) will automatically scale up our workloads to meet surges in traffic and scale down again during quiet periods to help ensure we aren’t using more compute than necessary. This cost-efficient provisioning means we will no longer have to worry about the ability of our system to handle periods of peak traffic, with our customers benefiting from a fast, reliable service. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;Taking our developers up a gear with easy-to-use, managed services&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;Our development efficiency has already significantly improved, thanks to the built-in features and managed services of Google Cloud. Apigee, for example, has a built-in key management system to enable APIs to communicate securely, which means our development team doesn’t need to spend time and money building our own system. Similarly, the fact that &lt;/span&gt;&lt;a href="https://cloud.google.com/sql?hl=en"&gt;&lt;span style="text-decoration: underline; vertical-align: baseline;"&gt;Cloud SQL&lt;/span&gt;&lt;/a&gt;&lt;span style="vertical-align: baseline;"&gt; is a managed database means we don’t need to spend time updating and maintaining it. GKE, meanwhile, improves our developer efficiency thanks to its easy integration with automation tools, increasing our deployment speed by at least 15%.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;All of this means that our developers are free to focus on the core business logic and developing new features, such as new RPA-based technology to gather appointment availability from garage websites, which we were able to build and release inside a week, where it would have taken a month using the old infrastructure.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong style="vertical-align: baseline;"&gt;A firm foundation for sustained growth&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;With a number of other significant deals in the pipeline, repareo is now entering a period of sustained growth as we rapidly increase our customer base and prepare to enter new markets. That level of scaling simply wouldn’t be possible without Google Cloud, with its global network and regional data centers making it easy to move into any new region and enjoy rapid response times. While its scalable architecture means we can be confident that our infrastructure will always be able to scale with us.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="vertical-align: baseline;"&gt;As we continue our mission to make auto repair more transparent and convenient for more people, we are confident that with Google Cloud we have the right provider to help move our business into the fast lane.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;</description><pubDate>Tue, 07 May 2024 08:00:00 +0000</pubDate><guid>https://cloud.google.com/blog/products/application-modernization/how-repareo-is-modernizing-auto-repair-with-google-cloud/</guid><category>Infrastructure Modernization</category><category>Customers</category><category>Application Modernization</category><media:content height="540" url="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_30.max-600x600.jpg" width="540"></media:content><og xmlns:og="http://ogp.me/ns#"><type>article</type><title>repareo adopts cloud-based microservices architecture to scale its auto repair business</title><description></description><image>https://storage.googleapis.com/gweb-cloudblog-publish/images/image_30.max-600x600.jpg</image><site_name>Google</site_name><url>https://cloud.google.com/blog/products/application-modernization/how-repareo-is-modernizing-auto-repair-with-google-cloud/</url></og><author xmlns:author="http://www.w3.org/2005/Atom"><name>Philipp Haac</name><title>CEO, repareo</title><department></department><company></company></author><author xmlns:author="http://www.w3.org/2005/Atom"><name>Tobias Reisner</name><title>CTO &amp; Co-Founder, repareo</title><department></department><company></company></author></item></channel></rss>