<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Guillaume Laforge on Medium]]></title>
        <description><![CDATA[Stories by Guillaume Laforge on Medium]]></description>
        <link>https://medium.com/@glaforge?source=rss-431147437aeb------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/0*Fbu6IxbH-XbgjDml.jpeg</url>
            <title>Stories by Guillaume Laforge on Medium</title>
            <link>https://medium.com/@glaforge?source=rss-431147437aeb------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 01 Apr 2024 06:28:46 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@glaforge/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Gemini codelab for Java developers using LangChain4j]]></title>
            <link>https://medium.com/google-cloud/gemini-codelab-for-java-developers-using-langchain4j-769fbd419756?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/769fbd419756</guid>
            <category><![CDATA[langchain4j]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[java]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Wed, 27 Mar 2024 00:00:46 GMT</pubDate>
            <atom:updated>2024-03-29T03:27:13.378Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*f6qD8NYosB2mVZD2.jpg" /></figure><p>No need to be a Python developer to do Generative AI! If you’re a Java developer, you can take advantage of <a href="https://docs.langchain4j.dev/">LangChain4j</a> to implement some advanced LLM integrations in your Java applications. And if you’re interested in using <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini</a>, one of the best models available, I invite you to have a look at the following “codelab” that I worked on:</p><p><a href="https://codelabs.developers.google.com/codelabs/gemini-java-developers">Codelab — Gemini for Java Developers using LangChain4j</a></p><p>In this workshop, you’ll find various examples covering the following use cases, in <em>crescendo</em> approach:</p><ul><li>Making your fist call to Gemini (streaming &amp; non-streaming)</li><li>Maintaining a conversation</li><li>Taking advantage of multimodality by analysing images with your prompts</li><li>Extracting structured information from unstructured text</li><li>Using prompt templates</li><li>Doing text classification with few-shot prompting</li><li>Implementing Retrieval Augmented Generation to chat with your documentation</li><li>How to do Function Calling to expand the LLM to interact with external APIs and services</li></ul><p>You’ll find all the <a href="https://github.com/glaforge/gemini-workshop-for-java-developers">code samples on Github</a>.</p><p>If you’re attending Devoxx France, be sure to attend the <a href="https://www.devoxx.fr/en/schedule/talk/?id=40285">Hands-on-Lab workshop</a> with my colleagues <a href="https://twitter.com/meteatamel">Mete Atamel</a> and <a href="https://twitter.com/val_deleplace">Valentin Deleplace</a> who will guide you through this codelab.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/03/27/gemini-codelab-for-java-developers/"><em>https://glaforge.dev</em></a><em> on March 27, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=769fbd419756" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/gemini-codelab-for-java-developers-using-langchain4j-769fbd419756">Gemini codelab for Java developers using LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualize PaLM-based LLM tokens]]></title>
            <link>https://medium.com/google-cloud/visualize-palm-based-llm-tokens-8760b3122c0f?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/8760b3122c0f</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Mon, 05 Feb 2024 00:00:45 GMT</pubDate>
            <atom:updated>2024-02-05T15:46:50.859Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iUeKnOWdjYkVi3BY.jpg" /></figure><p>As I was working on tweaking the Vertex AI text embedding model in <a href="https://github.com/langchain4j">LangChain4j</a>, I wanted to better understand how the textembedding-gecko<a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings">model</a> tokenizes the text, in particular when we implement the <a href="https://arxiv.org/abs/2005.11401">Retrieval Augmented Generation</a> approach.</p><p>The various PaLM-based models offer a computeTokens endpoint, which returns a list of tokens (encoded in Base 64) and their respective IDs.</p><blockquote><strong><em>Note:</em></strong><em> At the time of this writing, there’s no equivalent endpoint for Gemini models.</em></blockquote><p>So I decided to create a <a href="https://tokens-lpj6s2duga-ew.a.run.app/">small application</a> that lets users:</p><ul><li>input some text,</li><li>select a model,</li><li>calculate the number of tokens,</li><li>and visualize them with some nice pastel colors.</li></ul><p>The available PaLM-based models are:</p><ul><li>textembedding-gecko</li><li>textembedding-gecko-multilingual</li><li>text-bison</li><li>text-unicorn</li><li>chat-bison</li><li>code-gecko</li><li>code-bison</li><li>codechat-bison</li></ul><p>You can <a href="https://tokens-lpj6s2duga-ew.a.run.app/">try the application</a> online.</p><p>And also have a look at the <a href="https://github.com/glaforge/llm-text-tokenization">source code</a> on Github. It’s a <a href="https://micronaut.io/">Micronaut</a> application. I serve the static assets as explained in my recent <a href="https://glaforge.dev/posts/2024/01/21/serving-static-assets-with-micronaut/">article</a>. I deployed the application on <a href="https://cloud.run/">Google Cloud Run</a>, the easiest way to deploy a container, and let it auto-scale for you. I did a source based deployment, as explained at the bottom <a href="https://glaforge.dev/posts/2022/10/24/build-deploy-java-17-apps-on-cloud-run-with-cloud-native-buildpacks-on-temurin/">here</a>.</p><p>And <em>voilà</em> I can visualize my LLM tokens!</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/02/05/visualize-palm-based-llm-tokens/"><em>https://glaforge.dev</em></a><em> on February 5, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8760b3122c0f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/visualize-palm-based-llm-tokens-8760b3122c0f">Visualize PaLM-based LLM tokens</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Image generation with Imagen and LangChain4j]]></title>
            <link>https://medium.com/google-cloud/image-generation-with-imagen-and-langchain4j-61ca08ae6aac?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/61ca08ae6aac</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[imagen]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[langchain]]></category>
            <category><![CDATA[generative-ai-use-cases]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Thu, 01 Feb 2024 00:00:35 GMT</pubDate>
            <atom:updated>2024-02-02T03:38:58.802Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0iPZQvd0ZVDuJjeC.jpg" /></figure><p>This week <a href="https://github.com/langchain4j">LangChain4j</a>, the LLM orchestration framework for Java developers, released version <a href="https://github.com/langchain4j/langchain4j/releases/tag/0.26.1">0.26.1</a>, which contains my first significant contribution to the open source project: <strong>support for the Imagen image generation model</strong>.</p><p><strong>Imagen</strong> is a text-to-image diffusion model that was <a href="https://imagen.research.google/">announced</a> last year. And it recently upgraded to <a href="https://deepmind.google/technologies/imagen-2/">Imagen v2</a>, with even higher quality graphics generation. As I was curious to integrate it in some of my generative AI projects, I thought that would be a great first <a href="https://github.com/langchain4j/langchain4j/pull/456">contribution</a> to LangChain4j.</p><blockquote><strong><em>Caution:</em></strong><em> At the time of this writing, image generation is still only for allow-listed accounts.</em></blockquote><blockquote><em>Furthermore, to run the snippets covered below, you should have an account on Google Cloud Platform, created a project, configured a billing account, enabled the Vertex AI API, and authenticated with the gcloud SDK and the command: </em><em>gcloud auth application-default login.</em></blockquote><p>Now let’s dive in how to use Imagen v1 and v2 with LangChain4j in Java!</p><h3>Generate your first images</h3><p>In the following examples, I’m using the following constants, to point at my project details, the endpoint, the region, etc:</p><pre>private static final String ENDPOINT = &quot;us-central1-aiplatform.googleapis.com:443&quot;;<br>private static final String LOCATION = &quot;us-central1&quot;;<br>private static final String PROJECT = &quot;YOUR_PROJECT_ID&quot;;<br>private static final String PUBLISHER = &quot;google&quot;;</pre><p>First, we’re going to create an instance of the model:</p><pre>VertexAiImageModel imagenModel = VertexAiImageModel.builder()<br>    .endpoint(ENDPOINT)<br>    .location(LOCATION)<br>    .project(PROJECT)<br>    .publisher(PUBLISHER)<br>    .modelName(&quot;imagegeneration@005&quot;)<br>    .maxRetries(2)<br>    .withPersisting()<br>    .build();</pre><p>There are 2 models you can use:</p><ul><li>imagegeneration@005 corresponds to Imagen 2</li><li>imagegeneration@002 is the previous version (Imagen 1)</li></ul><p>In this article, we’ll use both models. Why? Because currently Imagen 2 doesn’t support image editing, so we’ll have to use Imagen 1 for that purpose.</p><p>The configuration above uses withPersisting() to save the generated images in a temporary folder on your system. If you don&#39;t persist the image files, the content of the image is avaiable as Base 64 encoded bytes in the Images objects returned. You can also specify persistTo(somePath) to specify a particular directory where you want the generated files to be saved.</p><p>Let’s create our first image:</p><pre>Response&lt;Image&gt; imageResponse = imagenModel.generate(<br>    &quot;watercolor of a colorful parrot drinking a cup of coffee&quot;);</pre><p>The Response object wraps the created Image. You can get the Image by calling imageResponse.getContent(). And you can retrieve the URL of the image (if saved locally) with imageResponse.getContent().url(). The Base 64 encoded bytes can be retrieved with imageResponse.getContent().base64Data()</p><p>Some other tweaks to the model configuration:</p><ul><li>Specify the <strong>language</strong> of the prompt: language(&quot;ja&quot;) (if the language is not officially supported, it&#39;s usually translated back to English anyway).</li><li>Define a <strong>negative prompt</strong> with things you don’t want to see in the picture: negativePrompt(&quot;black feathers&quot;).</li><li>Use a particular <strong>seed</strong> to always generate the same image with the same seed: seed(1234L).</li></ul><p>So if you want to generate a picture of a pizza with a prompt in Japanese, but you don’t want to have pepperoni and pineapple, you could configure your model and generate as follows:</p><pre>VertexAiImageModel imagenModel = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@005&quot;)<br>        .language(&quot;ja&quot;)<br>        .negativePrompt(&quot;pepperoni, pineapple&quot;)<br>        .maxRetries(2)<br>        .withPersisting()<br>        .build();<br><br>Response&lt;Image&gt; imageResponse = imagenModel.generate(&quot;ピザ&quot;); // pizza</pre><h3>Image editing with Imagen 1</h3><p>With Imagen 1, you can <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/image/edit-images?hl=en">edit</a> existing images:</p><ul><li><strong>mask-based editing:</strong> you can specify a mask, a black &amp; white image where the white parts are the corresponding parts of the original image that should be edited,</li><li><strong>mask free editing:</strong> where you just give a prompt and let the model figure out what should be edited on its own or following the prompt.</li></ul><p>When generating and editing with Imagen 1, you can also configure the model to use a particular style (with Imagen 2, you just specify it in the prompt) with sampleImageStyle(VertexAiImageModel.ImageStyle.photograph):</p><p>- photograph<br>- digital_art<br>- landscape<br>- sketch<br>- watercolor<br>- cyberpunk<br>- pop_art</p><p>When editing an image, you may wish to decide how strong or not the modification should be, with .guidanceScale(100). Usually, between 0 and 20 or so, it&#39;s lightly edited, between 20 and 100 it&#39;s getting more impactful edits, and 100 and above it&#39;s the maximum edition level.</p><p>Let’s say I generated an image of a lush forrest (I’ll use that as my original image):</p><pre>VertexAiImageModel model = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@002&quot;)<br>        .seed(19707L)<br>        .sampleImageStyle(VertexAiImageModel.ImageStyle.photograph)<br>        .guidanceScale(100)<br>        .maxRetries(4)<br>        .withPersisting()<br>        .build();<br><br>Response&lt;Image&gt; forestResp = model.generate(&quot;lush forest&quot;);</pre><p>Now I want to edit my forrest to add a small red tree in the bottom of the image. I’m loading a black and white mask image with a white square at the bottom. And I pass the original image, the mask image, and the modification prompt, to the new edit() method:</p><pre>URI maskFileUri = getClass().getClassLoader().getResource(&quot;mask.png&quot;).toURI();<br><br>Response&lt;Image&gt; compositeResp = model.edit(<br>        forestResp.content(),              // original image to edit<br>        fromPath(Paths.get(maskFileUri)),  // the mask image<br>        &quot;red trees&quot;                        // the new prompt<br>);</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*AfQ24hvdH9hoTLsT.jpg" /></figure><p>Another kind of editing you can do is to upscale an existing image. As far as I know, it’s only supported for Imagen v1 for now, so we’ll continue with that model.</p><p>In this example, we’ll generate an image of 1024x1024 pixels, and we’ll scale it to 4096x4096:</p><pre>VertexAiImageModel imagenModel = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@002&quot;)<br>        .sampleImageSize(1024)<br>        .withPersisting()<br>        .persistTo(defaultTempDirPath)<br>        .maxRetries(3)<br>        .build();<br><br>Response&lt;Image&gt; imageResponse =<br>        imagenModel.generate(&quot;A black bird looking itself in an antique mirror&quot;);<br><br>VertexAiImageModel imagenModelForUpscaling = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@002&quot;)<br>        .sampleImageSize(4096)<br>        .withPersisting()<br>        .persistTo(defaultTempDirPath)<br>        .maxRetries(3)<br>        .build();<br><br>Response&lt;Image&gt; upscaledImageResponse =<br>        imagenModelForUpscaling.edit(imageResponse.content(), &quot;&quot;);</pre><p>And now you have a much bigger image!</p><h3>Conclusion</h3><p>That’s about it for image generation and editing with <strong>Imagen</strong> in <strong>LangChain4j</strong> today! Be sure to use LangChain4j <strong>v0.26.1</strong> which contains that new integration. And I’m looking forward to seeing the pictures you generate with it!</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/02/01/image-generation-with-imagen-and-langchain4j/"><em>https://glaforge.dev</em></a><em> on February 1, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61ca08ae6aac" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/image-generation-with-imagen-and-langchain4j-61ca08ae6aac">Image generation with Imagen and LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gemini Function Calling]]></title>
            <link>https://medium.com/google-cloud/gemini-function-calling-1585c044d28d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/1585c044d28d</guid>
            <category><![CDATA[java]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[gemini]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 22 Dec 2023 00:00:37 GMT</pubDate>
            <atom:updated>2023-12-28T07:03:58.457Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yW4nG0dXoxO1tsdaOYzWuA.png" /></figure><p>A promising feature of the Gemini large language model released recently by <a href="https://deepmind.google/">Google DeepMind</a>, is the support for <a href="https://ai.google.dev/docs/function_calling">function calls</a>. It’s a way to supllement the model, by letting it know an external functions or APIs can be called. So you’re not limited by the knowledge cut-off of the model: instead, in the flow of the conversation with the model, you can pass a list of functions the model will know are available to get the information it needs, to complete the generation of its answer.</p><p>For example, if you want to ask the model about the weather, it doesn’t have the realtime information about the weather forecast. But we can tell it that there’s a function that can be called, to get the forecast for a given location. Internally, the model will acknowledge it doesn’t know the answer about the weather, but it will request that you call an external function that you describe, using a specific set of parameters which correspond to the user’s request.</p><p>Just days ago, I wrote about how to <a href="https://glaforge.dev/posts/2023/12/13/get-started-with-gemini-in-java/">get started with Gemini in Java</a>. In that article, we explored how to use the hand-written Java SDK that is available to interact with Gemini from Java. However, the Java SDK doesn’t yet expose all the features of the model: in particular, function calling is missing. But not all hope is lost! Because under the hood, the SDK relies on the generated protobuf classes library, which exposes everything!</p><blockquote><em>Soon, Gemini will be supported by </em><a href="https://github.com/langchain4j/langchain4j"><em>LangChain4j</em></a><em>, and the Java SDK will also provide an easier way to take care of function calling. But in this article, I wanted to explore the use of the internal protobuf classes, to see how to best implement its support in the SDK.</em></blockquote><p>Let’s go step by step!</p><p>Instead of using the GenerativeModel API from the SDK, we&#39;ll go straight with the PredictionServiceClient:</p><pre>try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>  PredictionServiceClient client = vertexAI.getPredictionServiceClient();<br>  ...<br>}</pre><p>We need to prepare a function declaration to describe the kind of functions that the LLM can ask us to call, and we’ll wrap it in a Tool:</p><pre>FunctionDeclaration functionDeclaration = FunctionDeclaration.newBuilder()<br>    .setName(&quot;getCurrentWeather&quot;)<br>    .setDescription(&quot;Get the current weather in a given location&quot;)<br>    .setParameters(<br>        Schema.newBuilder()<br>            .setType(Type.OBJECT)<br>            .putProperties(&quot;location&quot;, Schema.newBuilder()<br>                .setType(Type.STRING)<br>                .setDescription(&quot;location&quot;)<br>                .build()<br>            )<br>            .addRequired(&quot;location&quot;)<br>            .build()<br>    )<br>    .build();<br><br>Tool tool = Tool.newBuilder()<br>    .addFunctionDeclarations(functionDeclaration)<br>    .build();</pre><p>Functions are described using classes that represent a subset of the OpenAPI 3 specification.</p><blockquote><em>This is important to provide descriptions for the functions and its parameters, as the LLM will use that information to figure out which function to call, and which parameters should be passed.</em></blockquote><p>Next, let’s prepare a question asking about the weather in Paris, and configuring the text generation request with that prompt and the tool defined above:</p><pre>String resourceName = String.format(<br>    &quot;projects/%s/locations/%s/publishers/google/models/%s&quot;,<br>    vertexAI.getProjectId(), vertexAI.getLocation(), modelName);<br><br>Content questionContent =<br>    ContentMaker.fromString(&quot;What&#39;s the weather in Paris?&quot;);<br><br>GenerateContentRequest questionContentRequest =<br>    GenerateContentRequest.newBuilder()<br>        .setEndpoint(resourceName)<br>        .setModel(resourceName)<br>        .addTools(tool)<br>        .addContents(questionContent)<br>        .build();<br><br>ResponseStream&lt;GenerateContentResponse&gt; responseStream =<br>    new ResponseStream&lt;&gt;(new ResponseStreamIteratorWithHistory&lt;&gt;(<br>        client<br>            .streamGenerateContentCallable()<br>            .call(questionContentRequest)<br>            .iterator())<br>);<br><br>GenerateContentResponse generateContentResponse =<br>    responseStream.stream().findFirst().get();<br>Content callResponseContent =<br>    generateContentResponse.getCandidates(0).getContent();</pre><p>If you print the callResponseContent variable, you&#39;ll see that it contains a function call request, suggesting that you should call the predefined function with the parameter of Paris:</p><pre>role: &quot;model&quot;<br>parts {<br>  function_call {<br>    name: &quot;getCurrentWeather&quot;<br>    args {<br>      fields {<br>        key: &quot;location&quot;<br>        value {<br>          string_value: &quot;Paris&quot;<br>        }<br>      }<br>    }<br>  }<br>}</pre><p>At that point, as the developer, it’s your turn to work a little, and make the call to that function yourself! Let’s pretend I called an external Web Service that gives weather information, and that it returns some JSON payload that would look like so:</p><pre>{<br>  &quot;weather&quot;: &quot;sunny&quot;,<br>  &quot;location&quot;: &quot;Paris&quot;<br>}</pre><p>We need now to create a function response structure to pass that information back to the LLM:</p><pre>Content contentFnResp = Content.newBuilder()<br>    .addParts(Part.newBuilder()<br>        .setFunctionResponse(<br>            FunctionResponse.newBuilder()<br>                .setResponse(<br>                    Struct.newBuilder()<br>                        .putFields(&quot;weather&quot;,<br>                            Value.newBuilder().setStringValue(&quot;sunny&quot;).build())<br>                        .putFields(&quot;location&quot;,<br>                            Value.newBuilder().setStringValue(&quot;Paris&quot;).build())<br>                        .build()<br>                )<br>                .build()<br>        )<br>        .build())<br>    .build();</pre><p>Then, since LLMs are actually stateless beasts, we need to give it the whole context of the conversation again, passing the query, the function call response the model suggested us to make, as well as the response we got from the external weather service:</p><pre>GenerateContentRequest generateContentRequest = GenerateContentRequest.newBuilder()<br>    .setEndpoint(resourceName)<br>    .setModel(resourceName)<br>    .addContents(questionContent)<br>    .addContents(callResponseContent)<br>    .addContents(contentFnResp)<br>    .addTools(tool)<br>    .build();</pre><p>And to finish, we’ll invoke the client one last time with that whole dialog and information, and print a response out:</p><pre>responseStream = new ResponseStream&lt;&gt;(new ResponseStreamIteratorWithHistory&lt;&gt;(<br>    client<br>        .streamGenerateContentCallable()<br>        .call(generateContentRequest)<br>        .iterator())<br>);<br><br>for (GenerateContentResponse resp : responseStream) {<br>    System.out.println(ResponseHandler.getText(resp));<br>}</pre><p>And happily, Gemini will reply to us that:</p><pre>The weather in Paris is sunny.</pre><p>What a lovely way to start the holiday season with a nice and sunny weather!</p><p>I wish you all happy year end festivities, and I look forward to seeing you next year. Hopefully next month, I’ll be able to show you some cool new SDK features or the LangChain4j integration! Thanks for reading.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/22/gemini-function-calling/"><em>https://glaforge.dev</em></a><em> on December 22, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1585c044d28d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/gemini-function-calling-1585c044d28d">Gemini Function Calling</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualize and Inspect Workflows Executions]]></title>
            <link>https://medium.com/google-cloud/visualize-and-inspect-workflows-executions-00aafe188d3d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/00aafe188d3d</guid>
            <category><![CDATA[gcp-workflows]]></category>
            <category><![CDATA[workflow]]></category>
            <category><![CDATA[debugging]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 22 Dec 2023 00:00:27 GMT</pubDate>
            <atom:updated>2023-12-22T14:23:56.467Z</atom:updated>
            <content:encoded><![CDATA[<p>When using a service like Google Cloud <a href="https://cloud.google.com/workflows/">Workflows</a>, in particular as your workflows get bigger, it can be difficult to understand what’s going on under the hood. With multiple branches, step jumps, iterations, and also parallel branches and iterations, if your workflow fails during an execution, until now, you had to check the execution status, or go deep through the logs to find more details about the failed step.</p><p>I have good news for you! Workflows recently added some deeper introspection capability: you can now <a href="https://cloud.google.com/workflows/docs/debug-steps">view the history of execution steps</a>. From the Google Cloud console, you can see the lists of steps, and see the logical flow between them. The usual workflow visualisation will also highlight in green the successful steps, and in red the failed one. Of course, it is also possible to make a curl call to get the JSON of the <a href="https://cloud.google.com/workflows/docs/debug-steps#list-entries">list of executed steps</a>.</p><p>Let’s have a look!</p><p>In the console, when you click on an execution, in the summary tab, you&#39;ll see not only the failed step, but also the nice workflow graph colored green and red:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xomRWaQkCtyo6N5S.png" /></figure><p>That way, you know which path the execution followed, in a visual manner. But you can also see the actual list of steps executed, with more details, by clicking on the steps tab:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/888/0*rgzpqpjUpJ3YmsD8.png" /></figure><p>From this table, the filter will let you further refine particular type of steps you’d like to investigate, or visualise the steps of a subworkflow only, etc.</p><p>This is a nice improvement to the developer experience, and for your ops team, to better understand what happens during your workflow executions! Feel free to read more about this new capabability in the documentation about <a href="https://cloud.google.com/workflows/docs/debug-steps">viewing the history of execution steps</a>.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/22/visualize-and-inspect-workflows-executions/"><em>https://glaforge.dev</em></a><em> on December 22, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=00aafe188d3d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/visualize-and-inspect-workflows-executions-00aafe188d3d">Visualize and Inspect Workflows Executions</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hands on Codelabs to dabble with Large Language Models in Java]]></title>
            <link>https://medium.com/google-cloud/hands-on-codelabs-to-dabble-with-large-language-models-in-java-ee7bc330f5fe?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/ee7bc330f5fe</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[langchain]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Mon, 18 Dec 2023 00:00:36 GMT</pubDate>
            <atom:updated>2023-12-19T06:49:15.192Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QAqyUui7Mp-t2o8U.png" /></figure><p>Hot on the heels of the <a href="https://glaforge.dev/posts/2023/12/13/get-started-with-gemini-in-java/">release of Gemini</a>, I’d like to share a couple of resources I created to get your hands on large language models, using <a href="https://github.com/langchain4j/">LangChain4J</a>, and the <a href="https://ai.google/discover/palm2/">PaLM 2</a> model. Later on, I’ll also share with you articles and codelabs that take advantage of Gemini, of course.</p><p>The PaLM 2 model supports 2 modes:</p><ul><li>text generation,</li><li>and chat.</li></ul><p>In the 2 codelabs, you’ll need to have created an account on Google Cloud, and created a project. The codelabs will guide you through the steps to setup the environment, and show you how to use the Google Cloud built-in shell and code editor, to develop in the cloud.</p><p>You should be a Java developer, as the examples are in Java, use the <a href="https://github.com/langchain4j/">LangChain4J</a> project, and Maven for building the code.</p><h3>Generative AI text generation in Java with PaLM and LangChain4J</h3><p>In the first <a href="https://codelabs.developers.google.com/codelabs/genai-text-gen-java-palm-langchain4j?hl=en#0">codelab</a> you can explore:</p><ul><li>how to make your first call to PaLM for simple question/answer scenarios</li><li>how to extract structured data out of unstructured text</li><li>how to use prompts and prompt templates</li><li>how to classify text, with an example on sentiment analysis</li></ul><h3>Generative AI powered chat with users and docs in Java with PaLM and LangChain4J</h3><p>In the second <a href="https://codelabs.developers.google.com/codelabs/genai-chat-java-palm-langchain4j?hl=en#0">codelab</a> you’ll use the chat model to learn:</p><ul><li>how to create your first chat with the PaLM model</li><li>how to give your chatbot a personality, with an example with a chess player</li><li>how to extract structured data out of unstructured text using LangChain4J’s AiServices and its annotations</li><li>how to implement Retrieval Augmented Generation (RAG) to answer questions about your own documentation</li></ul><h3>Going further with Generative AI</h3><p>If you’re interested in going further with Generative AI, and learn more, feel free to <a href="https://goo.gle/generativeai">join the Google Cloud Innovators program</a>.</p><p>Google Cloud Innovators is <strong>free</strong> and includes:</p><ul><li>live discussions, AMAs, and roadmap sessions to learn the latest directly from Googlers,</li><li>the latest Google Cloud news right in your inbox,</li><li>digital badge and video conference background,</li><li>500 credits of labs and learning on Skills Boost.</li></ul><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/18/get-hands-on-codelabs-to-dabble-with-llms/"><em>https://glaforge.dev</em></a><em> on December 18, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ee7bc330f5fe" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/hands-on-codelabs-to-dabble-with-large-language-models-in-java-ee7bc330f5fe">Hands on Codelabs to dabble with Large Language Models in Java</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Get Started with Gemini in Java]]></title>
            <link>https://medium.com/google-cloud/get-started-with-gemini-in-java-923f2069ea4d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/923f2069ea4d</guid>
            <category><![CDATA[java]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Wed, 13 Dec 2023 00:00:25 GMT</pubDate>
            <atom:updated>2023-12-18T02:13:02.158Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Logo of the Gemini large language model launched by Google" src="https://cdn-images-1.medium.com/max/1024/0*4ohfuLfaP-ZAbo5_" /></figure><p>Google announced today the availability of <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-support-on-vertex-ai">Gemini</a>, its latest and more powerful Large Language Model. Gemini is <strong>multimodal</strong>, which means it’s able to consume not only text, but also images or videos.</p><p>I had the pleasure of working on the Java samples and help with the Java SDK, with wonderful engineer colleagues, and I’d like to share some examples of <strong>what you can do with Gemini, using Java</strong>!</p><p>First of all, you’ll need to have an account on Google Cloud and created a project. The Vertex AI API should be enabled, to be able to access the Generative AI services, and in particular the Gemini large language model. Be sure to check out the <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal?hl=en">instructions</a>.</p><h3>Preparing your project build</h3><p>To get started with some coding, you’ll need to create a Gradle or a Maven build file that requires the Google Cloud libraries BOM, and the google-cloud-vertexai library. Here&#39;s an example with Maven:</p><pre>...<br>&lt;dependencyManagement&gt;<br>    &lt;dependencies&gt;<br>        &lt;dependency&gt;<br>            &lt;artifactId&gt;libraries-bom&lt;/artifactId&gt;<br>            &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;<br>            &lt;scope&gt;import&lt;/scope&gt;<br>            &lt;type&gt;pom&lt;/type&gt;<br>            &lt;version&gt;26.29.0&lt;/version&gt;<br>        &lt;/dependency&gt;<br>    &lt;/dependencies&gt;<br>&lt;/dependencyManagement&gt;<br><br>&lt;dependencies&gt;<br>    &lt;dependency&gt;<br>        &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;<br>        &lt;artifactId&gt;google-cloud-vertexai&lt;/artifactId&gt;<br>    &lt;/dependency&gt;<br>    ...<br>&lt;/dependencies&gt;<br>...</pre><h3>Your first queries</h3><p>Now let’s have a look at our first multimodal example, mixing text prompts and images:</p><pre>try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>    byte[] imageBytes = Base64.getDecoder().decode(dataImageBase64);<br><br>    GenerativeModel model = new GenerativeModel(&quot;gemini-pro-vision&quot;, vertexAI);<br>    GenerateContentResponse response = model.generateContent(<br>        ContentMaker.fromMultiModalData(<br>            &quot;What is this image about?&quot;,<br>            PartMaker.fromMimeTypeAndData(&quot;image/jpg&quot;, imageBytes)<br>        ));<br><br>    System.out.println(ResponseHandler.getText(response));<br>}</pre><p>You instantiate VertexAI with your Google Cloud project ID, and the region location of your choice. To pass images to Gemini, you should either pass the bytes directly, or you can pass a URI of an image stored in a cloud storage bucket (like gs://my-bucket/my-img.jpg). You create an instance of the model. Here, I&#39;m using gemini-pro-vision. But later on, a gemini-ultra-vision model will also be available. Let&#39;s ask the model to generate content with the generateContent() method, by passing both a text prompt, and also an image. The ContentMaker and PartMaker classes are helpers to further simplify the creation of more advanced prompts that mix different modalities. But you could also just pass a simple string as argument of the generateContent() method. The ResponseHandler utility will retrieve all the text of the answer of the model.</p><p>Instead of getting the whole output once all the text is generated, you can also adopt a streaming approach:</p><pre>model.generateContentStream(&quot;Why is the sky blue?&quot;)<br>    .stream()<br>    .forEach(System.out::print);</pre><p>You can also iterate over the stream with a for loop:</p><pre>ResponseStream&lt;GenerateContentResponse&gt; responseStream =<br>    model.generateContentStream(&quot;Why is the sky blue?&quot;);<br><br>for (GenerateContentResponse responsePart: responseStream) {<br>    System.out.print(ResponseHandler.getText(responsePart));<br>}</pre><h3>Let’s chat!</h3><p>Gemini is a multimodal model, and it’s actually both a text generation model, but also a chat model. So you can chat with Gemini, and ask a series of questions in context. There’s a handy ChatSession utility class which simplifies the handling of the conversation:</p><pre>try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>    GenerateContentResponse response;<br><br>    GenerativeModel model = new GenerativeModel(modelName, vertexAI);<br>    ChatSession chatSession = new ChatSession(model);<br><br>    response = chatSession.sendMessage(&quot;Hello.&quot;);<br>    System.out.println(ResponseHandler.getText(response));<br><br>    response = chatSession.sendMessage(&quot;What are all the colors in a rainbow?&quot;);<br>    System.out.println(ResponseHandler.getText(response));<br><br>    response = chatSession.sendMessage(&quot;Why does it appear when it rains?&quot;);<br>    System.out.println(ResponseHandler.getText(response));<br>}</pre><p>This is convenient to use ChatSession as it takes care of keeping track of past questions from the user, and answers from the assistant.</p><h3>Going further</h3><p>This is just a few examples of the capabilities of Gemini. Be sure to check out some of the <a href="https://github.com/GoogleCloudPlatform/java-docs-samples/tree/main/vertexai/snippets/src/main/java/vertexai/gemini">samples that are available on Github</a>. Read <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal?hl=en">more about Gemini and Generative AI</a> in the Google Cloud documentation.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/13/get-started-with-gemini-in-java/"><em>https://glaforge.dev</em></a><em> on December 13, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=923f2069ea4d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/get-started-with-gemini-in-java-923f2069ea4d">Get Started with Gemini in Java</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tech Watch #4 — October, 27, 2023]]></title>
            <link>https://glaforge.medium.com/tech-watch-4-october-27-2023-d48a1449eeb0?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/d48a1449eeb0</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[tech-watch]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 27 Oct 2023 15:04:58 GMT</pubDate>
            <atom:updated>2023-10-27T15:04:58.796Z</atom:updated>
            <content:encoded><![CDATA[<h3>Tech Watch #4 — October, 27, 2023</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_2CIOvErFLG1qdX8tDCSqQ.png" /></figure><ul><li>The <a href="https://www.stateof.ai/">State of AI report</a> is pretty interesting to read (even if long!). Among the major sections: research, industry, but also politics, safety, and some predictions. You’ll find an executive summary in one slide, on slide #8.<br>On #22, <strong>emergent capabilities of LLMs</strong> is covered and mentions Stanford’s research that talks about the importance of more linear and continuous measures as otherwise capabilities sound like they emerge out of the blue.<br>On #23, they talk about the <strong>context length of LLMs being the new parameter count</strong>, as models try to have bigger context windows.<br>However, on slide #24, they also talk about researchers who showed that <strong>in long context windows the content provided in the middle is more ignored</strong> by LLMs compared to content at the beginning or end of the window.<br>So be sure to <strong>put the important bits first or last</strong>, but not lost in the middle.<br>Slide #26 speaks about <strong>smaller models trained with smaller curated datasets and can rival 50x bigger models</strong>.<br>Slide #28 wonders if we’re <strong>running out of human-generated data</strong>, and thus, if we’re going to have our LLMs trained on… LLM generated data!</li><li><a href="https://projector.tensorflow.org/">3D visualisation of vector embeddings from Tensorflow</a><br>As I’m working on a small application that would help visuliase vector embeddings, I was looking for existing apps or articles that show how vectors can be similar, and thus their semantic to be similar as well. And I came across this existing visualisation from the Tensorflow project, which uses the Word2Vec embedding approach. I like the fact you can use different 3D projections techniques like t-SNE or PCA, and you see related vectors closer in the 3D space, as their meaning is closer too.</li><li><a href="https://www.citusdata.com/blog/2023/10/26/making-postgres-tick-new-features-in-pg-cron/">A cron extension for PostgreSQL</a><br>pg_cron is an extension for the PostgreSQL database that adds scheduling capabilities. It can even be scheduled to run your procedures or other SQL queries every few seconds.</li><li><a href="https://protomaps.com/">Protomaps</a> is a free and open source map of the world, deployable as a single static file on cloud storage (including Google Cloud Storage). You can use OpenStreetMap tiles, as it’s distributed with a version of OSM. It’s using an efficient and open archive format for pyramids of tile data, accessible via HTTP Range requests.</li><li><a href="https://artistassistapp.com/">ArtistAssistApp</a> is an application which can tell you which oil or water color paints to use and mix to create similar looking colors for your painting, as you try to reproduce a photo. As a wannabe painter myself, I always struggle creating mixes that match real colors, and this tool is pretty clever to let you find the right mix (at least if you use some well-known paint brands). This also reminds me of <a href="https://scrtwpns.com/mixbox/">mixbox</a> which simulates color mixing as real color pigments mix in real paint, and using such algorithm would greatly improve the real-life accuracy of color mixes in digital art painting applications.</li><li><a href="https://vectorizer.ai/">Vectorizer</a> is an online tool to transform an image into an SVG file. As I’m playing a bit with Generative AI-based image generation, sometimes, the upscalers don’t suffice, and you want to transform a nice generated image into a vectorial format (for example clipart-like illustrations), so they scale gracefully in slide decks or on websites.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d48a1449eeb0" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tech Watch #3 — October, 20, 2023]]></title>
            <link>https://glaforge.medium.com/tech-watch-3-october-20-2023-11a70245017d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/11a70245017d</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[unicode]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[groovy]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 20 Oct 2023 19:49:34 GMT</pubDate>
            <atom:updated>2023-10-20T19:49:34.036Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*R11l0mdYmdHwQkT2PgRidQ.png" /></figure><h3>Tech Watch #3 — October, 20, 2023</h3><ul><li><a href="https://horstmann.com/unblog/2023-10-03/index.html">Stop Using char in Java. And Code Points</a><br>It’s a can of worms, when you start messing with chars, code points, and you’re likely going to get it wrong in the end. As much as possible, stay away from chars and code points, and instead, use as much as possible the String methods like indexOf() / substring(), and some regex when you really need to find grapheme clusters.</li><li>Paul King shared his presentations on <a href="https://speakerdeck.com/paulk/groovy-today">Why use Groovy in 2023</a> and an <a href="https://speakerdeck.com/paulk/groovy-roadmap">update on the Groovy 5 roadmap</a>It’s interesting to see how and where Groovy goes beyond what is offered by Java, sometimes thanks to its dynamic nature, sometimes because of its compile-time transformation capabilities. When Groovy adopts the latest Java features, there’s always a twist to make things even groovier in Groovy!</li><li><a href="https://blog.scottlogic.com/2023/10/18/the-state-of-webassembly-2023.html">The State of WebAssembly in 2023</a><br>I often enjoy the articles from the folks at Scott Logic. This one is about a survey they ran on the topic of WebAssembly. Languages like Rust and JavaScript are seeing increased usage (for targeting wasm). Wasm is used a lot for web app development, but serverless seems to be he second most common use case, as well as for hosting plugin environments. The survey also mentions that threads, garbage collection and the new component model are the features developer are most interested in. For WASI, all the I/O related proposals like HTTP, filesystem support, sockets, are the ones developers want (although WASIX which covered this area received mixed reactions).</li><li><a href="https://arstechnica.com/information-technology/2023/09/telling-ai-model-to-take-a-deep-breath-causes-math-scores-to-soar-in-study/">Tell your LLM to take a deep breath!</a><br>We tend to humanize large language models via <a href="https://en.wikipedia.org/wiki/Anthropomorphism">anthropomorphism</a>, as much as we see human faces in anything like with <a href="https://en.wikipedia.org/wiki/Pareidolia">pareildolia</a>, although LLMs are neither sentients nor human. So it’s pretty ironic that to get a better result in some logic problem solving, we need to tell the LLM to actually take a deep breath! Are they now able to breathe?</li><li><a href="https://hackerone.com/reports/2199174">Wannabe security researcher asks Bard for vulnerabilities in cURL</a><br>Large Language Models can be super creative, that’s why we employ them to imagine new stories, create narratives, etc. And it seems wannabe security experts believe that what LLMs say is pure facts, probably what happened to this person that reported that they asked Bard to find a vulnerability in cURL! And Bard indeed managed to be creative enough to craft an hypothetical exploit, even explaining where a possible integer overflow could take place. Unfortunately, the generated exploit text contained many errors (wrong method signature, invented changelog, code that doesn’t compile, etc.)</li><li><a href="https://www.beren.io/2023-03-19-LLMs-confabulate-not-hallucinate/">LLMs confabulate, they don’t hallucinate</a><br>A few times, I’ve seen this mention on social networks about the fact we should say that LLM confabulate, instead of hallucinate. Confabulation is usually a brain disorder that makes people confidently tell things that may be true or not, in a convincing fashion (they don’t even know it’s false or a lie). Hallucination is more of a misinterpretation of the sensory input, like having the impression to see a pink elephant! The article linked above explains the rationale.</li><li>Greg Kamradt tweets about the <a href="https://twitter.com/GregKamradt/status/1711772496159252981">use cases for multimodal vision+text LLMs</a><br>You’d think that you could just get a model that describes a picture as a text, and then mix that description with other text snippets. But models that really fully understand both images and texts are way more powerful than this. In this tweet, Greg distinguishes different scenarios: description, interpretation, recommendation, convertion, extraction, assistance and evaluatation. For example, we could imagine transforming an architecture diagram into a proper Terraform YAML file, or a UI mockup into a snippet of code that builds that UI for real. You You could show a picture of a dish, and ask for its recipe!</li><li><a href="https://blog.jetbrains.com/blog/2023/10/16/ai-graphics-at-jetbrains-story/">The Story of AI Graphics at JetBrains</a><br>I’ve always loved generative and procedural art, both for games and indeed for art. I really enjoyed this article which is going through the story of how they are generating their nice splash screens and animations for the JetBrains family of products. Neural networks at play here!</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=11a70245017d" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Client-side consumption of a rate-limited API in Java]]></title>
            <link>https://glaforge.medium.com/client-side-consumption-of-a-rate-limited-api-in-java-9fbf08673791?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/9fbf08673791</guid>
            <category><![CDATA[rest]]></category>
            <category><![CDATA[api]]></category>
            <category><![CDATA[concurrency]]></category>
            <category><![CDATA[java]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Mon, 02 Oct 2023 00:00:53 GMT</pubDate>
            <atom:updated>2023-10-02T21:22:23.538Z</atom:updated>
            <content:encoded><![CDATA[<p>In the literature, you’ll easily find information on how to rate-limit your API. I even talked about <a href="https://speakerdeck.com/glaforge/the-never-ending-rest-api-design-debate-devoxx-france-2016?slide=80">Web API rate limitation</a> years ago at a conference, covering the usage of <a href="https://www.ietf.org/archive/id/draft-ietf-httpapi-ratelimit-headers-07.html">HTTP headers like X-RateLimit-*</a>.</p><p>Rate limiting is important to help your service cope with too much load, or also to implement a tiered pricing scheme (the more you pay, the more requests you’re allowed to make in a certain amount of time). There are useful libraries like <a href="https://micronaut-projects.github.io/micronaut-ratelimiter/snapshot/guide/index.html">Resilience4j</a> that you can configure for Micronaut web controllers, or <a href="https://www.baeldung.com/spring-bucket4j">Bucket4j</a> for your Spring controllers.</p><p>Oddly, what is harder to find is information about how to consume APIs that are rate-limited. Although there are usually way more consumers of rate-limited APIs, than producers of such APIs!</p><p>Today, I’d like to talk about this topic: how to consume APIs that are rate-limited. And since I’m a Java developer, I’ll focus on Java-based solutions.</p><p>The use case which led me to talk about this topic (on Twitter / X in particular, with a <a href="https://twitter.com/glaforge/status/1705933799635268050">fruitful conversation</a> with my followers) is actually about a Java API which is an SDK for a Web API that is rate-limited. I’ll briefly cover consuming Web APIs, but will focus more on using the Java API instead.</p><h3>Consuming Web APIs</h3><p>Let’s say we are calling a Web API that is rate-limited to 60 requests per minute. We could call it 60 times in a second without hitting the limit then wait for a minute, or call it once every second. Usually, a sliding time window is used to check that within that minute, no more than 60 requests have been made.</p><p>If the rate-limited API is well behaved and provides X-RateLimit headers, you can check what those headers say. Taking the explanations from the IETF <a href="https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/">draft</a>:</p><ul><li><strong>RateLimit-Limit</strong>: containing the requests quota in the time window;</li><li><strong>RateLimit-Remaining</strong>: containing the remaining requests quota in the current window;</li><li><strong>RateLimit-Reset</strong>: containing the time remaining in the current window, specified in seconds.</li></ul><p>Note that the draft mentions RateLimit-* as headers, but often in the wild, I’ve seen those headers always prefixed with “X-” instead. And sometimes, some APIs add a hyphen between Rate and Limit! So it’s hard to create a general consumer class that could deal with <a href="https://stackoverflow.com/questions/16022624/examples-of-http-api-rate-limiting-http-response-headers">all cases</a>.</p><p>Those headers inform you about the quota, how much is left, and when the quota should be back to its full capacity (if you don’t consume more requests). So you could certainly stage your requests accordingly — we will talk about how to schedule your requests in Java in the second section.</p><p>Another thing to keep in mind is that the quota may be shared among API consumers. Maybe you have several parallel threads that will call the API and consume the API quota. So when you see the reset header, maybe the API will have been called by another thread already, leaving you with a smaller amount of requests left in the quota.</p><h3>Exponential backoff and jitter</h3><p>The API that triggered my research actually doesn’t provide any rate limitation headers. So another approach is needed. A classical approach is to use an exponential backoff. It was nicely <a href="https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/">documented</a> on the AWS blog, a while ago.</p><p>The idea is that when you face an over quota error, you’re going to retry the call after, for example, one second. And if you’re getting another error, you’ll wait a little longer, by multiplying the interval by a constant, like doubling. So at first, on the first error, you wait 1 second before retrying, next 2 seconds, then 4 seconds, etc. You can use a fractional multiplier, of course.</p><p>But as explained in the article, if all clients fail at the same time, they will retry roughly at the same time as well, after one, two, four seconds. So the idea is to add some randomness, the jitter, to more evenly spread out the retries, to avoid having new bursts of traffic at roughly the same moments.</p><p>There’s another good article on Baeldung about <a href="https://www.baeldung.com/resilience4j-backoff-jitter">exponential backoff and jitter using Resilience4J</a> for your API consumers.</p><h3>Consuming a Java API</h3><p>Back to my use case, the underlying Web API I’m using doesn’t feature rate limitation headers. And since there’s a Java library that wraps that API anyway, I’d rather just use that Java API for convenience. When a rate limit is hit, the API will throw an exception. So I can catch that exception, and deal with it, maybe applying the same exponential backoff + jitter strategy.</p><p>However, I know the rate limit of the API. So instead of eagerly calling the API as fast as possible, getting an exception, waiting a bit, and trying again&amp;mldr; I’d rather just call the API at the pace I’m allowed to use it.</p><p>Let’s say I have a hypothetical API that takes a String as argument and returns a String:</p><pre>public  class  RateLimitedApi {<br>    public String call(String arg) {<br>        return arg.toUpperCase();<br>    }<br>}</pre><h3>Sleeping a bit…</h3><p>A first, but naive, idea would be to just add some pause after each call:</p><pre>for (int i = 0; i &lt; 20; i++) {<br>    api.call(&quot;abc&quot;);<br>    Thread.sleep(100);<br>}</pre><p>And instead of making the same call with the same argument, you could iterate over an array or list:</p><pre>for (String s : args) {<br>    api.call(s);<br>    Thread.sleep(100);<br>}</pre><p>Well, it works, but the API call takes some time as well, so you may have to adjust the sleep time accordingly, so it’s not really ideal. The call could also be longer than the actual wait time really needed between two invocations.</p><h3>Scheduled execution</h3><p>A better approach would be to use Java’s scheduled executors, with a few threads, in case of long API execution times that overlap.</p><pre>try (var scheduler = Executors.newScheduledThreadPool(4)) {<br>    var scheduledCalls = scheduler.scheduleAtFixedRate(<br>        () -&gt; api.call(&quot;abc&quot;),<br>        0, 100, TimeUnit.MILLISECONDS);<br>}</pre><p>Instead of calling the API with the same argument, how would you call it for a series of different values, but then stop the scheduler once we’re done with all the values? You could take advantage of some kind of queue (here a ConcurrentLinkedDeque) to pop the arguments one at a time. Once you’ve cleared all the elements of the queue, you shut down the scheduler altogether.</p><pre>var args = new ConcurrentLinkedDeque&lt;&gt;(<br>    List.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, ...&quot;x&quot;, &quot;y&quot;, &quot;z&quot;));<br><br>try (var scheduler = Executors.newScheduledThreadPool(4)) {<br>    scheduler.scheduleAtFixedRate(() -&gt; {<br>        if (!args.isEmpty()) {<br>                api.call(args.pop());<br>        } else {<br>                scheduler.shutdown();<br>        }<br>    }, 0, 100, TimeUnit.MILLISECONDS);<br>}</pre><h3>One more in the Bucket4J!</h3><p>In the introduction, I mentioned some great libraries like Resilience4J and Bucket4J. Let’s have a look at an approach using <a href="https://bucket4j.com/">Bucket4J</a>.</p><p>Scheduling is fine, to respect the rate limit, but you may perhaps want to get as many calls through as possible, while still respecting the rate. So a different approach is necessary.</p><p>Bucket4J is based on the <a href="https://en.wikipedia.org/wiki/Token_bucket">token bucket algorithm</a>. It offers a very rich and fine-grained set of rate limit definitions, if you want to allow bursts, or prefer a regular flow (like our schedule earlier). Be sure to check out the <a href="https://bucket4j.com/8.4.0/toc.html#quick-start-examples">documentation</a> for the details.</p><p>Let’s see how to define my limited consumption rate of 10 per second:</p><pre>var args = List.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, ...&quot;x&quot;, &quot;y&quot;, &quot;z&quot;);<br><br>var bucket = Bucket.builder()<br>    .addLimit(Bandwidth.simple(10, Duration.ofSeconds(1)))<br>    .build();<br><br>for (String arg : args) {<br>    bucket.asBlocking().consumeUninterruptibly(1);<br>    api.call(arg);<br>}</pre><p>It’s pretty explicit: I create a limit that corresponds to a bandwidth of 10 tokens per second. With this simple strategy, the bucket is refilled greedily: every 100ms a new token will be available again. But it’s also possible to configure it differently, to say that you want to allow another 10 calls once every second.</p><p>Then I have a simple for loop to iterate over the list of arguments I must pass to the API, but I introduce an instruction that blocks until a token is available — ie. that I have the right to call the API again while respecting the rate limit.</p><p>Also beware of API calls that take a lot of time, as here we’re using a blocking call that blocks the calling thread. So if API calls take longer than the time a new token is available in the bucket, you’ll end up calling the API much less frequently than the allowed rate limit.</p><p>However, with Bucket4J, the bucket can be used in a thread-safe manner, you can have several threads consuming from the same API, using the same shared bucket, or you can make parallel calls with a single consumer as well, to use the quota to its maximum.</p><p>Let’s use executors to parallelize our calls:</p><pre>try (ExecutorService executor = Executors.newFixedThreadPool(4)) {<br>    for (String arg : args) {<br>        bucket.asBlocking().consumeUninterruptibly(1);<br>        executor.submit(() -&gt; api.call(arg));<br>    }<br>}</pre><p>Be careful though, that doing so, your API calls are not necessarily following the exact same order as the order of the input collection. In my case, I didn’t care about the order of execution.</p><p>Last little tweak we could make since Java 21 was released recently, we could make use of virtual threads, instead of threads! So let’s push our example forward in the 21th century with this small change when creating our executor service:</p><pre>Executors.newVirtualThreadPerTaskExecutor()</pre><p>So far, we have only called the API without taking care of the returned result. We could update the examples above with an extra line to add the argument and result in a ConcurrentHashMap or to use the result immediately. Or we could also explore one last solution, using CompletableFutures and/or ExecutorCompletionService. But I’m not 100% satisfied with what I came up with so far. So I might update this article if I find a convenient and elegant solution later on.</p><p>Time to wrap up!</p><h3>Summary</h3><p>In this article, we explored the less covered topic of consuming a rate-limited API. First, we discussed approaches for consuming Web APIs that are well-behaved, exposing rate limitation headers, and those less well-behaved using an exponential backoff and jitter approach. We then moved on to the case of Java APIs, doing a simple sleep to call the API at a cadence that respects the rate limit. We also had a look at scheduled executions. And we finished our journey with the help of the powerful Bucket4J library.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/10/02/client-side-consumption-of-a-rate-limited-api-in-java/"><em>https://glaforge.dev</em></a><em> on October 2, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fbf08673791" width="1" height="1" alt="">]]></content:encoded>
        </item>
    </channel>
</rss>