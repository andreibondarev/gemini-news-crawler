# [NewsFiler v2] NewsPaper: Google Cloud - Medium (this should go in the Entries as of v2)
# [NewsFiler v2] GUID: https://medium.com/p/7b40189a3c81
# [NewsFiler v2] entries.keys: ["title", "url", "author", "categories", "published", "entry_id", "content"]
--- !ruby/object:Feedjira::Parser::RSSEntry
title: Design your Landing Zone — Design Considerations Part 3 — Monitoring, Logging,
  Billing and…
url: https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-3-monitoring-logging-billing-and-7b40189a3c81?source=rss----e52cf94d98af---4
author: Dazbo (Darren Lester)
categories:
- labelling
- gcp-security-operations
- logging-and-monitoring
- google-cloud-platform
- landingzone
published: 2024-03-28 02:45:19.000000000 Z
entry_id: !ruby/object:Feedjira::Parser::GloballyUniqueIdentifier
  is_perma_link: 'false'
  guid: https://medium.com/p/7b40189a3c81
carlessian_info:
  news_filer_version: 2
  newspaper: Google Cloud - Medium
  macro_region: Blogs
rss_fields:
- title
- url
- author
- categories
- published
- entry_id
- content
content: "<h3>Design your Landing Zone — Design Considerations Part 3 — Monitoring,
  Logging, Billing and Labelling (Google Cloud Adoption Series)</h3><p>Welcome to
  Part 3 of Landing Zone Design Considerations. This is part of the <a href=\"https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc\">Google
  Cloud Adoption and Migration: From Strategy to Operation</a> series.</p><p>In this
  part I’ll cover:</p><ul><li><strong>Monitoring strategy</strong></li><li><strong>Logging
  strategy</strong></li><li><strong>Billing management and billing exports, and labelling</strong></li></ul><h3>9.
  Monitoring Strategy</h3><p><strong>Monitoring is the process of collecting, processing,
  aggregating and displaying real time and historical quantitative data about a system.</strong></p><h4>Metrics</h4><p>Google
  Cloud provides out-of-the-box monitoring, through the <a href=\"https://cloud.google.com/monitoring/docs\">Cloud
  Monitoring</a> component (formerly known as Stackdriver) of Google Cloud Operations
  (GCO) Suite. Cloud Monitoring automatically ingests over 1500 different metrics
  from over 100 different Google Cloud resources. There is <strong>no cost</strong>
  for ingestion of these metrics. For example:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*z2cAyus-9hJwGMOI\"
  /><figcaption>Cloud Monitoring ingests metrics from Google Cloud services and applications</figcaption></figure><p>In
  addition, we can ingest metrics such as:</p><ul><li><strong>Custom metrics</strong>,
  where we programmatically create custom telemetry using, e.g. with the Cloud Monitoring
  API, with OpenCensus, and (for GKE) with Prometheus.</li><li><strong>Log-based metrics</strong>,
  i.e. where real-time information is derived from logs.</li><li>Additional VM process
  metrics, via the <strong>Cloud Ops Agent</strong>.</li><li>Out-of-the-box <strong>application
  metrics</strong>, such as nginx, Apache web server, MongoDB, Tomcat.</li><li><strong>GKE
  workloads</strong>, using natively-integrated <strong>Prometheus and OpenTelemetry.</strong></li><li><strong>Hybrid
  cloud monitoring</strong> — e.g. monitoring signals from on-prem, AWS and Azure — using
  Blue Medora BindPlane.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/876/1*Z13YCBUiR0Dwvk26zdWbdw.png\"
  /><figcaption>Types of metrics collected by Google Cloud Monitoring</figcaption></figure><p>Be
  mindful that certain metrics — such as custom metrics and logs-based metrics — have
  an ingestion cost. So for such metrics, only ingest what is truly valuable.</p><h4>Visualisation
  and Analysis</h4><p>Of course, it’s not much use collecting metrics if you don’t
  do anything with this data. Cloud Monitoring allows you to:</p><ul><li>Create charts</li><li>Use
  predefined and custom dashboards</li><li>Share charts and dashboards</li></ul><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/947/0*nYnr6VuP1KsB6enp\" /><figcaption>Custom
  dashboards are assembled from charts</figcaption></figure><ul><li>Create healthchecks</li><li>Define
  services and <a href=\"https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b\">SLOs</a></li><li>Create
  <a href=\"https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b\">alerting policies</a></li></ul><p>So
  the end-to-end consumption and use of metrics looks something like this:</p><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*RXYR2m78q2tVDiOq\" /><figcaption>Making
  use of your metrics in Google Cloud Monitoring</figcaption></figure><h4>Metrics
  Scope Strategy</h4><p>Google Cloud Monitoring defines an object called a <strong>metrics
  scope</strong> (formerly called a workspace). Such a scope defines the set of Google
  Cloud projects who metrics are visible from a <em>“single pane of glass”</em>. Each
  scope contains:</p><ul><li>Predefined and custom dashboards</li><li>Alerting policies</li><li>Uptime
  checks</li><li>Notification channels</li><li>(Resource) group definitions</li></ul><p>Every
  project has its own metrics scope, and — by default — this metrics scope only has
  visibility of the resources in <em>that </em>project. However, we can extend this
  scope to include the metrics from other projects. And <strong>this is how we can
  centralise and aggregate monitoring across projects</strong>.</p><figure><img alt=\"\"
  src=\"https://cdn-images-1.medium.com/max/953/1*GZkybWPHuPjzDJsWo794TQ.png\" /><figcaption>Using
  a single metrics scope to monitor production projects</figcaption></figure><p>And
  consequently, it is important to design your metrics scope strategy. Broadly, you
  have three options:</p><ol><li><strong>One metrics scope for many related projects,
  across different environments</strong>. Here we have a single pane of glass for
  all related projects. However, anyone with the monitoring.viewer IAM role for the
  monitoring project can see all metrics for all of the projects. Thus, we have no
  granular control over monitoring.</li><li><strong>Maximum isolation</strong> — every
  project is monitored by a separate scope. Viewers in one project can’t (by default)
  see any metrics from another. This provides the highest degree of separation, but
  also potentially the higest operational management overheads.</li><li><strong>In-between</strong> — i.e.
  one scope for a small number of related projects. For example, we might have different
  scopes for production versus non-production. We do need to think about our project
  groupings.</li></ol><h4>Decision Points</h4><p>Here’s a recap of some of the key
  decisions that need to be made</p><ul><li><strong>Where is your monitoring single
  pane of glass? </strong>Where possible, I’d recommend using only GCO Cloud Monitoring
  for the monitoring of Google Cloud infrastructure, services and applications. I.e.
  align to the “Cloud native / Cloud managed” principle. However, in hybrid environments,
  organisations may want to make use of existing external monitoring tools, such as
  New Relic. Be advised that this can considerably increase monitoring costs, e.g.
  through licensing of the third party monitoring tool, as well as egress costs.</li><li><strong>Organisational
  granularity and visibility of monitoring</strong>? SRE approach? Metrics scope strategy?
  Here we should consider our <a href=\"https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b\">organisational
  SRE model</a>. And we should decide on the appropriate level of metrics scope granularity.</li><li><strong>Establish
  your organisational best practices</strong> <strong>for what will be monitored</strong>,
  e.g. golden signals (latency, traffic, errors, saturation), SLIs, SLOs, and alerting
  policies, defined at application level. You will want to make this easily consumable
  and repeatable.</li><li><strong>Decide which chargeable metrics you want and need.</strong></li><li><strong>Establish
  your alerting channels and best practices.</strong> E.g. define what sorts of alerts
  are urgent and require human interaction, and will therefore generate pages. Other
  alerts should generate tickets, and use alternative notification approaches.</li></ul><h3>10.
  Logging Strategy</h3><p>Also part of the Google Cloud Operations suite, <a href=\"https://cloud.google.com/logging/docs/overview\"><strong>Cloud
  Logging</strong></a><strong> is a fully-managed serverless service for the ingestion,
  storing, viewing, searching and analysis of logs.</strong> Cloud Logging components
  are exposed through the Logging API.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/887/1*FhJ1xCaL1dwx3u62_AUCyQ.png\"
  /><figcaption>Google Cloud Logging Capabilities</figcaption></figure><p>Logs are
  ingested from various sources, such as Google Cloud resources, GKE, third party
  applications, and user applications runing on Google Cloud.</p><p>The overall Cloud
  Logging architecture looks like this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*p9T72CmrW-fwRJGS\"
  /><figcaption>Google Cloud Logging architecture</figcaption></figure><h4>Log Routing
  and Sinks</h4><p>The <strong>Log Router</strong> is responsible for ensuring that
  logs are reliably and efficiently streamed to logging destinations, called <strong>Log
  Sinks</strong>. A log sink is actually the combination of a destination, along with
  inclusion and exclusion filters:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*5YgTa_Bo_gtxyIDz\"
  /><figcaption>A Log Sink</figcaption></figure><p>Possible sinks include:</p><ul><li><strong>Cloud
  Logging buckets</strong> — special storage buckets optimised for storing logging
  data, and which can be interrogated directly from the Logs Explorer in the Google
  Cloud Console.</li><li><strong>Google Cloud Storage</strong> — ideal for cheap long-term
  and automated log archiving.</li><li><strong>BigQuery </strong>— which is ideal
  for long-term storage, SQL-based analytics, and dashboarding.</li><li><strong>Pub/Sub
  topics</strong> — perfect for sending log data to downstream systems, such as on-prem
  Splunk. But also, we can use this to stream logs via Dataflow. This can be useful
  if we want to process the logging data on the fly.</li></ul><figure><img alt=\"\"
  src=\"https://cdn-images-1.medium.com/max/1024/0*WV4rFpxQFqh5aGR9\" /><figcaption>Example:
  Real-time processing of streaming log data</figcaption></figure><p>Two predefined
  sinks are created in each Google Cloud project:</p><ul><li><strong>_Required </strong>— where
  all admin, system, and access transparency <strong>audit logs</strong> go. These
  logs are retained for 400 days. The _Required log bucket cannot be modified or deleted.</li><li><strong>_Default
  </strong>—all logs go here, unless they are are sent to _Required or user-defined
  sinks. By default, retention is 30 days, but it can be increased to 3650 days (10
  years). The _Default bucket cannot be deleted, but the sink can be disabled.</li></ul><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*dUjAw5dmLhwyPJ8s\" /><figcaption>Routing
  to log sinks</figcaption></figure><p>Note that it is possible export logs using
  a sink, whilst also EXCLUDING logs from being ingested in Cloud Logging buckets.
  This is very useful for managing logging costs.</p><p><strong>We can aggregate logging
  to folder, or even organisational sinks</strong>. This logging aggregation works
  by including logs from child resources. We do this by creating an aggregated sink
  at folder or org level, and setting the includeChildren parameter to True. Then
  we select the destination for the sink, just like any other sink.</p><p>(When doing
  this, it is wise to add an exclusion filter to the _Defaultsink, so that logs are
  not retained in both sinks.)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*6m9WRUCCmclDayVj\"
  /><figcaption>Logging aggregation</figcaption></figure><p>It is a good idea to aggregate
  logs from across the organisation into a few logging buckets. Consider subdividing
  these buckets based on properties such as:</p><ul><li>Retention period</li><li>User
  access requirements</li><li>Data residency requirements</li><li>Where you may need
  to use customer-managed encryption keys.</li></ul><p>We can then use Log views and
  IAM bindings to determine who can see the logs.</p><h4>Audit Logs</h4><p>Most Google
  Cloud services create audit logs. <strong>They are intended to answer the question
  of: <em>“Who did what, where, and when?” </em></strong>These are incredibly important
  for compliance in regulated industries.</p><p>There are five categories of audit logs:</p><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/789/1*Q9VSfFfNwSbVHJNacC0UBA.png\"
  /><figcaption>Google Cloud audit logs</figcaption></figure><ul><li>The <strong>Admin
  logs</strong> are always enabled, retained for 400 days, and have no charge. They
  record any API calls that modify the configuration of resources, such as creating
  networks, creating or modifying instances, etc.</li><li>The <strong>System Event</strong>
  logs are always enabled, retained for 400 days, and have no charge. They record
  activities that modify the configuration of resources, but which occur due to Google-initiated
  events, rather than direct user interactions. For example, live migrations.</li><li>The
  <strong>Data Access logs</strong> are disabled by default, and chargeable. They
  are useful for identifying costly data queries.</li><li>The <strong>Policy Denied
  logs</strong> are enabled by default, but can be disabled. They are chargeable.</li><li><strong>Access
  transparency logs</strong> give visibility of any actions performed by Googel staff.
  For example, as part of an open incident with Google.</li></ul><h4>Network Logs</h4><p><strong>VPC
  flow logs allow you to record samples of network flows sent or received by VM instances,
  including GKE nodes. </strong>These logs are useful for diagnosing network issues,
  network and security analytics, and forensics. They are disabled by default, but
  can be enabled on a per-VPC subnet basis.</p><p>Logging volumes can be significant,
  so it’s important to optimise the aggregation interval and sampling rate.</p><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/655/0*2EfSemOhFN9WD1Ru\" /><figcaption>Configuring
  VPC Flow logs</figcaption></figure><p><strong>Cloud Firewall Rules Logging</strong>
  can be used to answer questions like:</p><blockquote>“How many connections match
  this rule?”<br>“Did this rule cause my application outage?”<br>“Is this rule incorrectly
  stopping traffic?”</blockquote><p>Like VPC Flow logs, Firewall Rule Logging is disabled
  by default. It can be enabled on a per-rule basis. E.g.</p><pre>gcloud compute firewall-rules
  update &lt;rule-name&gt; --enable-logging</pre><p>And as with VPC Flow logs, this
  can result in a lot of logging, and it can get expensive quickly. A common strategy
  is to NOT enable rules logging by default, but to only turn on rules logging when
  diagnosing issues. And you can always start by creating a <em>low priority</em>
  allow-all rule, to verify that the traffic is hitting the firewall in the first place.</p><p>Also
  worth mentioning: we can use <strong>Cloud NAT logs</strong> to capture connections
  created and packets dropped by Cloud NAT.</p><h4>Summary of Cloud Logging Decision Points</h4><ul><li><strong>Will
  you use Google Cloud Logging only, or do you need to export logs to downstream systems,
  like Splunk?</strong> Some organisations use Splunk for SIEM, so you may have a
  requirement to export audit logs to Splunk. But be careful not to do this for ALL
  logs, since Splunk is extremely expensive when working with large volumes of logs.</li><li>What
  are log <strong>compliance, data residency, and retention requirements</strong>?</li><li><strong>Which
  logs will you exclude?</strong></li><li>Will you <strong>enable VPC flow logs?</strong>
  If so, what sampling rate?</li><li>Will you <strong>archive logs</strong> for long-term
  storage? Will you use GCS lifecycle management to automate this?</li><li>Will you
  <strong>export logs to BigQuery</strong> for analytics?</li><li>Which <strong>aggregated
  log sinks</strong> will you define?</li><li><strong>Who will be given access to
  logs?</strong> Who will be given access to aggregated logs?</li><li><strong>When
  writing logs from applications, what standards will you follow?</strong> A good
  practice is to write structured JSON log entries, since these can then be easily
  parsed and filtered.</li><li>Will we implement a mechanism to dynamically enable
  logging for troubleshooting?</li></ul><h3>11. Billing Management, Billing Exports,
  and Labelling</h3><p>One of the big advantages of cloud is that it makes your costs
  visible and transparent. It’s easy to see how much you’re paying, what you’re paying
  for, and who’s using the resources that you’re paying for. But to do this effectively,
  a bit of planning is required.</p><p><strong>Google Cloud accumulates costs at the
  project level</strong>. I.e. every resource belongs to a project. Furthermore, <strong>projects
  are associated with one and only one billing account</strong>. This is how your
  organisation actually gets billed by Google.</p><p>A project <em>must </em>be associated
  with a billing account, in order to consume any chargeable services. A billing account
  can be associated with many projects. Some organisations — typically Google resellers — will
  have billing sub-accounts, which can be used to aggregate billing for specific clients.</p><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*o8wlVb62rCLnk4ok.png\"
  /><figcaption>Billing hierarchy</figcaption></figure><h4>Some Roles to Care About</h4><p>The
  <strong>Billing Account Admin</strong> role is typically given to individuals with
  financial responsibility. They will be able to:</p><ul><li>Link and unlink billing
  accounts to projects.</li><li>Enable billing exports.</li><li>View spend and costs.</li><li>Set
  budgets and alerts.</li></ul><p>The <strong>Billing Account Viewer</strong> can
  view billing accounts, but can’t make any changes.</p><p>The <strong>Project Billing
  Manager</strong> can assign a billing account to a project, and disable billing
  of a project. (Though you’d normally automate this.)</p><h4>Billing Visibility</h4><p>The
  Google Cloud Console includes built-in Billing Reports:</p><figure><img alt=\"\"
  src=\"https://cdn-images-1.medium.com/max/1024/0*jtzMGJZGGeNTD-ar\" /><figcaption>Billing
  in the Google Console</figcaption></figure><p>This information can be filtered based
  on properties such as project, date, and products.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/577/0*stN9KLwKvRD0ems6\"
  /></figure><p>A really useful feature is the ability to view cost trends:</p><figure><img
  alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*bxkRUqpZ1APVqmkj\" /><figcaption>Cost
  trends in the Billing Report</figcaption></figure><p>We can also export this billing
  data into BigQuery, which allows for much more sophisticated analysis. Not only
  can we then query the data using SQL, but we can then easily visualise and analyse
  the data using tools like Google Looker Studio.</p><h4>Labelling</h4><p>Labelling
  (not to be confused with network tags) is incredibly important!! They are key-value
  pairs which we can assign to any Google Cloud project or resource.</p><p>We can
  then use these labels to help us analyse our consumption and billing data. For example,
  we can run something like this in BigQuery:</p><pre>SELECT <br>  project.labels.key,
  <br>  project.labels.value, <br>  sum(cost) as cost_total, <br>  sum(usage.amount)
  as usage_total<br>FROM billing_table<br>WHERE <br>  Start_Time &gt;= &#39;2024-02-01
  00:00:00&#39; AND Start_Time &lt; &#39;2024-03-01 00:00:00&#39;<br>GROUP BY <br>
  \ project.labels.key, <br>  project.labels.value</pre><p>Consequently, it’s a good
  idea to come up with a labelling strategy and label naming standards. Wherever possible,
  we should set labels in an automated fashion, when we create resources using infrastructure-as-code.</p><p>We
  can define any labels we want. But here are some suggestions:</p><ul><li>Team /
  cost centre — e.g. “team:research”</li><li>Environment — e.g. “environment:staging”</li><li>Component — e.g.
  “component:fe”</li><li>State — e.g. “state:pending-deletion”</li><li>Shared resource — e.g.
  “shared:true”. This can be useful for identifying projects that are used by many
  tenants, and for subsequently attributing costs.</li></ul><h4>Key Design Considerations</h4><ul><li><strong>What
  organisational changes do you need to make</strong>, in order to take advantage
  of consumption-based (pay-as-you-go) resources, and to drive cost-savvy decisions?</li><li><strong>How
  many billing accounts do we need?</strong></li><li><strong>Who will be given the
  Billing Administrator role? </strong>(Note that Org Admins do not have Billing Admin
  by default, but they can grant this role to others, and to themselves.)</li><li><strong>Who
  will be Billing Viewers?</strong></li><li><strong>How will projects be linked with
  billing accounts? Using IaC?</strong></li><li>Will we <strong>export billing data
  to BigQuery?</strong></li><li><strong>What is our labelling strategy? What are our
  label naming standards?</strong></li></ul><p>By the way, I’ve intentionally steered
  clear of FinOps. I’ll be covering that in a later article in this series.</p><h3>Wrap-Up</h3><p>Here
  we’ve covered the key LZ design considerations for monitoring, logging, and billing.
  In the next part, we’ll cover the last major LZ design consideration topic: infra-as-code
  (IaC) and GitOps.</p><h3>Before You Go</h3><ul><li><strong>Please share</strong>
  this with anyone that you think will be interested. It might help them, and it really
  helps me!</li><li>Feel free to <strong>leave a comment</strong> \U0001F4AC.</li><li><strong>Follow</strong>
  and <strong>subscribe, </strong>so you don’t miss my content. Go to my <a href=\"https://medium.com/@derailed.dash\">Profile
  Page</a>, and click on these icons:</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/163/0*fF62z2-FT03ui0O5.png\"
  /><figcaption>Follow and Subscribe</figcaption></figure><h3>Links</h3><ul><li><a
  href=\"https://medium.com/google-cloud/landing-zones-on-google-cloud-b42b08e1abaa\">Landing
  Zones on Google Cloud: What It Is, Why You Need One, and How to Create One</a></li><li><a
  href=\"https://cloud.google.com/architecture/landing-zones\">Landing zone design
  in Google Cloud</a></li><li><a href=\"https://medium.com/@derailed.dash/google-cloud-adoption-site-reliability-engineering-sre-and-best-practices-for-sli-slo-sla-6670c864c96b\">Google
  Cloud Adoption: SRE and Best Practices for SLI / SLO / SLA</a></li><li><a href=\"https://cloud.google.com/stackdriver/docs\">Observability
  in Google Cloud</a></li><li><a href=\"https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging\">Setup
  monitoring, alerting and logging</a></li><li><a href=\"https://cloud.google.com/monitoring/docs\">Google
  Cloud Monitoring</a></li><li><a href=\"https://cloud.google.com/blog/products/management-tools/use-bluemedoras-bindplane-with-google-cloud\">Blue
  Medora BindPlane</a></li><li><a href=\"https://cloud.google.com/logging/docs/overview\">Google
  Cloud Logging</a></li><li><a href=\"https://cloud.google.com/billing/docs/concepts\">Cloud
  Billing Overview</a></li><li><a href=\"https://www.google.com/url?q=https://cloud.google.com/billing/docs/how-to/export-data-bigquery&amp;sa=D&amp;source=docs&amp;ust=1711378716658242&amp;usg=AOvVaw0qPvb7L1IfJwwK_2In8wjl\">Export
  Cloud Billing Data to BigQuery</a></li><li><a href=\"https://cloud.google.com/billing/docs/how-to/bq-examples\">Example
  queries for Cloud Billing data export</a></li><li><a href=\"https://cloud.google.com/architecture/framework\">Google
  Cloud Architecture Framework</a></li><li><a href=\"https://cloud.google.com/architecture/security-foundations\">Enterprise
  Foundations Blueprint</a></li></ul><h3>Series Navigation</h3><ul><li><a href=\"https://medium.com/google-cloud/google-cloud-adoption-for-the-enterprise-from-strategy-to-operation-part-0-overview-9091f5a1ddfc\">Series
  overview and structure</a></li><li>Previous: <a href=\"https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-2-kubernetes-and-gke-google-cloud-5a500384cb03\">Design
  your Landing Zone — Design Considerations Part 2: Kubernetes</a></li><li>Next: Design
  your Landing Zone — Design Considerations Part 4: IaC, GitOps and CI/CD</li></ul><img
  src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7b40189a3c81\"
  width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/google-cloud/design-your-landing-zone-design-considerations-part-3-monitoring-logging-billing-and-7b40189a3c81\">Design
  your Landing Zone — Design Considerations Part 3 — Monitoring, Logging, Billing
  and…</a> was originally published in <a href=\"https://medium.com/google-cloud\">Google
  Cloud - Community</a> on Medium, where people are continuing the conversation by
  highlighting and responding to this story.</p>"
