# [NewsFiler v2] NewsPaper: Guillaume Laforge - Medium (this should go in the Entries as of v2)
# [NewsFiler v2] GUID: https://medium.com/p/9fbf08673791
# [NewsFiler v2] entries.keys: ["title", "url", "author", "categories", "published", "entry_id", "content"]
--- !ruby/object:Feedjira::Parser::RSSEntry
title: Client-side consumption of a rate-limited API in Java
url: https://glaforge.medium.com/client-side-consumption-of-a-rate-limited-api-in-java-9fbf08673791?source=rss-431147437aeb------2
author: Guillaume Laforge
categories:
- rest
- api
- concurrency
- java
published: 2023-10-02 00:00:53.000000000 Z
entry_id: !ruby/object:Feedjira::Parser::GloballyUniqueIdentifier
  is_perma_link: 'false'
  guid: https://medium.com/p/9fbf08673791
carlessian_info:
  news_filer_version: 2
  newspaper: Guillaume Laforge - Medium
  macro_region: Blogs
rss_fields:
- title
- url
- author
- categories
- published
- entry_id
- content
content: '<p>In the literature, you’ll easily find information on how to rate-limit
  your API. I even talked about <a href="https://speakerdeck.com/glaforge/the-never-ending-rest-api-design-debate-devoxx-france-2016?slide=80">Web
  API rate limitation</a> years ago at a conference, covering the usage of <a href="https://www.ietf.org/archive/id/draft-ietf-httpapi-ratelimit-headers-07.html">HTTP
  headers like X-RateLimit-*</a>.</p><p>Rate limiting is important to help your service
  cope with too much load, or also to implement a tiered pricing scheme (the more
  you pay, the more requests you’re allowed to make in a certain amount of time).
  There are useful libraries like <a href="https://micronaut-projects.github.io/micronaut-ratelimiter/snapshot/guide/index.html">Resilience4j</a>
  that you can configure for Micronaut web controllers, or <a href="https://www.baeldung.com/spring-bucket4j">Bucket4j</a>
  for your Spring controllers.</p><p>Oddly, what is harder to find is information
  about how to consume APIs that are rate-limited. Although there are usually way
  more consumers of rate-limited APIs, than producers of such APIs!</p><p>Today, I’d
  like to talk about this topic: how to consume APIs that are rate-limited. And since
  I’m a Java developer, I’ll focus on Java-based solutions.</p><p>The use case which
  led me to talk about this topic (on Twitter / X in particular, with a <a href="https://twitter.com/glaforge/status/1705933799635268050">fruitful
  conversation</a> with my followers) is actually about a Java API which is an SDK
  for a Web API that is rate-limited. I’ll briefly cover consuming Web APIs, but will
  focus more on using the Java API instead.</p><h3>Consuming Web APIs</h3><p>Let’s
  say we are calling a Web API that is rate-limited to 60 requests per minute. We
  could call it 60 times in a second without hitting the limit then wait for a minute,
  or call it once every second. Usually, a sliding time window is used to check that
  within that minute, no more than 60 requests have been made.</p><p>If the rate-limited
  API is well behaved and provides X-RateLimit headers, you can check what those headers
  say. Taking the explanations from the IETF <a href="https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/">draft</a>:</p><ul><li><strong>RateLimit-Limit</strong>:
  containing the requests quota in the time window;</li><li><strong>RateLimit-Remaining</strong>:
  containing the remaining requests quota in the current window;</li><li><strong>RateLimit-Reset</strong>:
  containing the time remaining in the current window, specified in seconds.</li></ul><p>Note
  that the draft mentions RateLimit-* as headers, but often in the wild, I’ve seen
  those headers always prefixed with “X-” instead. And sometimes, some APIs add a
  hyphen between Rate and Limit! So it’s hard to create a general consumer class that
  could deal with <a href="https://stackoverflow.com/questions/16022624/examples-of-http-api-rate-limiting-http-response-headers">all cases</a>.</p><p>Those
  headers inform you about the quota, how much is left, and when the quota should
  be back to its full capacity (if you don’t consume more requests). So you could
  certainly stage your requests accordingly — we will talk about how to schedule your
  requests in Java in the second section.</p><p>Another thing to keep in mind is that
  the quota may be shared among API consumers. Maybe you have several parallel threads
  that will call the API and consume the API quota. So when you see the reset header,
  maybe the API will have been called by another thread already, leaving you with
  a smaller amount of requests left in the quota.</p><h3>Exponential backoff and jitter</h3><p>The
  API that triggered my research actually doesn’t provide any rate limitation headers.
  So another approach is needed. A classical approach is to use an exponential backoff.
  It was nicely <a href="https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/">documented</a>
  on the AWS blog, a while ago.</p><p>The idea is that when you face an over quota
  error, you’re going to retry the call after, for example, one second. And if you’re
  getting another error, you’ll wait a little longer, by multiplying the interval
  by a constant, like doubling. So at first, on the first error, you wait 1 second
  before retrying, next 2 seconds, then 4 seconds, etc. You can use a fractional multiplier,
  of course.</p><p>But as explained in the article, if all clients fail at the same
  time, they will retry roughly at the same time as well, after one, two, four seconds.
  So the idea is to add some randomness, the jitter, to more evenly spread out the
  retries, to avoid having new bursts of traffic at roughly the same moments.</p><p>There’s
  another good article on Baeldung about <a href="https://www.baeldung.com/resilience4j-backoff-jitter">exponential
  backoff and jitter using Resilience4J</a> for your API consumers.</p><h3>Consuming
  a Java API</h3><p>Back to my use case, the underlying Web API I’m using doesn’t
  feature rate limitation headers. And since there’s a Java library that wraps that
  API anyway, I’d rather just use that Java API for convenience. When a rate limit
  is hit, the API will throw an exception. So I can catch that exception, and deal
  with it, maybe applying the same exponential backoff + jitter strategy.</p><p>However,
  I know the rate limit of the API. So instead of eagerly calling the API as fast
  as possible, getting an exception, waiting a bit, and trying again&amp;mldr; I’d
  rather just call the API at the pace I’m allowed to use it.</p><p>Let’s say I have
  a hypothetical API that takes a String as argument and returns a String:</p><pre>public  class  RateLimitedApi
  {<br>    public String call(String arg) {<br>        return arg.toUpperCase();<br>    }<br>}</pre><h3>Sleeping
  a bit…</h3><p>A first, but naive, idea would be to just add some pause after each call:</p><pre>for
  (int i = 0; i &lt; 20; i++) {<br>    api.call(&quot;abc&quot;);<br>    Thread.sleep(100);<br>}</pre><p>And
  instead of making the same call with the same argument, you could iterate over an
  array or list:</p><pre>for (String s : args) {<br>    api.call(s);<br>    Thread.sleep(100);<br>}</pre><p>Well,
  it works, but the API call takes some time as well, so you may have to adjust the
  sleep time accordingly, so it’s not really ideal. The call could also be longer
  than the actual wait time really needed between two invocations.</p><h3>Scheduled
  execution</h3><p>A better approach would be to use Java’s scheduled executors, with
  a few threads, in case of long API execution times that overlap.</p><pre>try (var
  scheduler = Executors.newScheduledThreadPool(4)) {<br>    var scheduledCalls = scheduler.scheduleAtFixedRate(<br>        ()
  -&gt; api.call(&quot;abc&quot;),<br>        0, 100, TimeUnit.MILLISECONDS);<br>}</pre><p>Instead
  of calling the API with the same argument, how would you call it for a series of
  different values, but then stop the scheduler once we’re done with all the values?
  You could take advantage of some kind of queue (here a ConcurrentLinkedDeque) to
  pop the arguments one at a time. Once you’ve cleared all the elements of the queue,
  you shut down the scheduler altogether.</p><pre>var args = new ConcurrentLinkedDeque&lt;&gt;(<br>    List.of(&quot;a&quot;,
  &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;,
  &quot;h&quot;, ...&quot;x&quot;, &quot;y&quot;, &quot;z&quot;));<br><br>try (var
  scheduler = Executors.newScheduledThreadPool(4)) {<br>    scheduler.scheduleAtFixedRate(()
  -&gt; {<br>        if (!args.isEmpty()) {<br>                api.call(args.pop());<br>        }
  else {<br>                scheduler.shutdown();<br>        }<br>    }, 0, 100, TimeUnit.MILLISECONDS);<br>}</pre><h3>One
  more in the Bucket4J!</h3><p>In the introduction, I mentioned some great libraries
  like Resilience4J and Bucket4J. Let’s have a look at an approach using <a href="https://bucket4j.com/">Bucket4J</a>.</p><p>Scheduling
  is fine, to respect the rate limit, but you may perhaps want to get as many calls
  through as possible, while still respecting the rate. So a different approach is
  necessary.</p><p>Bucket4J is based on the <a href="https://en.wikipedia.org/wiki/Token_bucket">token
  bucket algorithm</a>. It offers a very rich and fine-grained set of rate limit definitions,
  if you want to allow bursts, or prefer a regular flow (like our schedule earlier).
  Be sure to check out the <a href="https://bucket4j.com/8.4.0/toc.html#quick-start-examples">documentation</a>
  for the details.</p><p>Let’s see how to define my limited consumption rate of 10
  per second:</p><pre>var args = List.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;,
  &quot;d&quot;, &quot;e&quot;, ...&quot;x&quot;, &quot;y&quot;, &quot;z&quot;);<br><br>var
  bucket = Bucket.builder()<br>    .addLimit(Bandwidth.simple(10, Duration.ofSeconds(1)))<br>    .build();<br><br>for
  (String arg : args) {<br>    bucket.asBlocking().consumeUninterruptibly(1);<br>    api.call(arg);<br>}</pre><p>It’s
  pretty explicit: I create a limit that corresponds to a bandwidth of 10 tokens per
  second. With this simple strategy, the bucket is refilled greedily: every 100ms
  a new token will be available again. But it’s also possible to configure it differently,
  to say that you want to allow another 10 calls once every second.</p><p>Then I have
  a simple for loop to iterate over the list of arguments I must pass to the API,
  but I introduce an instruction that blocks until a token is available — ie. that
  I have the right to call the API again while respecting the rate limit.</p><p>Also
  beware of API calls that take a lot of time, as here we’re using a blocking call
  that blocks the calling thread. So if API calls take longer than the time a new
  token is available in the bucket, you’ll end up calling the API much less frequently
  than the allowed rate limit.</p><p>However, with Bucket4J, the bucket can be used
  in a thread-safe manner, you can have several threads consuming from the same API,
  using the same shared bucket, or you can make parallel calls with a single consumer
  as well, to use the quota to its maximum.</p><p>Let’s use executors to parallelize
  our calls:</p><pre>try (ExecutorService executor = Executors.newFixedThreadPool(4))
  {<br>    for (String arg : args) {<br>        bucket.asBlocking().consumeUninterruptibly(1);<br>        executor.submit(()
  -&gt; api.call(arg));<br>    }<br>}</pre><p>Be careful though, that doing so, your
  API calls are not necessarily following the exact same order as the order of the
  input collection. In my case, I didn’t care about the order of execution.</p><p>Last
  little tweak we could make since Java 21 was released recently, we could make use
  of virtual threads, instead of threads! So let’s push our example forward in the
  21th century with this small change when creating our executor service:</p><pre>Executors.newVirtualThreadPerTaskExecutor()</pre><p>So
  far, we have only called the API without taking care of the returned result. We
  could update the examples above with an extra line to add the argument and result
  in a ConcurrentHashMap or to use the result immediately. Or we could also explore
  one last solution, using CompletableFutures and/or ExecutorCompletionService. But
  I’m not 100% satisfied with what I came up with so far. So I might update this article
  if I find a convenient and elegant solution later on.</p><p>Time to wrap up!</p><h3>Summary</h3><p>In
  this article, we explored the less covered topic of consuming a rate-limited API.
  First, we discussed approaches for consuming Web APIs that are well-behaved, exposing
  rate limitation headers, and those less well-behaved using an exponential backoff
  and jitter approach. We then moved on to the case of Java APIs, doing a simple sleep
  to call the API at a cadence that respects the rate limit. We also had a look at
  scheduled executions. And we finished our journey with the help of the powerful
  Bucket4J library.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/10/02/client-side-consumption-of-a-rate-limited-api-in-java/"><em>https://glaforge.dev</em></a><em>
  on October 2, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fbf08673791"
  width="1" height="1" alt="">'
